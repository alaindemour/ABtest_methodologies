{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": "# From Manual Bayesian A/B Tests to Automated Thompson Sampling\n\n**A practitioner's guide to modern A/B testing for web and mobile product teams**\n\nThis presentation shows how we analyze product launches (e.g. passkey rollouts) with Bayesian A/B test analysis, then suggests how to systematize and automate experimentation with Thompson Sampling — an automated version of the Bayesian technique.\n\n---\n\n## Presentation Overview\n\n1. **Classical Statistics vs. Bayesian Statistics** — NHST $P(D \\mid \\theta)$ vs. Bayesian $P(\\theta \\mid D)$\n2. **Why Bayesian is Better for CX / Pricing A/B Tests** — practical, technical, and institutional reasons\n3. **Our First Test: Non-Inferiority** — NHST provides no answer; our Bayesian test does, even on a small sample\n4. **Select Best Variant** — choose the winning variant with direct probability\n5. **Practical Issues in Large Corporate Environments** — release engineering, approvals, and iteration speed\n6. **Multi-Armed Bandits and Thompson Sampling** — the fully automated solution"
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": "# Part 1: Classical Statistics vs. Bayesian Statistics\n\n---\n\n## The Fundamental Question\n\nWhen we run an experiment and observe data, two very different questions can be asked:\n\n| Framework | Question | Notation |\n|-----------|----------|----------|\n| **NHST (Frequentist)** | \"How likely is this data, assuming the hypothesis is true?\" | $P(\\text{data} \\mid \\theta)$ |\n| **Bayesian** | \"How likely is the hypothesis, given the data we observed?\" | $P(\\theta \\mid \\text{data})$ |\n\nThese look similar but are **fundamentally different**.\n\n- **NHST** bakes the decision rule *into* the probability computation — you set a significance level $\\alpha$ (e.g., 5%), compute a p-value, and get a binary reject/fail-to-reject answer.\n- **Bayesian** keeps the decision rule *outside* the probability computation — you get a full posterior distribution, then apply whatever business logic you need.\n\n> **Analogy**: NHST is like a smoke detector (binary alarm). Bayesian is like a thermometer (continuous reading you can act on however you choose).\n\n---\n\n## NHST in a Nutshell\n\n1. **Assume what you *don't* want to see** — the **null hypothesis** $H_0$\n   - Medicine: \"the drug has no effect\"\n   - A/B test: \"the new experience degrades conversion\"\n2. **Run the experiment** and compute a test statistic\n3. **Ask**: If $H_0$ were true, how likely is a result at least this extreme?\n   - If that probability (the **p-value**) is below threshold $\\alpha$, **reject** $H_0$\n\n### Key Caveats\n\n- Rejecting $H_0$ does **not** prove the alternative is true — it only says the data would be unlikely *if* $H_0$ were correct.\n- The p-value itself is $P(\\text{data} \\mid H_0)$, but the resulting decision comes with **no probability of being correct**.\n- \"Unlikely enough\" is arbitrary — the 5% threshold is a convention, not a law of nature.\n- Without $P(H_0 \\mid \\text{data})$, we **cannot** compute expected values for decision-making. If deploying a bad variant costs \\$100k and a good one gains \\$50k, NHST provides no framework to quantify the expected value of the decision.\n\n---\n\n## Bayesian in a Nutshell\n\n1. **Pick a prior belief** — express it as a probability distribution (e.g., Beta distribution)\n2. **Run the experiment** — observe data\n3. **Apply Bayes' theorem** — update the prior into a **posterior**\n4. **Rinse and repeat** — the posterior becomes the new prior for the next batch of data\n\n$$\n\\boxed{P(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}}\n$$\n\nThe posterior gives us a **full probability distribution** over the unknown parameter — we can compute any quantity we need: point estimates, credible intervals, probability of being better than a threshold, expected loss, etc.\n\n---\n\n### References\n\n- E.T. Jaynes, *Probability Theory: The Logic of Science* — the philosophical case for Bayesian reasoning as \"the language of science\"\n  ([Cambridge University Press](https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99))\n- Kruschke, J.K., *Doing Bayesian Data Analysis* — accessible introduction with practical examples\n  ([Academic Press](https://sites.google.com/site/doingbayesiandataanalysis/))\n"
  },
  {
   "cell_type": "markdown",
   "id": "cgcvolkjwed",
   "source": "# Part 2: Why Bayesian is Better for CX / Pricing A/B Tests\n\n---\n\n## Known Issues with NHST for Product A/B Tests\n\n| NHST Problem | Why It Matters for Product Teams |\n|---|---|\n| **Underpowered with small samples** | At launch, variants get only 2-5% of traffic (hundreds of users). NHST often \"fails to reject\" — giving no actionable answer. |\n| **p-value hacking** | Checking results before the planned sample size inflates false positives. Teams are tempted to \"peek\" — and NHST forbids it. |\n| **Unbalanced samples** | Bugs or misconfiguration in traffic splitters create wildly unequal groups (e.g., 7,000 control vs. 150 variant). NHST loses efficiency; Bayesian handles this naturally. |\n| **No updatability** | Cannot incorporate previous experiment results or domain knowledge. Each test starts from scratch. |\n| **Binary output** | \"Reject\" or \"fail to reject\" — no probability of being better, no expected value, no quantified risk. |\n| **Multiple comparisons** | Comparing 3+ variants requires Bonferroni or other corrections, making an already underpowered test even more conservative. |\n| **Replication crisis** | The accumulated effect of these issues has led to a well-documented crisis of replicability in medicine and social sciences. |\n\n---\n\n## Why Bayesian Excels Here\n\n| Bayesian Advantage | Detail |\n|---|---|\n| **Works with small samples** | Incorporates prior knowledge; provides meaningful conclusions even with n=150 per variant. |\n| **Handles unbalanced allocation** | 90% control, 10% variants? Each variant analyzed independently — no \"balanced design\" needed. |\n| **Scales to many variants** | Single coherent analysis — no multiple comparison penalties. Direct answer: P(A is best)=31%, P(B is best)=47%, etc. |\n| **Provides actionable probabilities** | Instead of \"cannot reject $H_0$\": \"47% chance B is best, 22% it's worse than control.\" Can compute expected values. |\n| **Continuous monitoring** | Check results anytime without p-hacking concerns. Update posteriors incrementally as data arrives. Stop early if a clear winner emerges. |\n| **Updatable** | The posterior from one experiment becomes the prior for the next — mathematically rigorous sequential learning. |\n\n---\n\n## When Does It *Not* Matter?\n\nAll of this matters most when working with **smaller samples** and needing to **make decisions quickly**. Once you have millions of data points and can wait weeks, NHST and Bayesian approaches converge to similar conclusions. The advantage is clearest in the early, high-uncertainty phase of product launches.\n\n---\n\n## Institutional Shift: FDA Bayesian Guidance (January 2026)\n\n> In January 2026, the FDA issued a landmark draft guidance titled **\"Use of Bayesian Methodology in Clinical Trials of Drugs and Biological Products\"**, marking a significant shift in how the agency approaches drug approval.\n\nIf the most conservative regulator in the world is embracing Bayesian methods for drug trials, the case for product A/B testing is even stronger — our stakes are lower and our iteration speed is higher.\n\n- [FDA Draft Guidance (2026)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/use-bayesian-methodology-clinical-trials-drugs-and-biological-products)\n\n---\n\n## Summary Comparison Table\n\n| Aspect | Traditional NHST | Bayesian Approach |\n|--------|-----------------|-------------------|\n| Small samples | Underpowered, inconclusive | Works well with prior knowledge |\n| Unbalanced allocation | Loses efficiency | No problem |\n| Multiple variants | Complex corrections needed | Natural single analysis |\n| Interpretation | p-value (hard to explain) | Probability (intuitive) |\n| Decision making | Binary reject/fail | Quantified risk/confidence |\n| Continuous monitoring | Forbidden (p-hacking) | Allowed and rigorous |\n| Time to decision | Weeks (need larger n) | Days (works with small n) |\n\n---\n\n### References\n\n- Ioannidis, J.P.A. (2005), \"Why Most Published Research Findings Are False\" — the paper that launched the replication crisis discussion\n  ([PLOS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124))\n- Gelman, A. et al., *Bayesian Data Analysis* (3rd ed.) — the standard graduate reference\n  ([Columbia University](http://www.stat.columbia.edu/~gelman/book/))\n- Google/Optimizely engineering blogs on Bayesian A/B testing:\n  - [Optimizely Stats Engine](https://www.optimizely.com/optimization-glossary/stats-engine/)\n  - [Google Analytics Bayesian approach](https://support.google.com/analytics/answer/2846882)\n- VWO knowledge base on Bayesian testing:\n  ([VWO](https://vwo.com/bayesian-ab-testing/))",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "kbg1kcmyqum",
   "source": "# Part 3: Our First Test — Non-Inferiority\n\n---\n\n## The Setup\n\nWe have an existing digital identity + credentials creation flow with a **completion rate of ~71%**. We are adding passkey creation, which adds extra pages and clicks.\n\n- Keep **x%** of traffic on the current experience as the **control group** $C$\n- Send the remaining traffic to one or more **variants** $A_1$, $A_2$, $A_3$\n\n**First question**: Does adding passkey creation cause an **unacceptable degradation** of the completion rate?\n\nThis is a **non-inferiority test** — we want to show the new experience is \"no worse\" than the current one (within a tolerance $\\epsilon$).\n\n---\n\n## What Happened with NHST\n\nWith our real experiment data:\n- **Control**: n=32,106, conversion rate ~70.9%\n- **Variant C** (smallest): n=2,022, conversion rate ~69.0%\n- **Non-inferiority margin**: $\\epsilon$ = 2%\n\nThe NHST non-inferiority test computes a p-value by:\n1. Estimating the standard error from the data (plug-in principle — circular but pragmatically accepted)\n2. Modeling the test statistic under $H_0$ as Gaussian with mean $-\\epsilon$\n3. Computing the right-tail probability\n\n**Result**: p-value $\\approx$ 0.45 — far from the 5% threshold. NHST **fails to reject** $H_0$.\n\n**Translation**: \"We can't say anything. We don't know whether the new CX causes unacceptable degradation.\"\n\nThis is because the sample is small. At launch, you typically put a tiny fraction of traffic on new features — and those small samples are often insufficient for NHST.\n\n> See `ABmethodologies.ipynb` cells 5-22 for the full NHST derivation and numerical examples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "vdvt0xmwzm",
   "source": "%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm, beta as beta_dist\n\n# --- Experiment Data ---\nnC = 32106\nxC = 22772\ncontrol_rate = xC / nC\n\nvariants = {\n    'A': {'n': 4625, 'x': 3244},\n    'B': {'n': 2100, 'x': 1433},\n    'C': {'n': 2022, 'x': 1396}\n}\n\nepsilon = 0.02  # 2% non-inferiority margin\n\nprint(f\"Control: n={nC:,}, rate={control_rate:.2%}\")\nfor name, d in variants.items():\n    r = d['x'] / d['n']\n    print(f\"Variant {name}: n={d['n']:,}, rate={r:.2%}\")\nprint(f\"\\nNon-inferiority margin (epsilon): {epsilon:.0%}\")\nprint(f\"Non-inferiority threshold: {control_rate - epsilon:.2%}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "pmbcsd9ncke",
   "source": "## NHST Result: Inconclusive\n\nLet's run the NHST non-inferiority test on Variant C (the smallest sample) to see why it fails.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hx63j9ik5r5",
   "source": "# NHST non-inferiority test on Variant C\nnX = variants['C']['n']\nxX = variants['C']['x']\nhatpC = xC / nC\nhatpA = xX / nX\nhatDelta = hatpA - hatpC\n\n# Unpooled SE (appropriate for non-inferiority)\nSE = np.sqrt(hatpC * (1 - hatpC) / nC + hatpA * (1 - hatpA) / nX)\nmu_H0 = -epsilon\n\n# p-value: right tail P(Delta >= observed | H0)\np_value = norm.sf(hatDelta, loc=mu_H0, scale=SE)\n\n# Power analysis\npooled_p = (xC + xX) / (nC + nX)\nSE_H1 = np.sqrt(pooled_p * (1 - pooled_p) * (1/nC + 1/nX))\ncritical_value = norm.isf(0.05, loc=mu_H0, scale=SE)\npower = 1 - norm.cdf(critical_value, loc=0, scale=SE_H1)\n\nprint(\"NHST Non-Inferiority Test (Variant C)\")\nprint(\"=\" * 50)\nprint(f\"Observed difference: {hatDelta:.4f}\")\nprint(f\"Standard error: {SE:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\nprint(f\"Power: {power:.1%}\")\nprint()\nif p_value <= 0.05:\n    print(\"Result: REJECT H0 — non-inferiority established\")\nelse:\n    print(\"Result: FAIL TO REJECT — inconclusive\")\n    print(f\"  (Power is only {power:.1%} — test is severely underpowered)\")\n    print(f\"  NHST cannot help us with n={nX} for this variant.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "o6euo7gox4t",
   "source": "## Bayesian Non-Inferiority: Actionable Results\n\nThe Bayesian approach uses a **weakly informative prior** centered on the control rate (since variants operate in the same range), then updates with observed data.\n\n**Key insight**: the prior reflects domain knowledge (\"we added pages, so conversion should be *around* the control rate\") while the non-inferiority threshold reflects a business requirement (\"we can tolerate up to 2% degradation\").\n\n**Posterior update rule** (Beta-Binomial conjugacy):\n$$\n\\text{Prior: } \\text{Beta}(\\alpha_0, \\beta_0) \\quad + \\quad \\text{Data: } k \\text{ successes in } n \\text{ trials} \\quad \\Rightarrow \\quad \\text{Posterior: } \\text{Beta}(\\alpha_0 + k, \\; \\beta_0 + n - k)\n$$\n\nThen we directly compute: $P(\\text{variant rate} > \\text{control rate} - \\epsilon \\mid \\text{data})$\n\n> See `ABmethodologies.ipynb` cells 25-41 and `Bayesian_AB_Test_Workflow.ipynb` cells 5-8 for the full treatment.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "1b6tbnxplj7i",
   "source": "from bayesian import test_non_inferiority_weakly_informative\nfrom plotting_utils import plot_weakly_informative_prior_with_variants\n\n# Run Bayesian non-inferiority test on all variants\nexpected_degradation = 0.01  # Domain knowledge: adding clicks may degrade by ~1%\n\nresults_ni = test_non_inferiority_weakly_informative(\n    n_control=nC,\n    x_control=xC,\n    variants_data=variants,\n    epsilon=epsilon,\n    expected_degradation=expected_degradation,\n    alpha_prior_strength=20,  # Weak prior (high entropy)\n    threshold=0.95\n)\n\nprint(\"Bayesian Non-Inferiority Test Results\")\nprint(\"=\" * 60)\nprint(f\"Prior centered at: {control_rate - expected_degradation:.2%}\")\nprint(f\"Test threshold: {control_rate - epsilon:.2%}\")\nprint()\n\nfor name, res in results_ni.items():\n    status = \"NON-INFERIOR\" if res['is_non_inferior'] else \"NOT NON-INFERIOR\"\n    observed_rate = variants[name]['x'] / variants[name]['n']\n    print(f\"Variant {name}: {status}\")\n    print(f\"  Observed rate: {observed_rate:.2%}, Posterior mean: {res['variant_rate']:.2%}\")\n    print(f\"  P(variant > threshold): {res['probability']:.2%}\")\n    print()\n\n# Visualize\nfig, ax = plot_weakly_informative_prior_with_variants(results_ni)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h3yaw7ixyzs",
   "source": "## Takeaway: Same Data, Different Answers\n\n| Method | Variant C Result | Actionable? |\n|--------|-----------------|-------------|\n| **NHST** | p-value ~0.45, \"fail to reject\" | No — inconclusive |\n| **Bayesian** | P(non-inferior) > 95% | Yes — non-inferiority established |\n\nThe Bayesian approach succeeds because it:\n1. Uses a **weakly informative prior** reflecting domain knowledge (variants should perform \"around\" the control rate)\n2. Provides a **direct probability** rather than a p-value\n3. Works naturally with the **small, unbalanced samples** typical of product launches\n\n---\n\n### References\n\n- Christensen, R. et al., *Bayesian Ideas and Data Analysis* — practical Bayesian methods\n  ([CRC Press](https://www.routledge.com/Bayesian-Ideas-and-Data-Analysis/Christensen-Johnson-Branscum-Hanson/p/book/9781439803547))\n- FDA guidance on non-inferiority trial design:\n  ([FDA Non-Inferiority Guidance](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/non-inferiority-clinical-trials-establish-effectiveness))",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "iecqvjrpgxa",
   "source": "# Part 4: Select Best Variant\n\n---\n\n## The Problem NHST Was Not Designed For\n\nNHST was built for **asymmetric** questions: \"Is this drug better than placebo?\" It struggles with **symmetric** questions: \"Which of A, B, C is best?\"\n\n| NHST Approach | Problem |\n|---|---|\n| **Winner-takes-all** (highest observed rate) | Ignores uncertainty; easily picks wrong variant with small samples |\n| **Pairwise t-tests + Bonferroni** | Very conservative (higher Type II error); only gives significant/not-significant |\n| **ANOVA + post-hoc** | Only says \"something differs\" — not which is best or by how much |\n| **Confidence interval overlap** | Inconclusive; overlapping CIs don't mean \"no difference\" |\n\n**None of these directly answer**: \"What is the probability that variant A is the best?\"\n\n---\n\n## Bayesian: Direct Probability of Being Best\n\nThe Bayesian framework answers the question we actually care about:\n\n1. Compute the **posterior Beta distribution** for each variant\n2. Draw a large number of samples (e.g., 100k) from each posterior via **Monte Carlo simulation**\n3. For each draw, identify which variant has the highest conversion rate\n4. Report: $P(A \\text{ is best}), \\; P(B \\text{ is best}), \\; P(C \\text{ is best})$\n\n**Advantages**:\n- **Direct answer**: \"Variant A is best with 88% probability\"\n- **No multiple-comparison corrections** — single coherent analysis\n- **Scales naturally** to any number of variants\n- **Quantifies uncertainty** — not just yes/no\n- **Business-friendly** — easy to factor in risk, cost, implementation difficulty\n\n> See `ABmethodologies.ipynb` cells 42-50 and `Bayesian_AB_Test_Workflow.ipynb` cells 9-12 for the full analysis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5t1x5kthjzt",
   "source": "from bayesian import select_best_variant\nfrom plotting_utils import plot_multiple_posteriors_comparison\n\n# Select the best variant using Monte Carlo simulation\nselection = select_best_variant(\n    variants_data=variants,\n    alpha_prior=1,   # Non-informative prior for fair comparison\n    beta_prior=1,\n    credible_level=0.95,\n    n_simulations=100000\n)\n\n# Display results\nprint(\"Probability Each Variant is Best\")\nprint(\"=\" * 50)\nfor name in ['A', 'B', 'C']:\n    prob = selection['probabilities'][name]\n    bar = '#' * int(prob * 50)\n    print(f\"  P({name} is best) = {prob:.2%}  {bar}\")\n\nwinner = selection['best_variant']\nprint(f\"\\nWinner: Variant {winner}\")\nprint(f\"  Probability of being best: {selection['probabilities'][winner]:.2%}\")\nprint(f\"  Posterior mean: {selection['posterior_means'][winner]:.2%}\")\nci = selection['credible_intervals'][winner]\nprint(f\"  95% Credible interval: [{ci[0]:.2%}, {ci[1]:.2%}]\")\nprint(f\"  Expected loss: {selection['expected_loss'][winner]:.4f}\")\n\n# Visualize posterior distributions\nposteriors = {}\nfor name, data in variants.items():\n    alpha_post = data['x'] + 1\n    beta_post = data['n'] - data['x'] + 1\n    posteriors[name] = {\n        'alpha': alpha_post,\n        'beta': beta_post,\n        'mean': alpha_post / (alpha_post + beta_post),\n        'ci_95': (beta_dist.ppf(0.025, alpha_post, beta_post),\n                  beta_dist.ppf(0.975, alpha_post, beta_post))\n    }\n\nfig, ax = plot_multiple_posteriors_comparison(\n    posteriors=posteriors,\n    control_group_conversion_rate=control_rate,\n    epsilon=epsilon\n)\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "yh6pts6oyoh",
   "source": "# Part 5: Practical Issues in Large Corporate Environments\n\n---\n\n## The Iteration Speed Problem\n\nEven after a Bayesian analysis delivers a clear winner, **deploying that winner can take months** in a large organization:\n\n### A Real-World Timeline\n\n| Date | Event |\n|------|-------|\n| **Early November** | Analysis complete. Variant A identified as winner with >88% probability. Decision made to deploy. |\n| **Mid November** | Release freeze begins (Black Friday / holiday season). No changes allowed. |\n| **Late November** | Bug discovered in the custom A/B traffic splitter. Needs fix before deployment. |\n| **December** | Legal review required for the change. Holiday schedules slow approvals. |\n| **January** | Winning variant finally deployed to 100% of traffic. |\n\n**Result**: ~2 months between \"we know the answer\" and \"users benefit from it.\"\n\n---\n\n## The Bottlenecks\n\n1. **Release Engineering**: Code freezes, deployment windows, staging environments, QA cycles\n2. **Approvals**: Legal, compliance, product management sign-offs\n3. **Custom Infrastructure**: Bespoke traffic splitters that need manual reconfiguration\n4. **Organizational Inertia**: Multiple teams need to coordinate for a simple traffic reallocation\n\n---\n\n## The Cost of Delay\n\nEvery day we keep showing inferior variants to users:\n- **Lost conversions** — users see a worse experience than necessary\n- **Opportunity cost** — the team can't start the next experiment\n- **Compounding delay** — each experiment in the pipeline waits for the previous one\n\nIf the winning variant converts 2% better and we see 10,000 users/day, a 60-day delay means ~12,000 lost conversions.\n\n---\n\n## The Solution: Automate the Decision Loop\n\nWhat if the system could **automatically shift traffic** to better-performing variants — without manual intervention, release cycles, or approvals for each reallocation?\n\nThis is exactly what **Thompson Sampling** provides.\n\n> Instead of: Experiment → Analyze → Decide → Request release → Wait → Deploy → Repeat\n>\n> We get: Deploy all variants → Algorithm continuously optimizes traffic → Winner emerges automatically",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "zhhnmf5clyp",
   "source": "# Part 6: Multi-Armed Bandits and Thompson Sampling\n\n---\n\n## The Multi-Armed Bandit Problem\n\nImagine a casino with **K slot machines** (\"one-armed bandits\"), each with an unknown payout probability. You have a limited budget. How do you maximize total payout?\n\n- **Exploration**: Try different machines to learn which is best\n- **Exploitation**: Play the machine you currently think is best\n\nToo much exploration wastes pulls on bad machines. Too much exploitation might miss a better machine.\n\n### A/B Testing *Is* a Bandit Problem\n\n| Casino | A/B Testing |\n|--------|-------------|\n| Slot machines (\"arms\") | Variants (A, B, C, control) |\n| Pull a lever | Show a variant to a user |\n| Payout | User converts |\n| Unknown probability | True conversion rate |\n| Limited budget | Finite users |\n\n**Goal**: Maximize total conversions (not just *identify* the best variant).\n\n**Regret**: The difference between what we *would* have achieved always showing the best variant vs. what we *actually* achieved.\n\n---\n\n## Thompson Sampling: The Algorithm\n\nThompson Sampling is **provably optimal** for minimizing cumulative regret and is **incredibly simple**:\n\nFor each incoming user:\n1. **Sample** once from each variant's posterior: $\\theta_i \\sim \\text{Beta}(\\alpha_i, \\beta_i)$\n2. **Choose** the variant with the highest sample: $i^* = \\arg\\max_i \\theta_i$\n3. **Show** that variant to the user\n4. **Observe** the outcome: conversion (1) or not (0)\n5. **Update** that variant's posterior: $\\alpha_{i^*} \\mathrel{+}= r, \\; \\beta_{i^*} \\mathrel{+}= (1 - r)$\n\n**That's it.** Five lines of logic — simpler than any classical statistical test.\n\n### Why It Works\n\n- **Early on**: Wide posteriors → high variance in samples → more exploration\n- **Later**: Narrow posteriors → low variance → exploitation of the best variant\n- **Automatically**: No parameters to tune, no stopping rules, no sample size calculations\n\nThe algorithm allocates traffic to variant $i$ proportionally to $P(\\text{variant } i \\text{ is best} \\mid \\text{data})$ — which is *exactly* what we computed in the Bayesian best-variant selection above.\n\n> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` for the full treatment, simulation code, and production considerations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cmyfo4shpps",
   "source": "# --- Thompson Sampling Simulation ---\nnp.random.seed(42)\n\ntrue_rates = {\n    'A': 3244 / 4625,  # ~70.1%\n    'B': 1433 / 2100,  # ~68.2%\n    'C': 1396 / 2022,  # ~69.0%\n}\n\ndef run_thompson_sampling(true_rates, n_users):\n    \"\"\"Simulate Thompson sampling and return results.\"\"\"\n    variants_list = list(true_rates.keys())\n    alpha = {v: 1 for v in variants_list}\n    beta = {v: 1 for v in variants_list}\n    n_shown = {v: 0 for v in variants_list}\n    n_conv = {v: 0 for v in variants_list}\n    total_conv = 0\n    \n    history = {'user': [], 'prob_A': [], 'prob_B': [], 'prob_C': []}\n    \n    for uid in range(n_users):\n        samples = {v: np.random.beta(alpha[v], beta[v]) for v in variants_list}\n        chosen = max(samples, key=samples.get)\n        converted = int(np.random.random() < true_rates[chosen])\n        \n        alpha[chosen] += converted\n        beta[chosen] += (1 - converted)\n        n_shown[chosen] += 1\n        n_conv[chosen] += converted\n        total_conv += converted\n        \n        if uid % 50 == 0:\n            mc = 10000\n            counts = {v: 0 for v in variants_list}\n            for _ in range(mc):\n                s = {v: np.random.beta(alpha[v], beta[v]) for v in variants_list}\n                counts[max(s, key=s.get)] += 1\n            history['user'].append(uid)\n            for v in variants_list:\n                history[f'prob_{v}'].append(counts[v] / mc)\n    \n    return n_shown, n_conv, total_conv, history\n\ndef run_fixed_allocation(true_rates, n_users):\n    \"\"\"Simulate fixed equal allocation.\"\"\"\n    variants_list = list(true_rates.keys())\n    n_shown = {v: 0 for v in variants_list}\n    n_conv = {v: 0 for v in variants_list}\n    total_conv = 0\n    for uid in range(n_users):\n        chosen = variants_list[uid % len(variants_list)]\n        converted = int(np.random.random() < true_rates[chosen])\n        n_shown[chosen] += 1\n        n_conv[chosen] += converted\n        total_conv += converted\n    return n_shown, n_conv, total_conv\n\nn_users = 5000\n\n# Run both strategies\nts_shown, ts_conv, ts_total, ts_history = run_thompson_sampling(true_rates, n_users)\nfx_shown, fx_conv, fx_total = run_fixed_allocation(true_rates, n_users)\n\nbest = max(true_rates, key=true_rates.get)\noptimal = n_users * true_rates[best]\n\nprint(\"THOMPSON SAMPLING vs FIXED ALLOCATION\")\nprint(\"=\" * 60)\nprint(f\"{'':20s} {'Thompson':>12s} {'Fixed':>12s}\")\nprint(\"-\" * 60)\nfor v in ['A', 'B', 'C']:\n    ts_pct = 100 * ts_shown[v] / n_users\n    fx_pct = 100 * fx_shown[v] / n_users\n    print(f\"Variant {v} traffic:    {ts_pct:10.1f}%  {fx_pct:10.1f}%\")\nprint(\"-\" * 60)\nprint(f\"Total conversions:  {ts_total:10d}   {fx_total:10d}\")\nprint(f\"Conversion rate:    {100*ts_total/n_users:10.2f}%  {100*fx_total/n_users:10.2f}%\")\nprint(f\"Regret:             {optimal - ts_total:10.0f}   {optimal - fx_total:10.0f}\")\nprint(f\"\\nThompson Sampling gained {ts_total - fx_total:.0f} extra conversions\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cgv0frlw0k",
   "source": "# Visualize: P(variant is best) over time\nfig, ax = plt.subplots(figsize=(12, 5))\nax.plot(ts_history['user'], ts_history['prob_A'], label='P(A is best)', lw=2, color='#2ecc71')\nax.plot(ts_history['user'], ts_history['prob_B'], label='P(B is best)', lw=2, color='#e74c3c')\nax.plot(ts_history['user'], ts_history['prob_C'], label='P(C is best)', lw=2, color='#3498db')\nax.axhline(y=0.95, color='gray', ls='--', lw=1, alpha=0.5, label='95% threshold')\nax.set_xlabel('Number of Users', fontsize=12)\nax.set_ylabel('Probability of Being Best', fontsize=12)\nax.set_title('Thompson Sampling: Learning Which Variant is Best Over Time', fontsize=14, fontweight='bold')\nax.legend(loc='right', fontsize=10)\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 1.05)\nplt.tight_layout()\nplt.show()\n\n# Find when 95% confidence reached\nfor i, p in enumerate(ts_history['prob_A']):\n    if p >= 0.95:\n        print(f\"Reached 95% confidence that A is best after ~{ts_history['user'][i]:,} users\")\n        break\nelse:\n    print(f\"Did not reach 95% confidence within {n_users:,} users (but traffic was already optimized)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "lo2s6ev108",
   "source": "## Key Benefits of Thompson Sampling\n\n### Dynamic Traffic Allocation\nThompson Sampling **automatically** routes more traffic to better-performing variants. Inferior variants naturally fade out without manual intervention.\n\n### Adding New Variants Dynamically\nOne of the greatest practical advantages: **new variants can enter at any time**.\n\n1. New variant arrives → initialize with prior Beta(1, 1)\n2. It immediately competes in sampling with existing variants\n3. Wide posterior → gets explored (sometimes samples high → gets traffic)\n4. Good variants prove themselves; bad ones fade out\n\nNo need to stop the test, redistribute traffic, recalculate sample sizes, or worry about multiple comparisons.\n\n> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` cells 13-15 for a full simulation of adding variant D mid-experiment.\n\n### Production Considerations\n\nWhen implementing Thompson Sampling in production, there are important real-world considerations beyond the basic algorithm:\n\n| Consideration | Issue | Solution |\n|---|---|---|\n| **Delayed feedback** | Conversion may happen minutes/hours after variant shown | Batch updates (every 10-60 min); weakly informative priors reduce early regret |\n| **Non-stationarity** | Conversion rates drift over time (seasonality, product changes) | Exponential decay or sliding window on observations |\n| **Scalability** | High-traffic systems need low-latency decisions | Store $(\\alpha, \\beta)$ in distributed cache; batch posterior updates |\n| **Cold start** | New variants start with no data | Weakly informative priors; accept initial exploration phase |\n\n> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` Appendix for detailed treatment of delayed feedback, non-stationarity, and production architecture.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "ce1roxv4l9l",
   "source": "# Summary\n\n---\n\n## The Journey\n\n| Step | Manual / NHST | Bayesian / Automated |\n|------|--------------|---------------------|\n| **1. Non-inferiority** | NHST: \"fail to reject\" (inconclusive) | Bayesian: \"95%+ probability of non-inferiority\" |\n| **2. Best variant** | Pairwise tests + corrections (complex, conservative) | Monte Carlo: \"Variant A is best with 88% probability\" |\n| **3. Deploy winner** | Manual release cycle (weeks/months) | Thompson Sampling: automatic, continuous optimization |\n| **4. Add new variant** | Stop test, redesign, restart | Add anytime — algorithm adapts seamlessly |\n\n---\n\n## The Full Comparison\n\n| Aspect | Traditional A/B (NHST) | Thompson Sampling |\n|--------|----------------------|-------------------|\n| Traffic allocation | Fixed (e.g., 33/33/33) | Dynamic (adapts to performance) |\n| Total conversions | Suboptimal (wastes traffic) | Near-optimal (minimizes regret) |\n| Time to decision | Wait for significance | Continuous improvement |\n| Adding variants | Restart test | Add anytime |\n| Removing variants | Manual rebalance | Automatic fade-out |\n| Multiple comparisons | Need corrections | No problem |\n| Stopping rule | Pre-determined | Flexible |\n| Implementation | Complex statistics | 5 lines of code |\n\n---\n\n## Bottom Line\n\nFor modern product development with rapid iteration cycles, risk-averse traffic allocation, and multiple design options:\n\n1. **Use Bayesian methods** for non-inferiority testing and variant selection — they work with small samples, provide actionable probabilities, and handle unbalanced designs naturally.\n\n2. **Use Thompson Sampling** to automate the entire experimentation loop — it minimizes regret, adapts traffic dynamically, and eliminates the release-engineering bottleneck.\n\n3. **Start simple** — the mathematical foundations (Beta-Binomial conjugacy) are elegant and the code is minimal. A production MVP can be built with a distributed cache for $(\\alpha, \\beta)$ parameters and a few lines of sampling logic.\n\n---\n\n## Companion Notebooks\n\n| Notebook | Content |\n|----------|---------|\n| `ABmethodologies.ipynb` | Full mathematical derivation of NHST and Bayesian approaches, numerical examples, plotting utilities |\n| `Bayesian_AB_Test_Workflow.ipynb` | Concise Bayesian workflow: non-inferiority test → variant selection, with utility functions |\n| `ThompsonSampling_DynamicTrafficAllocation.ipynb` | Thompson Sampling algorithm, simulations, dynamic variant addition, production considerations |\n\n---\n\n## References\n\n### Books\n- Jaynes, E.T. *Probability Theory: The Logic of Science* — [Cambridge University Press](https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99)\n- Gelman, A. et al. *Bayesian Data Analysis* (3rd ed.) — [Columbia University](http://www.stat.columbia.edu/~gelman/book/)\n- Kruschke, J.K. *Doing Bayesian Data Analysis* — [Academic Press](https://sites.google.com/site/doingbayesiandataanalysis/)\n\n### Papers\n- Ioannidis, J.P.A. (2005) \"Why Most Published Research Findings Are False\" — [PLOS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)\n- Thompson, W.R. (1933) \"On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples\" — [Biometrika](https://doi.org/10.1093/biomet/25.3-4.285)\n- Chapelle, O. & Li, L. (2011) \"An Empirical Evaluation of Thompson Sampling\" — [NeurIPS](https://papers.nips.cc/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html)\n- Russo, D.J. et al. (2018) \"A Tutorial on Thompson Sampling\" — [Foundations and Trends in Machine Learning](https://arxiv.org/abs/1707.02038)\n- Agrawal, S. & Goyal, N. (2012) \"Analysis of Thompson Sampling for the Multi-armed Bandit Problem\" — [COLT](https://arxiv.org/abs/1111.1797)\n\n### Industry & Regulatory\n- FDA Draft Guidance (2026): \"Use of Bayesian Methodology in Clinical Trials\" — [FDA](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/use-bayesian-methodology-clinical-trials-drugs-and-biological-products)\n- Optimizely Stats Engine — [Optimizely](https://www.optimizely.com/optimization-glossary/stats-engine/)\n- VWO Bayesian A/B Testing — [VWO](https://vwo.com/bayesian-ab-testing/)\n- Google \"Multi-Armed Bandits\" (2024) — [Google AI Blog](https://ai.googleblog.com/)\n\n### Tutorials\n- Cam Davidson-Pilon, *Bayesian Methods for Hackers* — [GitHub / online book](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n- Lilian Weng, \"The Multi-Armed Bandit Problem and Its Solutions\" — [Blog post](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)",
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}