{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# From Manual Bayesian A/B Tests to Automated Thompson Sampling\n",
    "\n",
    "**A practitioner's guide to modern A/B testing for web and mobile product teams**\n",
    "\n",
    "This presentation shows how we analyze product launches (e.g. passkey rollouts) with Bayesian A/B test analysis, then suggests how to systematize and automate experimentation with Thompson Sampling — an automated version of the Bayesian technique.\n",
    "\n",
    "---\n",
    "\n",
    "## Presentation Overview\n",
    "\n",
    "1. **Classical Statistics vs. Bayesian Statistics** — NHST $P(D \\mid \\theta)$ vs. Bayesian $P(\\theta \\mid D)$\n",
    "2. **Why Bayesian is Better for CX / Pricing A/B Tests** — practical, technical, and institutional reasons\n",
    "3. **Our First Test: Non-Inferiority** — NHST provides no answer; our Bayesian test does, even on a small sample\n",
    "4. **Select Best Variant** — choose the winning variant with direct probability\n",
    "5. **Practical Issues in Large Corporate Environments** — release engineering, approvals, and iteration speed\n",
    "6. **Multi-Armed Bandits and Thompson Sampling** — the fully automated solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Part 1: Classical Statistics vs. Bayesian Statistics\n",
    "\n",
    "---\n",
    "\n",
    "## The Fundamental Question\n",
    "\n",
    "When we run an experiment and observe data, two very different questions can be asked:\n",
    "\n",
    "| Framework | Question | Notation |\n",
    "|-----------|----------|----------|\n",
    "| **NHST (Frequentist)** | \"How likely is this data, assuming the hypothesis is true?\" | $P(\\text{data} \\mid \\theta)$ |\n",
    "| **Bayesian** | \"How likely is the hypothesis, given the data we observed?\" | $P(\\theta \\mid \\text{data})$ |\n",
    "\n",
    "These look similar but are **fundamentally different**.\n",
    "\n",
    "- **NHST** bakes the decision rule *into* the probability computation — you set a significance level $\\alpha$ (e.g., 5%), compute a p-value, and get a binary reject/fail-to-reject answer.\n",
    "- **Bayesian** keeps the decision rule *outside* the probability computation — you get a full posterior distribution, then apply whatever business logic you need.\n",
    "\n",
    "> **Analogy**: NHST is like a smoke detector (binary alarm). Bayesian is like a thermometer (continuous reading you can act on however you choose).\n",
    "\n",
    "---\n",
    "\n",
    "## NHST in a Nutshell\n",
    "\n",
    "1. **Assume what you *don't* want to see** — the **null hypothesis** $H_0$\n",
    "   - Medicine: \"the drug has no effect\"\n",
    "   - A/B test: \"the new experience degrades conversion\"\n",
    "2. **Run the experiment** and compute a test statistic\n",
    "3. **Ask**: If $H_0$ were true, how likely is a result at least this extreme?\n",
    "   - If that probability (the **p-value**) is below threshold $\\alpha$, **reject** $H_0$\n",
    "\n",
    "### Key Caveats\n",
    "\n",
    "- Rejecting $H_0$ does **not** prove the alternative is true — it only says the data would be unlikely *if* $H_0$ were correct.\n",
    "- The p-value itself is $P(\\text{data} \\mid H_0)$, but the resulting decision comes with **no probability of being correct**.\n",
    "- \"Unlikely enough\" is arbitrary — the 5% threshold is a convention, not a law of nature.\n",
    "- Without $P(H_0 \\mid \\text{data})$, we **cannot** compute expected values for decision-making. If deploying a bad variant costs \\$100k and a good one gains \\$50k, NHST provides no framework to quantify the expected value of the decision.\n",
    "\n",
    "---\n",
    "\n",
    "## Bayesian in a Nutshell\n",
    "\n",
    "1. **Pick a prior belief** — express it as a probability distribution (e.g., Beta distribution)\n",
    "2. **Run the experiment** — observe data\n",
    "3. **Apply Bayes' theorem** — update the prior into a **posterior**\n",
    "4. **Rinse and repeat** — the posterior becomes the new prior for the next batch of data\n",
    "\n",
    "$$\n",
    "\\boxed{P(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}}\n",
    "$$\n",
    "\n",
    "The posterior gives us a **full probability distribution** over the unknown parameter — we can compute any quantity we need: point estimates, credible intervals, probability of being better than a threshold, expected loss, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- E.T. Jaynes, *Probability Theory: The Logic of Science* — the philosophical case for Bayesian reasoning as \"the language of science\"\n",
    "  ([Cambridge University Press](https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99))\n",
    "- Kruschke, J.K., *Doing Bayesian Data Analysis* — accessible introduction with practical examples\n",
    "  ([Academic Press](https://sites.google.com/site/doingbayesiandataanalysis/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Part 2: Why Bayesian is Better for CX / Pricing A/B Tests\n",
    "\n",
    "---\n",
    "\n",
    "## Known Issues with NHST for Product A/B Tests\n",
    "\n",
    "| NHST Problem | Why It Matters for Product Teams |\n",
    "|---|---|\n",
    "| **Underpowered with small samples** | At launch, variants get only 2-5% of traffic (hundreds of users). NHST often \"fails to reject\" — giving no actionable answer. |\n",
    "| **p-value hacking** | Checking results before the planned sample size inflates false positives. Teams are tempted to \"peek\" — and NHST forbids it. |\n",
    "| **Unbalanced samples** | Bugs or misconfiguration in traffic splitters create wildly unequal groups (e.g., 7,000 control vs. 150 variant). NHST loses efficiency; Bayesian handles this naturally. |\n",
    "| **No updatability** | Cannot incorporate previous experiment results or domain knowledge. Each test starts from scratch. |\n",
    "| **Binary output** | \"Reject\" or \"fail to reject\" — no probability of being better, no expected value, no quantified risk. |\n",
    "| **Multiple comparisons** | Comparing 3+ variants requires Bonferroni or other corrections, making an already underpowered test even more conservative. |\n",
    "| **Replication crisis** | The accumulated effect of these issues has led to a well-documented crisis of replicability in medicine and social sciences. |\n",
    "\n",
    "---\n",
    "\n",
    "## Why Bayesian Excels Here\n",
    "\n",
    "| Bayesian Advantage | Detail |\n",
    "|---|---|\n",
    "| **Works with small samples** | Incorporates prior knowledge; provides meaningful conclusions even with n=150 per variant. |\n",
    "| **Handles unbalanced allocation** | 90% control, 10% variants? Each variant analyzed independently — no \"balanced design\" needed. |\n",
    "| **Scales to many variants** | Single coherent analysis — no multiple comparison penalties. Direct answer: P(A is best)=31%, P(B is best)=47%, etc. |\n",
    "| **Provides actionable probabilities** | Instead of \"cannot reject $H_0$\": \"47% chance B is best, 22% it's worse than control.\" Can compute expected values. |\n",
    "| **Continuous monitoring** | Check results anytime without p-hacking concerns. Update posteriors incrementally as data arrives. Stop early if a clear winner emerges. |\n",
    "| **Updatable** | The posterior from one experiment becomes the prior for the next — mathematically rigorous sequential learning. |\n",
    "\n",
    "---\n",
    "\n",
    "## When Does It *Not* Matter?\n",
    "\n",
    "All of this matters most when working with **smaller samples** and needing to **make decisions quickly**. Once you have millions of data points and can wait weeks, NHST and Bayesian approaches converge to similar conclusions. The advantage is clearest in the early, high-uncertainty phase of product launches.\n",
    "\n",
    "---\n",
    "\n",
    "## Institutional Shift: FDA Bayesian Guidance (January 2026)\n",
    "\n",
    "> In January 2026, the FDA issued a landmark draft guidance titled **\"Use of Bayesian Methodology in Clinical Trials of Drugs and Biological Products\"**, marking a significant shift in how the agency approaches drug approval.\n",
    "\n",
    "If the most conservative regulator in the world is embracing Bayesian methods for drug trials, the case for product A/B testing is even stronger — our stakes are lower and our iteration speed is higher.\n",
    "\n",
    "- [FDA Draft Guidance (2026)](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/use-bayesian-methodology-clinical-trials-drugs-and-biological-products)\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Comparison Table\n",
    "\n",
    "| Aspect | Traditional NHST | Bayesian Approach |\n",
    "|--------|-----------------|-------------------|\n",
    "| Small samples | Underpowered, inconclusive | Works well with prior knowledge |\n",
    "| Unbalanced allocation | Loses efficiency | No problem |\n",
    "| Multiple variants | Complex corrections needed | Natural single analysis |\n",
    "| Interpretation | p-value (hard to explain) | Probability (intuitive) |\n",
    "| Decision making | Binary reject/fail | Quantified risk/confidence |\n",
    "| Continuous monitoring | Forbidden (p-hacking) | Allowed and rigorous |\n",
    "| Time to decision | Weeks (need larger n) | Days (works with small n) |\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- Ioannidis, J.P.A. (2005), \"Why Most Published Research Findings Are False\" — the paper that launched the replication crisis discussion\n",
    "  ([PLOS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124))\n",
    "- Gelman, A. et al., *Bayesian Data Analysis* (3rd ed.) — the standard graduate reference\n",
    "  ([Columbia University](http://www.stat.columbia.edu/~gelman/book/))\n",
    "- Google/Optimizely engineering blogs on Bayesian A/B testing:\n",
    "  - [Optimizely Stats Engine](https://www.optimizely.com/optimization-glossary/stats-engine/)\n",
    "  - [Google Analytics Bayesian approach](https://support.google.com/analytics/answer/2846882)\n",
    "- VWO knowledge base on Bayesian testing:\n",
    "  ([VWO](https://vwo.com/bayesian-ab-testing/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "# Part 3: Our First Test — Non-Inferiority\n",
    "\n",
    "---\n",
    "\n",
    "## The Setup\n",
    "\n",
    "We have an existing digital identity + credentials creation flow with a **completion rate of ~71%**. We are adding passkey creation, which adds extra pages and clicks.\n",
    "\n",
    "- Keep **x%** of traffic on the current experience as the **control group** $C$\n",
    "- Send the remaining traffic to one or more **variants** $A_1$, $A_2$, $A_3$\n",
    "\n",
    "**First question**: Does adding passkey creation cause an **unacceptable degradation** of the completion rate?\n",
    "\n",
    "This is a **non-inferiority test** — we want to show the new experience is \"no worse\" than the current one (within a tolerance $\\epsilon$).\n",
    "\n",
    "---\n",
    "\n",
    "## What Happened with NHST\n",
    "\n",
    "With our real experiment data:\n",
    "- **Control**: n=32,106, conversion rate ~70.9%\n",
    "- **Variant C** (smallest): n=2,022, conversion rate ~69.0%\n",
    "- **Non-inferiority margin**: $\\epsilon$ = 2%\n",
    "\n",
    "The NHST non-inferiority test computes a p-value by:\n",
    "1. Estimating the standard error from the data (plug-in principle — circular but pragmatically accepted)\n",
    "2. Modeling the test statistic under $H_0$ as Gaussian with mean $-\\epsilon$\n",
    "3. Computing the right-tail probability\n",
    "\n",
    "**Result**: p-value $\\approx$ 0.45 — far from the 5% threshold. NHST **fails to reject** $H_0$.\n",
    "\n",
    "**Translation**: \"We can't say anything. We don't know whether the new CX causes unacceptable degradation.\"\n",
    "\n",
    "This is because the sample is small. At launch, you typically put a tiny fraction of traffic on new features — and those small samples are often insufficient for NHST.\n",
    "\n",
    "> See `ABmethodologies.ipynb` cells 5-22 for the full NHST derivation and numerical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, beta as beta_dist\n",
    "\n",
    "# --- Experiment Data ---\n",
    "nC = 32106\n",
    "xC = 22772\n",
    "control_rate = xC / nC\n",
    "\n",
    "variants = {\n",
    "    'A': {'n': 4625, 'x': 3244},\n",
    "    'B': {'n': 2100, 'x': 1433},\n",
    "    'C': {'n': 2022, 'x': 1396}\n",
    "}\n",
    "\n",
    "epsilon = 0.02  # 2% non-inferiority margin\n",
    "\n",
    "print(f\"Control: n={nC:,}, rate={control_rate:.2%}\")\n",
    "for name, d in variants.items():\n",
    "    r = d['x'] / d['n']\n",
    "    print(f\"Variant {name}: n={d['n']:,}, rate={r:.2%}\")\n",
    "print(f\"\\nNon-inferiority margin (epsilon): {epsilon:.0%}\")\n",
    "print(f\"Non-inferiority threshold: {control_rate - epsilon:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## NHST Result: Inconclusive\n",
    "\n",
    "Let's run the NHST non-inferiority test on Variant C (the smallest sample) to see why it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NHST non-inferiority test on Variant C\n",
    "nX = variants['C']['n']\n",
    "xX = variants['C']['x']\n",
    "hatpC = xC / nC\n",
    "hatpA = xX / nX\n",
    "hatDelta = hatpA - hatpC\n",
    "\n",
    "# Unpooled SE (appropriate for non-inferiority)\n",
    "SE = np.sqrt(hatpC * (1 - hatpC) / nC + hatpA * (1 - hatpA) / nX)\n",
    "mu_H0 = -epsilon\n",
    "\n",
    "# p-value: right tail P(Delta >= observed | H0)\n",
    "p_value = norm.sf(hatDelta, loc=mu_H0, scale=SE)\n",
    "\n",
    "# Power analysis\n",
    "pooled_p = (xC + xX) / (nC + nX)\n",
    "SE_H1 = np.sqrt(pooled_p * (1 - pooled_p) * (1/nC + 1/nX))\n",
    "critical_value = norm.isf(0.05, loc=mu_H0, scale=SE)\n",
    "power = 1 - norm.cdf(critical_value, loc=0, scale=SE_H1)\n",
    "\n",
    "print(\"NHST Non-Inferiority Test (Variant C)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Observed difference: {hatDelta:.4f}\")\n",
    "print(f\"Standard error: {SE:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "print(f\"Power: {power:.1%}\")\n",
    "print()\n",
    "if p_value <= 0.05:\n",
    "    print(\"Result: REJECT H0 — non-inferiority established\")\n",
    "else:\n",
    "    print(\"Result: FAIL TO REJECT — inconclusive\")\n",
    "    print(f\"  (Power is only {power:.1%} — test is severely underpowered)\")\n",
    "    print(f\"  NHST cannot help us with n={nX} for this variant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Bayesian Non-Inferiority: Actionable Results\n",
    "\n",
    "The Bayesian approach uses a **weakly informative prior** centered on the control rate (since variants operate in the same range), then updates with observed data.\n",
    "\n",
    "**Key insight**: the prior reflects domain knowledge (\"we added pages, so conversion should be *around* the control rate\") while the non-inferiority threshold reflects a business requirement (\"we can tolerate up to 2% degradation\").\n",
    "\n",
    "**Posterior update rule** (Beta-Binomial conjugacy):\n",
    "$$\n",
    "\\text{Prior: } \\text{Beta}(\\alpha_0, \\beta_0) \\quad + \\quad \\text{Data: } k \\text{ successes in } n \\text{ trials} \\quad \\Rightarrow \\quad \\text{Posterior: } \\text{Beta}(\\alpha_0 + k, \\; \\beta_0 + n - k)\n",
    "$$\n",
    "\n",
    "Then we directly compute: $P(\\text{variant rate} > \\text{control rate} - \\epsilon \\mid \\text{data})$\n",
    "\n",
    "> See `ABmethodologies.ipynb` cells 25-41 and `Bayesian_AB_Test_Workflow.ipynb` cells 5-8 for the full treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian import test_non_inferiority_weakly_informative\n",
    "from plotting_utils import plot_weakly_informative_prior_with_variants\n",
    "\n",
    "# Run Bayesian non-inferiority test on all variants\n",
    "expected_degradation = 0.01  # Domain knowledge: adding clicks may degrade by ~1%\n",
    "\n",
    "results_ni = test_non_inferiority_weakly_informative(\n",
    "    n_control=nC,\n",
    "    x_control=xC,\n",
    "    variants_data=variants,\n",
    "    epsilon=epsilon,\n",
    "    expected_degradation=expected_degradation,\n",
    "    alpha_prior_strength=20,  # Weak prior (high entropy)\n",
    "    threshold=0.95\n",
    ")\n",
    "\n",
    "print(\"Bayesian Non-Inferiority Test Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Prior centered at: {control_rate - expected_degradation:.2%}\")\n",
    "print(f\"Test threshold: {control_rate - epsilon:.2%}\")\n",
    "print()\n",
    "\n",
    "for name, res in results_ni.items():\n",
    "    status = \"NON-INFERIOR\" if res['is_non_inferior'] else \"NOT NON-INFERIOR\"\n",
    "    observed_rate = variants[name]['x'] / variants[name]['n']\n",
    "    print(f\"Variant {name}: {status}\")\n",
    "    print(f\"  Observed rate: {observed_rate:.2%}, Posterior mean: {res['variant_rate']:.2%}\")\n",
    "    print(f\"  P(variant > threshold): {res['probability']:.2%}\")\n",
    "    print()\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plot_weakly_informative_prior_with_variants(results_ni)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Takeaway: Same Data, Different Answers\n",
    "\n",
    "| Method | Variant C Result | Actionable? |\n",
    "|--------|-----------------|-------------|\n",
    "| **NHST** | p-value ~0.45, \"fail to reject\" | No — inconclusive |\n",
    "| **Bayesian** | P(non-inferior) > 95% | Yes — non-inferiority established |\n",
    "\n",
    "The Bayesian approach succeeds because it:\n",
    "1. Uses a **weakly informative prior** reflecting domain knowledge (variants should perform \"around\" the control rate)\n",
    "2. Provides a **direct probability** rather than a p-value\n",
    "3. Works naturally with the **small, unbalanced samples** typical of product launches\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "\n",
    "- Christensen, R. et al., *Bayesian Ideas and Data Analysis* — practical Bayesian methods\n",
    "  ([CRC Press](https://www.routledge.com/Bayesian-Ideas-and-Data-Analysis/Christensen-Johnson-Branscum-Hanson/p/book/9781439803547))\n",
    "- FDA guidance on non-inferiority trial design:\n",
    "  ([FDA Non-Inferiority Guidance](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/non-inferiority-clinical-trials-establish-effectiveness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Part 4: Select Best Variant\n",
    "\n",
    "---\n",
    "\n",
    "## The Problem NHST Was Not Designed For\n",
    "\n",
    "NHST was built for **asymmetric** questions: \"Is this drug better than placebo?\" It struggles with **symmetric** questions: \"Which of A, B, C is best?\"\n",
    "\n",
    "| NHST Approach | Problem |\n",
    "|---|---|\n",
    "| **Winner-takes-all** (highest observed rate) | Ignores uncertainty; easily picks wrong variant with small samples |\n",
    "| **Pairwise t-tests + Bonferroni** | Very conservative (higher Type II error); only gives significant/not-significant |\n",
    "| **ANOVA + post-hoc** | Only says \"something differs\" — not which is best or by how much |\n",
    "| **Confidence interval overlap** | Inconclusive; overlapping CIs don't mean \"no difference\" |\n",
    "\n",
    "**None of these directly answer**: \"What is the probability that variant A is the best?\"\n",
    "\n",
    "---\n",
    "\n",
    "## Bayesian: Direct Probability of Being Best\n",
    "\n",
    "The Bayesian framework answers the question we actually care about:\n",
    "\n",
    "1. Compute the **posterior Beta distribution** for each variant\n",
    "2. Draw a large number of samples (e.g., 100k) from each posterior via **Monte Carlo simulation**\n",
    "3. For each draw, identify which variant has the highest conversion rate\n",
    "4. Report: $P(A \\text{ is best}), \\; P(B \\text{ is best}), \\; P(C \\text{ is best})$\n",
    "\n",
    "**Advantages**:\n",
    "- **Direct answer**: \"Variant A is best with 88% probability\"\n",
    "- **No multiple-comparison corrections** — single coherent analysis\n",
    "- **Scales naturally** to any number of variants\n",
    "- **Quantifies uncertainty** — not just yes/no\n",
    "- **Business-friendly** — easy to factor in risk, cost, implementation difficulty\n",
    "\n",
    "> See `ABmethodologies.ipynb` cells 42-50 and `Bayesian_AB_Test_Workflow.ipynb` cells 9-12 for the full analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayesian import select_best_variant\n",
    "from plotting_utils import plot_multiple_posteriors_comparison\n",
    "\n",
    "# Select the best variant using Monte Carlo simulation\n",
    "selection = select_best_variant(\n",
    "    variants_data=variants,\n",
    "    alpha_prior=1,   # Non-informative prior for fair comparison\n",
    "    beta_prior=1,\n",
    "    credible_level=0.95,\n",
    "    n_simulations=100000\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Probability Each Variant is Best\")\n",
    "print(\"=\" * 50)\n",
    "for name in ['A', 'B', 'C']:\n",
    "    prob = selection['probabilities'][name]\n",
    "    bar = '#' * int(prob * 50)\n",
    "    print(f\"  P({name} is best) = {prob:.2%}  {bar}\")\n",
    "\n",
    "winner = selection['best_variant']\n",
    "print(f\"\\nWinner: Variant {winner}\")\n",
    "print(f\"  Probability of being best: {selection['probabilities'][winner]:.2%}\")\n",
    "print(f\"  Posterior mean: {selection['posterior_means'][winner]:.2%}\")\n",
    "ci = selection['credible_intervals'][winner]\n",
    "print(f\"  95% Credible interval: [{ci[0]:.2%}, {ci[1]:.2%}]\")\n",
    "print(f\"  Expected loss: {selection['expected_loss'][winner]:.4f}\")\n",
    "\n",
    "# Visualize posterior distributions\n",
    "posteriors = {}\n",
    "for name, data in variants.items():\n",
    "    alpha_post = data['x'] + 1\n",
    "    beta_post = data['n'] - data['x'] + 1\n",
    "    posteriors[name] = {\n",
    "        'alpha': alpha_post,\n",
    "        'beta': beta_post,\n",
    "        'mean': alpha_post / (alpha_post + beta_post),\n",
    "        'ci_95': (beta_dist.ppf(0.025, alpha_post, beta_post),\n",
    "                  beta_dist.ppf(0.975, alpha_post, beta_post))\n",
    "    }\n",
    "\n",
    "fig, ax = plot_multiple_posteriors_comparison(\n",
    "    posteriors=posteriors,\n",
    "    control_group_conversion_rate=control_rate,\n",
    "    epsilon=epsilon\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "# Part 5: Practical Issues in Large Corporate Environments\n",
    "\n",
    "---\n",
    "\n",
    "## The Iteration Speed Problem\n",
    "\n",
    "Even after a Bayesian analysis delivers a clear winner, **deploying that winner can take months** in a large organization:\n",
    "\n",
    "### A Real-World Timeline\n",
    "\n",
    "| Date | Event |\n",
    "|------|-------|\n",
    "| **Early November** | Analysis complete. Variant A identified as winner with >88% probability. Decision made to deploy. |\n",
    "| **Mid November** | Release freeze begins (Black Friday / holiday season). No changes allowed. |\n",
    "| **Late November** | Bug discovered in the custom A/B traffic splitter. Needs fix before deployment. |\n",
    "| **December** | Legal review required for the change. Holiday schedules slow approvals. |\n",
    "| **January** | Winning variant finally deployed to 100% of traffic. |\n",
    "\n",
    "**Result**: ~2 months between \"we know the answer\" and \"users benefit from it.\"\n",
    "\n",
    "---\n",
    "\n",
    "## The Bottlenecks\n",
    "\n",
    "1. **Release Engineering**: Code freezes, deployment windows, staging environments, QA cycles\n",
    "2. **Approvals**: Legal, compliance, product management sign-offs\n",
    "3. **Custom Infrastructure**: Bespoke traffic splitters that need manual reconfiguration\n",
    "4. **Organizational Inertia**: Multiple teams need to coordinate for a simple traffic reallocation\n",
    "\n",
    "---\n",
    "\n",
    "## The Cost of Delay\n",
    "\n",
    "Every day we keep showing inferior variants to users:\n",
    "- **Lost conversions** — users see a worse experience than necessary\n",
    "- **Opportunity cost** — the team can't start the next experiment\n",
    "- **Compounding delay** — each experiment in the pipeline waits for the previous one\n",
    "\n",
    "If the winning variant converts 2% better and we see 10,000 users/day, a 60-day delay means ~12,000 lost conversions.\n",
    "\n",
    "---\n",
    "\n",
    "## The Solution: Automate the Decision Loop\n",
    "\n",
    "What if the system could **automatically shift traffic** to better-performing variants — without manual intervention, release cycles, or approvals for each reallocation?\n",
    "\n",
    "This is exactly what **Thompson Sampling** provides.\n",
    "\n",
    "> Instead of: Experiment → Analyze → Decide → Request release → Wait → Deploy → Repeat\n",
    ">\n",
    "> We get: Deploy all variants → Algorithm continuously optimizes traffic → Winner emerges automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "# Part 6: Multi-Armed Bandits and Thompson Sampling\n",
    "\n",
    "---\n",
    "\n",
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "Imagine a casino with **K slot machines** (\"one-armed bandits\"), each with an unknown payout probability. You have a limited budget. How do you maximize total payout?\n",
    "\n",
    "- **Exploration**: Try different machines to learn which is best\n",
    "- **Exploitation**: Play the machine you currently think is best\n",
    "\n",
    "Too much exploration wastes pulls on bad machines. Too much exploitation might miss a better machine.\n",
    "\n",
    "### A/B Testing *Is* a Bandit Problem\n",
    "\n",
    "| Casino | A/B Testing |\n",
    "|--------|-------------|\n",
    "| Slot machines (\"arms\") | Variants (A, B, C, control) |\n",
    "| Pull a lever | Show a variant to a user |\n",
    "| Payout | User converts |\n",
    "| Unknown probability | True conversion rate |\n",
    "| Limited budget | Finite users |\n",
    "\n",
    "**Goal**: Maximize total conversions (not just *identify* the best variant).\n",
    "\n",
    "**Regret**: The difference between what we *would* have achieved always showing the best variant vs. what we *actually* achieved.\n",
    "\n",
    "---\n",
    "\n",
    "## Thompson Sampling: The Algorithm\n",
    "\n",
    "Thompson Sampling is **provably optimal** for minimizing cumulative regret and is **incredibly simple**:\n",
    "\n",
    "For each incoming user:\n",
    "1. **Sample** once from each variant's posterior: $\\theta_i \\sim \\text{Beta}(\\alpha_i, \\beta_i)$\n",
    "2. **Choose** the variant with the highest sample: $i^* = \\arg\\max_i \\theta_i$\n",
    "3. **Show** that variant to the user\n",
    "4. **Observe** the outcome: conversion (1) or not (0)\n",
    "5. **Update** that variant's posterior: $\\alpha_{i^*} \\mathrel{+}= r, \\; \\beta_{i^*} \\mathrel{+}= (1 - r)$\n",
    "\n",
    "**That's it.** Five lines of logic — simpler than any classical statistical test.\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "- **Early on**: Wide posteriors → high variance in samples → more exploration\n",
    "- **Later**: Narrow posteriors → low variance → exploitation of the best variant\n",
    "- **Automatically**: No parameters to tune, no stopping rules, no sample size calculations\n",
    "\n",
    "The algorithm allocates traffic to variant $i$ proportionally to $P(\\text{variant } i \\text{ is best} \\mid \\text{data})$ — which is *exactly* what we computed in the Bayesian best-variant selection above.\n",
    "\n",
    "> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` for the full treatment, simulation code, and production considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Thompson Sampling Simulation ---\n",
    "np.random.seed(42)\n",
    "\n",
    "true_rates = {\n",
    "    'A': 3244 / 4625,  # ~70.1%\n",
    "    'B': 1433 / 2100,  # ~68.2%\n",
    "    'C': 1396 / 2022,  # ~69.0%\n",
    "}\n",
    "\n",
    "def run_thompson_sampling(true_rates, n_users):\n",
    "    \"\"\"Simulate Thompson sampling and return results.\"\"\"\n",
    "    variants_list = list(true_rates.keys())\n",
    "    alpha = {v: 1 for v in variants_list}\n",
    "    beta = {v: 1 for v in variants_list}\n",
    "    n_shown = {v: 0 for v in variants_list}\n",
    "    n_conv = {v: 0 for v in variants_list}\n",
    "    total_conv = 0\n",
    "    \n",
    "    history = {'user': [], 'prob_A': [], 'prob_B': [], 'prob_C': []}\n",
    "    \n",
    "    for uid in range(n_users):\n",
    "        samples = {v: np.random.beta(alpha[v], beta[v]) for v in variants_list}\n",
    "        chosen = max(samples, key=samples.get)\n",
    "        converted = int(np.random.random() < true_rates[chosen])\n",
    "        \n",
    "        alpha[chosen] += converted\n",
    "        beta[chosen] += (1 - converted)\n",
    "        n_shown[chosen] += 1\n",
    "        n_conv[chosen] += converted\n",
    "        total_conv += converted\n",
    "        \n",
    "        if uid % 50 == 0:\n",
    "            mc = 10000\n",
    "            counts = {v: 0 for v in variants_list}\n",
    "            for _ in range(mc):\n",
    "                s = {v: np.random.beta(alpha[v], beta[v]) for v in variants_list}\n",
    "                counts[max(s, key=s.get)] += 1\n",
    "            history['user'].append(uid)\n",
    "            for v in variants_list:\n",
    "                history[f'prob_{v}'].append(counts[v] / mc)\n",
    "    \n",
    "    return n_shown, n_conv, total_conv, history\n",
    "\n",
    "def run_fixed_allocation(true_rates, n_users):\n",
    "    \"\"\"Simulate fixed equal allocation.\"\"\"\n",
    "    variants_list = list(true_rates.keys())\n",
    "    n_shown = {v: 0 for v in variants_list}\n",
    "    n_conv = {v: 0 for v in variants_list}\n",
    "    total_conv = 0\n",
    "    for uid in range(n_users):\n",
    "        chosen = variants_list[uid % len(variants_list)]\n",
    "        converted = int(np.random.random() < true_rates[chosen])\n",
    "        n_shown[chosen] += 1\n",
    "        n_conv[chosen] += converted\n",
    "        total_conv += converted\n",
    "    return n_shown, n_conv, total_conv\n",
    "\n",
    "n_users = 5000\n",
    "\n",
    "# Run both strategies\n",
    "ts_shown, ts_conv, ts_total, ts_history = run_thompson_sampling(true_rates, n_users)\n",
    "fx_shown, fx_conv, fx_total = run_fixed_allocation(true_rates, n_users)\n",
    "\n",
    "best = max(true_rates, key=true_rates.get)\n",
    "optimal = n_users * true_rates[best]\n",
    "\n",
    "print(\"THOMPSON SAMPLING vs FIXED ALLOCATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'':20s} {'Thompson':>12s} {'Fixed':>12s}\")\n",
    "print(\"-\" * 60)\n",
    "for v in ['A', 'B', 'C']:\n",
    "    ts_pct = 100 * ts_shown[v] / n_users\n",
    "    fx_pct = 100 * fx_shown[v] / n_users\n",
    "    print(f\"Variant {v} traffic:    {ts_pct:10.1f}%  {fx_pct:10.1f}%\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total conversions:  {ts_total:10d}   {fx_total:10d}\")\n",
    "print(f\"Conversion rate:    {100*ts_total/n_users:10.2f}%  {100*fx_total/n_users:10.2f}%\")\n",
    "print(f\"Regret:             {optimal - ts_total:10.0f}   {optimal - fx_total:10.0f}\")\n",
    "print(f\"\\nThompson Sampling gained {ts_total - fx_total:.0f} extra conversions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: P(variant is best) over time\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(ts_history['user'], ts_history['prob_A'], label='P(A is best)', lw=2, color='#2ecc71')\n",
    "ax.plot(ts_history['user'], ts_history['prob_B'], label='P(B is best)', lw=2, color='#e74c3c')\n",
    "ax.plot(ts_history['user'], ts_history['prob_C'], label='P(C is best)', lw=2, color='#3498db')\n",
    "ax.axhline(y=0.95, color='gray', ls='--', lw=1, alpha=0.5, label='95% threshold')\n",
    "ax.set_xlabel('Number of Users', fontsize=12)\n",
    "ax.set_ylabel('Probability of Being Best', fontsize=12)\n",
    "ax.set_title('Thompson Sampling: Learning Which Variant is Best Over Time', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find when 95% confidence reached\n",
    "for i, p in enumerate(ts_history['prob_A']):\n",
    "    if p >= 0.95:\n",
    "        print(f\"Reached 95% confidence that A is best after ~{ts_history['user'][i]:,} users\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"Did not reach 95% confidence within {n_users:,} users (but traffic was already optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Key Benefits of Thompson Sampling\n",
    "\n",
    "### Dynamic Traffic Allocation\n",
    "Thompson Sampling **automatically** routes more traffic to better-performing variants. Inferior variants naturally fade out without manual intervention.\n",
    "\n",
    "### Adding New Variants Dynamically\n",
    "One of the greatest practical advantages: **new variants can enter at any time**.\n",
    "\n",
    "1. New variant arrives → initialize with prior Beta(1, 1)\n",
    "2. It immediately competes in sampling with existing variants\n",
    "3. Wide posterior → gets explored (sometimes samples high → gets traffic)\n",
    "4. Good variants prove themselves; bad ones fade out\n",
    "\n",
    "No need to stop the test, redistribute traffic, recalculate sample sizes, or worry about multiple comparisons.\n",
    "\n",
    "> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` cells 13-15 for a full simulation of adding variant D mid-experiment.\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "When implementing Thompson Sampling in production, there are important real-world considerations beyond the basic algorithm:\n",
    "\n",
    "| Consideration | Issue | Solution |\n",
    "|---|---|---|\n",
    "| **Delayed feedback** | Conversion may happen minutes/hours after variant shown | Batch updates (every 10-60 min); weakly informative priors reduce early regret |\n",
    "| **Non-stationarity** | Conversion rates drift over time (seasonality, product changes) | Exponential decay or sliding window on observations |\n",
    "| **Scalability** | High-traffic systems need low-latency decisions | Store $(\\alpha, \\beta)$ in distributed cache; batch posterior updates |\n",
    "| **Cold start** | New variants start with no data | Weakly informative priors; accept initial exploration phase |\n",
    "\n",
    "> See `ThompsonSampling_DynamicTrafficAllocation.ipynb` Appendix for detailed treatment of delayed feedback, non-stationarity, and production architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "---\n",
    "\n",
    "## The Journey\n",
    "\n",
    "| Step | Manual / NHST | Bayesian / Automated |\n",
    "|------|--------------|---------------------|\n",
    "| **1. Non-inferiority** | NHST: \"fail to reject\" (inconclusive) | Bayesian: \"95%+ probability of non-inferiority\" |\n",
    "| **2. Best variant** | Pairwise tests + corrections (complex, conservative) | Monte Carlo: \"Variant A is best with 88% probability\" |\n",
    "| **3. Deploy winner** | Manual release cycle (weeks/months) | Thompson Sampling: automatic, continuous optimization |\n",
    "| **4. Add new variant** | Stop test, redesign, restart | Add anytime — algorithm adapts seamlessly |\n",
    "\n",
    "---\n",
    "\n",
    "## The Full Comparison\n",
    "\n",
    "| Aspect | Traditional A/B (NHST) | Thompson Sampling |\n",
    "|--------|----------------------|-------------------|\n",
    "| Traffic allocation | Fixed (e.g., 33/33/33) | Dynamic (adapts to performance) |\n",
    "| Total conversions | Suboptimal (wastes traffic) | Near-optimal (minimizes regret) |\n",
    "| Time to decision | Wait for significance | Continuous improvement |\n",
    "| Adding variants | Restart test | Add anytime |\n",
    "| Removing variants | Manual rebalance | Automatic fade-out |\n",
    "| Multiple comparisons | Need corrections | No problem |\n",
    "| Stopping rule | Pre-determined | Flexible |\n",
    "| Implementation | Complex statistics | 5 lines of code |\n",
    "\n",
    "---\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "For modern product development with rapid iteration cycles, risk-averse traffic allocation, and multiple design options:\n",
    "\n",
    "1. **Use Bayesian methods** for non-inferiority testing and variant selection — they work with small samples, provide actionable probabilities, and handle unbalanced designs naturally.\n",
    "\n",
    "2. **Use Thompson Sampling** to automate the entire experimentation loop — it minimizes regret, adapts traffic dynamically, and eliminates the release-engineering bottleneck.\n",
    "\n",
    "3. **Start simple** — the mathematical foundations (Beta-Binomial conjugacy) are elegant and the code is minimal. A production MVP can be built with a distributed cache for $(\\alpha, \\beta)$ parameters and a few lines of sampling logic.\n",
    "\n",
    "---\n",
    "\n",
    "## Companion Notebooks\n",
    "\n",
    "| Notebook | Content |\n",
    "|----------|---------|\n",
    "| `ABmethodologies.ipynb` | Full mathematical derivation of NHST and Bayesian approaches, numerical examples, plotting utilities |\n",
    "| `Bayesian_AB_Test_Workflow.ipynb` | Concise Bayesian workflow: non-inferiority test → variant selection, with utility functions |\n",
    "| `ThompsonSampling_DynamicTrafficAllocation.ipynb` | Thompson Sampling algorithm, simulations, dynamic variant addition, production considerations |\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "### Books\n",
    "- Jaynes, E.T. *Probability Theory: The Logic of Science* — [Cambridge University Press](https://www.cambridge.org/core/books/probability-theory/9CA08E224FF30123304E6D8935CF1A99)\n",
    "- Gelman, A. et al. *Bayesian Data Analysis* (3rd ed.) — [Columbia University](http://www.stat.columbia.edu/~gelman/book/)\n",
    "- Kruschke, J.K. *Doing Bayesian Data Analysis* — [Academic Press](https://sites.google.com/site/doingbayesiandataanalysis/)\n",
    "\n",
    "### Papers\n",
    "- Ioannidis, J.P.A. (2005) \"Why Most Published Research Findings Are False\" — [PLOS Medicine](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0020124)\n",
    "- Thompson, W.R. (1933) \"On the Likelihood that One Unknown Probability Exceeds Another in View of the Evidence of Two Samples\" — [Biometrika](https://doi.org/10.1093/biomet/25.3-4.285)\n",
    "- Chapelle, O. & Li, L. (2011) \"An Empirical Evaluation of Thompson Sampling\" — [NeurIPS](https://papers.nips.cc/paper/2011/hash/e53a0a2978c28872a4505bdb51db06dc-Abstract.html)\n",
    "- Russo, D.J. et al. (2018) \"A Tutorial on Thompson Sampling\" — [Foundations and Trends in Machine Learning](https://arxiv.org/abs/1707.02038)\n",
    "- Agrawal, S. & Goyal, N. (2012) \"Analysis of Thompson Sampling for the Multi-armed Bandit Problem\" — [COLT](https://arxiv.org/abs/1111.1797)\n",
    "\n",
    "### Industry & Regulatory\n",
    "- FDA Draft Guidance (2026): \"Use of Bayesian Methodology in Clinical Trials\" — [FDA](https://www.fda.gov/regulatory-information/search-fda-guidance-documents/use-bayesian-methodology-clinical-trials-drugs-and-biological-products)\n",
    "- Optimizely Stats Engine — [Optimizely](https://www.optimizely.com/optimization-glossary/stats-engine/)\n",
    "- VWO Bayesian A/B Testing — [VWO](https://vwo.com/bayesian-ab-testing/)\n",
    "- Google \"Multi-Armed Bandits\" (2024) — [Google AI Blog](https://ai.googleblog.com/)\n",
    "\n",
    "### Tutorials\n",
    "- Cam Davidson-Pilon, *Bayesian Methods for Hackers* — [GitHub / online book](https://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/)\n",
    "- Lilian Weng, \"The Multi-Armed Bandit Problem and Its Solutions\" — [Blog post](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
