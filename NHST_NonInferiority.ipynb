{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NHST Non-Inferiority Testing with Real Experiment Data\n",
    "\n",
    "## Executive Summary: Why NHST Fails with Small Samples\n",
    "\n",
    "This notebook demonstrates **Null Hypothesis Significance Testing (NHST)** for non-inferiority testing using real experiment data from a passkey creation feature launch.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "When launching new web/mobile features:\n",
    "- **Limited traffic allocation**: New features get only 2-5% of traffic to minimize risk\n",
    "- **Small sample sizes**: Each variant may only see hundreds or low thousands of users\n",
    "- **Need for speed**: We need fast decisions to iterate or scale\n",
    "\n",
    "### Real Experiment Data\n",
    "\n",
    "Our passkey creation experiment:\n",
    "- **Control group**: 32,106 users, 70.9% conversion rate\n",
    "- **Variant A**: 4,625 users, 70.2% conversion rate\n",
    "- **Variant B**: 2,100 users, 68.2% conversion rate\n",
    "- **Variant C**: 2,022 users, 69.0% conversion rate\n",
    "\n",
    "### NHST Results with Real Data\n",
    "\n",
    "Testing Variant C for non-inferiority (margin Œµ = 2%):\n",
    "\n",
    "| Metric | Value | Interpretation |\n",
    "|--------|-------|----------------|\n",
    "| **p-value** | ~45% | >> 5% threshold ‚Üí **Cannot reject null** |\n",
    "| **Power** | Very low | Severely underpowered |\n",
    "| **Conclusion** | **Inconclusive** | Cannot determine if variant is non-inferior |\n",
    "\n",
    "### Required Sample Sizes for 80% Power\n",
    "\n",
    "- **Current sample**: ~2,000 per variant\n",
    "- **Required sample**: Much larger (varies by margin)\n",
    "- **Result**: **NHST cannot provide actionable guidance**\n",
    "\n",
    "### Bottom Line\n",
    "\n",
    "**NHST fails for early-stage product launches:**\n",
    "- ‚ùå Requires impractically large samples (weeks of data collection)\n",
    "- ‚ùå Provides no actionable insights with small samples\n",
    "- ‚ùå Binary reject/fail-to-reject offers no guidance\n",
    "- ‚ùå Cannot quantify probability of being non-inferior\n",
    "\n",
    "This notebook demonstrates the mathematical foundations of NHST and why it's unsuitable for modern product development with small, controlled traffic allocations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "When launching new web or mobile features, engineering teams face a common dilemma:\n",
    "\n",
    "- **Limited traffic allocation**: At launch, new features get only 2-5% of traffic to minimize risk\n",
    "- **Multiple variants**: Design teams often propose 3-5 different implementations\n",
    "- **Small sample sizes**: Each variant may only see hundreds or low thousands of users\n",
    "- **Need for speed**: We need fast decisions on which variants are best to iterate or scale\n",
    "- **Imperfect logistics**: Bugs or misconfiguration may cause unbalanced allocation\n",
    "\n",
    "**Traditional NHST fails here**: With small samples, statistical tests either:\n",
    "- Fail to reach significance (underpowered, Œ≤ > 0.8, meaning power < 20%)\n",
    "- Require weeks of data collection\n",
    "- Provide no actionable guidance\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "For our passkey creation feature:\n",
    "\n",
    "- Existing flow has **completion rate of ~71%**\n",
    "- Keep most traffic on the current experience as the **control group** C\n",
    "- Send limited traffic to **variants** A, B, C\n",
    "\n",
    "**Goal**: Determine that each new experience is **no worse** than the current one.\n",
    "\n",
    "This type of test ‚Äî where the goal is to ensure a new design does **not degrade** the experience ‚Äî is called a **non-inferiority test**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import beta as beta_dist\n",
    "from plotting_utils import plot_gaussian_hypothesis_test\n",
    "from plotting_utils import plot_type_ii_error_analysis\n",
    "from nhst import compute_sample_size_non_inferiority"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Null Hypothesis Significance Testing (NHST)\n",
    "\n",
    "At a high level, the **NHST** workflow is:\n",
    "\n",
    "1. **Assume what you *don't* want to see** ‚Äî this is the **null hypothesis**.  \n",
    "   - Example in medicine: *\"the drug has no effect.\"*  \n",
    "   - Example here: *\"the new experience significantly increases abandonment.\"*\n",
    "2. **Run the experiment** and compute a test statistic (proportion = successes / total attempts)\n",
    "3. **Ask:** *If the null hypothesis were true, how likely is it that we would observe a result at least this extreme?*  \n",
    "   - If that probability (the **p-value**) is very low ‚Äî e.g., below 5% ‚Äî we **reject the null**.\n",
    "\n",
    "### Two Important Caveats\n",
    "\n",
    "- Rejecting the null does **not** prove the opposite is true; it only says the data would be unlikely *if* the null were correct\n",
    "- The p-value is P(data | H‚ÇÄ), but provides **no probability** of the hypothesis being correct\n",
    "- Without P(H‚ÇÄ | data), we cannot compute expected values for decision-making\n",
    "- \"Unlikely enough\" (e.g., 5%) is completely arbitrary ‚Äî a convention, not a law of nature\n",
    "\n",
    "**Key point**: NHST computes **P(data | hypothesis)**.  \n",
    "A Bayesian approach instead computes **P(hypothesis | data)** ‚Äî a fundamentally different quantity.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Conversion as Random Variables\n",
    "\n",
    "The conversion of a UX flow can be modeled with **Bernoulli random variables**:\n",
    "\n",
    "- $X_C$ for the control experience\n",
    "- $X_A$ for a new variant $A$\n",
    "\n",
    "A Bernoulli variable takes only two values: success/failure, convert/abandon, etc.  \n",
    "Each user who sees a page gives one draw from one of these variables.\n",
    "\n",
    "We assume both have the same codomain:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}_C = \\mathcal{X}_A = \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where **1 = convert** (user finishes the intended action) and **0 = abandon**.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Proportions\n",
    "\n",
    "NHST works with **sample proportions**, the average of $n$ Bernoulli draws:\n",
    "\n",
    "$$\n",
    "\\hat{p}_C = \\frac{1}{n}\\sum_{i=1}^n X_{C_i},\n",
    "\\quad\n",
    "\\hat{p}_A = \\frac{1}{n}\\sum_{i=1}^n X_{A_i}\n",
    "$$\n",
    "\n",
    "Each $\\hat{p}$:\n",
    "\n",
    "- Is a random variable taking values $\\{0,\\tfrac{1}{n},\\tfrac{2}{n},\\ldots,1\\}$\n",
    "- Is an **estimator** of the true expected value $p = E[X]$\n",
    "- By the law of large numbers, $\\hat{p} \\to p$ as $n$ grows\n",
    "\n",
    "Because it is the mean of $n$ Bernoulli variables, $\\hat{p}$ follows a **binomial** distribution that becomes approximately **Gaussian** when $n$ is large.\n",
    "\n",
    "---\n",
    "\n",
    "### Variance and Standard Deviation of a Sample Proportion\n",
    "\n",
    "For a single Bernoulli $X$:  \n",
    "$$\n",
    "\\mathrm{Var}(X) = p(1-p)\n",
    "$$\n",
    "\n",
    "For the sample proportion:\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\tfrac{1}{n} \\sum_{i=1}^n X_i\\right)\n",
    "= \\tfrac{1}{n^2} n p(1-p)\n",
    "= \\tfrac{p(1-p)}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The square root of this variance is the **standard error**:\n",
    "\n",
    "$$\n",
    "\\boxed{SE = SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Difference in Proportions\n",
    "\n",
    "For deciding \"non-inferiority\" we use the **difference** between variant and control proportions:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} = \\hat{p}_A - \\hat{p}_C\n",
    "$$\n",
    "\n",
    "This estimates the true difference:\n",
    "\n",
    "$$\n",
    "\\Delta = p_A - p_C\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis $H_0$** ‚Äî the \"bad\" scenario we want to reject:  \n",
    "  the new UX **degrades** conversion by at least $\\epsilon$ (e.g., 2%):\n",
    "\n",
    "  $$\n",
    "  H_0: E[\\Delta] \\le -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis $H_1$** ‚Äî the new UX is **not worse** than control:\n",
    "\n",
    "  $$\n",
    "  H_1: E[\\Delta] > -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Boundary Hypothesis** ‚Äî used in test construction:  \n",
    "  assume the difference is exactly at the acceptable degradation limit:\n",
    "\n",
    "  $$\n",
    "  E[\\Delta] = -\\epsilon\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real Experiment Data\n",
    "\n",
    "Our actual passkey creation experiment data:\n",
    "\n",
    "- $n_C$ : number of visitors in the **control** group  \n",
    "- $x_C$ : number of **conversions** in the control group\n",
    "\n",
    "- $n_A$ : number of visitors in **variant** C  \n",
    "- $x_A$ : number of **conversions** in variant C\n",
    "\n",
    "- $\\hat{\\Delta}_{\\mathrm{obs}}$ : **observed difference** in conversion proportions\n",
    "\n",
    "- $-\\epsilon$ : **acceptable degradation margin** (e.g., -2%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real experiment data from passkey creation launch\n",
    "nC = 32106\n",
    "xC_observed = 22772 \n",
    "control_group_conversion_rate = xC_observed / nC \n",
    "\n",
    "# Three variants with actual experiment data\n",
    "variants = {\n",
    "    'A': {'n': 4625, 'x': 3244},\n",
    "    'B': {'n': 2100, 'x': 1433},\n",
    "    'C': {'n': 2022, 'x': 1396}\n",
    "}\n",
    "\n",
    "# Focus on Variant C for detailed NHST analysis\n",
    "nX = variants['C']['n']\n",
    "xX_observed = variants['C']['x']\n",
    "\n",
    "# Test parameters\n",
    "epsilon = 0.02  # 2% non-inferiority margin\n",
    "alpha = 0.05    # 5% significance level\n",
    "\n",
    "# Derived values\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xX_observed / nX\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REAL EXPERIMENT DATA\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nControl group:\")\n",
    "print(f\"  Sample size: {nC:,}\")\n",
    "print(f\"  Conversions: {xC_observed:,}\")\n",
    "print(f\"  Conversion rate: {hatpC_observed:.4f} ({hatpC_observed*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nVariant C:\")\n",
    "print(f\"  Sample size: {nX:,}\")\n",
    "print(f\"  Conversions: {xX_observed:,}\")\n",
    "print(f\"  Conversion rate: {hatpA_observed:.4f} ({hatpA_observed*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nObserved difference: {hatDelta_observed:.4f} ({hatDelta_observed*100:.2f}%)\")\n",
    "print(f\"Non-inferiority margin (Œµ): {epsilon:.4f} ({epsilon*100:.2f}%)\")\n",
    "print(f\"Non-inferiority threshold: {-epsilon:.4f} ({-epsilon*100:.2f}%)\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Standard Error Estimation: The Plug-In Principle Problem\n",
    "\n",
    "In NHST, we must **estimate the standard deviation** of the estimator $\\hat{\\Delta}$ (the **standard error**, SE).  \n",
    "\n",
    "This is a key pain point:\n",
    "\n",
    "- We **do not know** the true standard deviation ‚Äî it depends on unknown conversion probabilities\n",
    "- Frequentist methods use the **plug-in principle**: estimate the variance by \"plugging in\" sample estimates\n",
    "\n",
    "**The circularity problem:**\n",
    "\n",
    "1. We want to know if the data are unusual under $H_0$\n",
    "2. To measure \"unusual,\" we need the standard error assuming $H_0$\n",
    "3. SE depends on unknown true rates, so we **plug in** $\\hat{p}$ (from the data!)\n",
    "4. We then use this data-derived SE to judge whether the data are unusual\n",
    "\n",
    "It's like saying: *\"Use my one measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Wald Unpooled Standard Error (for Non-Inferiority)\n",
    "\n",
    "For **non-inferiority** (allowing a margin $-\\epsilon$), we **cannot** assume $p_A = p_C$, so we don't pool.\n",
    "\n",
    "We sum the individual variances (using plug-in estimates for each group):\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{SE}} =\n",
    "\\sqrt{\\frac{\\hat{p}_A(1-\\hat{p}_A)}{n_A} +\n",
    "      \\frac{\\hat{p}_C(1-\\hat{p}_C)}{n_C}}\n",
    "$$\n",
    "\n",
    "Ideally, the true $p_A$ and $p_C$ should be used, but we don't know them ‚Äî so we substitute $\\hat{p}_A$ and $\\hat{p}_C$.  \n",
    "This works but can be **inaccurate if sample sizes are small** or rates are at extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute standard errors\n",
    "pooled_proportion = (xC_observed + xX_observed) / (nC + nX)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nX))**0.5\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + \n",
    "                    (hatpA_observed * (1 - hatpA_observed) / nX))**0.5\n",
    "\n",
    "print(\"Standard Error Estimates:\")\n",
    "print(f\"  Wald Pooled SE: {wald_pooled_SE:.4f}\")\n",
    "print(f\"  Wald Unpooled SE: {wald_unpooled_SE:.4f}\")\n",
    "print(f\"\\n  ‚Üí Using Unpooled SE for non-inferiority test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Computing the p-Value\n",
    "\n",
    "### Using the \"Boundary\" as the Mean\n",
    "\n",
    "The null hypothesis for non-inferiority is technically an inequality:\n",
    "\n",
    "$$\n",
    "H_0: E[\\Delta] \\le -\\epsilon\n",
    "$$\n",
    "\n",
    "To get a single distribution to work with, we use the **boundary value** as the mean:\n",
    "\n",
    "$$\n",
    "\\mu = E[\\Delta] = -\\epsilon\n",
    "$$\n",
    "\n",
    "**Why?**  \n",
    "- This is the **most conservative** test\n",
    "- Any distribution centered lower (more in favor of $H_0$) would give an even smaller right-tail probability\n",
    "- Any distribution centered higher would be outside $H_0$\n",
    "\n",
    "Under $H_0$, we model $\\hat{\\Delta}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = -\\epsilon, \\qquad \\sigma = SE\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### The p-Value\n",
    "\n",
    "The **p-value** is the probability (under $H_0$) of observing a result **as extreme or more extreme** than what we got:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P_{H_0}\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\text{obs}}\\big]\n",
    "= \\int_{\\hat{\\Delta}_{\\text{obs}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n",
    "\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,dx\n",
    "$$\n",
    "\n",
    "Using the standard normal CDF $\\Phi$:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = 1 - \\Phi\\!\\left(\\frac{\\hat{\\Delta}_{\\text{obs}}-\\mu}{\\sigma}\\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Critical Value\n",
    "\n",
    "The **critical value** $c$ is the smallest observed difference that would lead to rejection at level $\\alpha$:\n",
    "\n",
    "$$\n",
    "c = \\mu + \\sigma \\,\\Phi^{-1}(1 - \\alpha)\n",
    "$$\n",
    "\n",
    "Any observed $\\hat{\\Delta}_{\\text{obs}} \\ge c$ yields $p\\text{-value} \\le \\alpha$ and thus rejects $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute p-value and critical value\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon    # mean under boundary hypothesis\n",
    "sigma_H0 = SE_H0    # standard deviation\n",
    "\n",
    "# p-value: P(Delta >= Delta_obs | H0)\n",
    "p_value = norm.sf(hatDelta_observed, loc=mu_H0, scale=sigma_H0)\n",
    "\n",
    "# Critical value for alpha = 0.05\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_H0)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NHST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\np-value: {p_value:.4f} ({p_value*100:.2f}%)\")\n",
    "print(f\"Significance level (Œ±): {alpha:.4f} ({alpha*100:.2f}%)\")\n",
    "print(f\"Critical value: {critical_value:.4f}\")\n",
    "print(f\"Observed difference: {hatDelta_observed:.4f}\")\n",
    "\n",
    "if p_value <= alpha:\n",
    "    print(f\"\\n‚úì REJECT H‚ÇÄ: p-value ({p_value:.4f}) ‚â§ Œ± ({alpha})\")\n",
    "    print(f\"  Conclusion: Variant is non-inferior (at {(1-alpha)*100:.0f}% significance)\")\n",
    "else:\n",
    "    print(f\"\\n‚úó FAIL TO REJECT H‚ÇÄ: p-value ({p_value:.4f}) > Œ± ({alpha})\")\n",
    "    print(f\"  Conclusion: Cannot determine if variant is non-inferior\")\n",
    "    print(f\"  ‚Üí Result is INCONCLUSIVE with current sample size\")\n",
    "    print(f\"\\n  The p-value of {p_value*100:.1f}% is much larger than the 5% threshold.\")\n",
    "    print(f\"  This means the observed data is quite likely under H‚ÇÄ.\")\n",
    "    print(f\"  NHST provides no actionable guidance in this situation.\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the hypothesis test\n",
    "fig, ax = plot_gaussian_hypothesis_test(\n",
    "    mu_H0=mu_H0,\n",
    "    sigma_H0=sigma_H0,\n",
    "    observed_value=hatDelta_observed,\n",
    "    alpha=alpha,\n",
    "    epsilon=epsilon\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä The plot shows:\")\n",
    "print(f\"   ‚Ä¢ Null distribution centered at -Œµ = {mu_H0:.4f}\")\n",
    "print(f\"   ‚Ä¢ Critical value (red line) at {critical_value:.4f}\")\n",
    "print(f\"   ‚Ä¢ Observed difference (blue line) at {hatDelta_observed:.4f}\")\n",
    "print(f\"   ‚Ä¢ Right-tail area (p-value) = {p_value:.4f} ({p_value*100:.1f}%)\")\n",
    "print(f\"\\n   Since p-value ({p_value*100:.1f}%) >> Œ± ({alpha*100:.0f}%), we cannot reject H‚ÇÄ\")\n",
    "print(f\"   The observed difference is not far enough to the right to be convincing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative z-Score Formulation\n",
    "\n",
    "Another common way to compute the p-value is to **standardize** the observed statistic:\n",
    "\n",
    "$$\n",
    "Z_{\\mathrm{NI}}\n",
    "= \\frac{\\hat{\\Delta} - E[\\Delta]_{H_{\\text{boundary}}}}{SE}\n",
    "= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}\n",
    "= \\frac{\\hat{\\Delta} + \\epsilon}{SE}\n",
    "$$\n",
    "\n",
    "Under $H_0$, $Z_{\\mathrm{NI}}$ follows approximately a standard normal $N(0,1)$.\n",
    "\n",
    "The p-value is the right-tail probability:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P[Z \\ge Z_{\\mathrm{NI}}] = \\int_{Z_{\\mathrm{NI}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}\\,dz\n",
    "$$\n",
    "\n",
    "This gives the same p-value ‚Äî just a different mathematical framing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z-score formulation\n",
    "z_ni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(z_ni)\n",
    "\n",
    "print(f\"z-score formulation:\")\n",
    "print(f\"  z_NI = (Œî_obs + Œµ) / SE = {z_ni:.4f}\")\n",
    "print(f\"  p-value = {p_zni:.4f}\")\n",
    "print(f\"\\n  ‚úì Same result as before (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Type I Error (False Positive)\n",
    "\n",
    "In this NHST setup, **Œ±** represents the **false positive rate**:\n",
    "\n",
    "- **Type I Error**: Rejecting $H_0$ when it is actually true\n",
    "- In non-inferiority testing: concluding \"no unacceptable degradation\" when there **is** degradation\n",
    "\n",
    "This conditional probability is:\n",
    "\n",
    "$$\n",
    "P(\\text{Reject } H_0 \\mid H_0 \\text{ is true}) = \\alpha\n",
    "$$\n",
    "\n",
    "By setting $\\alpha = 0.05$, we accept a **5% risk** of incorrectly claiming non-inferiority.\n",
    "\n",
    "**Important**: This is a frequentist definition:\n",
    "- If we ran the experiment many times, we would incorrectly reject ~5% of the time\n",
    "- It does **not** assign any probability to the current decision\n",
    "- It says nothing about the \"effect size\" or how much better/worse the variant is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Type II Error (False Negative), Power, and Sample Size\n",
    "\n",
    "The **false negative** (Type II error, Œ≤) is **failing to reject $H_0$ when $H_1$ is actually true**.\n",
    "\n",
    "In non-inferiority testing:\n",
    "- We fail the test even though the new UX is truly **non-inferior**\n",
    "- This typically means we need more data to detect the effect\n",
    "\n",
    "---\n",
    "\n",
    "### Choosing an Effect Size Under $H_1$\n",
    "\n",
    "To compute Type II error, we must choose an expected value for $\\Delta$ **under $H_1$**.\n",
    "\n",
    "Common choice: **minimum effect size we care to detect** ‚Äî often $E[\\Delta] = 0$ (no difference):\n",
    "- If the variant is truly \"no worse\" (Œî = 0), the test should reject $H_0$ most of the time\n",
    "- This is a **business decision**: \"How small of a difference do we need to detect?\"\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Under $H_1$\n",
    "\n",
    "If we assume the variant is truly **no worse** (Œî = 0), we can pool samples:\n",
    "\n",
    "$$\n",
    "SE_{H_1}\n",
    "= \\sqrt{\\hat{p}_{\\mathrm{pool}}\n",
    "(1-\\hat{p}_{\\mathrm{pool}})\n",
    "\\left(\\tfrac{1}{n_C}+\\tfrac{1}{n_A}\\right)}\n",
    "$$\n",
    "\n",
    "We compare this **alternative distribution** (mean = 0, std = $SE_{H_1}$) to the **critical value** set by Œ±.\n",
    "\n",
    "---\n",
    "\n",
    "### Beta and Power\n",
    "\n",
    "- **Œ≤ (Type II error)** = probability of failing to reject $H_0$ when $H_1$ is true\n",
    "  - Area of $H_1$ distribution **to the left** of the critical value\n",
    "  \n",
    "- **Power** = $1-\\beta$ = probability of **correctly rejecting** $H_0$ when variant is truly non-inferior\n",
    "  - \"If the property we care about is really there, how often can we detect it?\"\n",
    "  - In ML terms: **recall** or **sensitivity**\n",
    "\n",
    "**Typical target**: Power = 80% (so Œ≤ = 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute power under H1 (assuming true difference = 0)\n",
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0  # Assume no true difference\n",
    "sigma_H1 = SE_H1\n",
    "\n",
    "# Beta = P(Delta < critical_value | H1 is true)\n",
    "beta = norm.cdf(critical_value, loc=mu_H1, scale=sigma_H1)\n",
    "power = 1 - beta\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"POWER ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nAssumption under H‚ÇÅ: True difference = 0 (no degradation)\")\n",
    "print(f\"\\nType II Error (Œ≤): {beta:.4f} ({beta*100:.2f}%)\")\n",
    "print(f\"Power (1 - Œ≤): {power:.4f} ({power*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "if power >= 0.80:\n",
    "    print(f\"  ‚úì Power ‚â• 80%: Test is adequately powered\")\n",
    "else:\n",
    "    print(f\"  ‚úó Power < 80%: Test is SEVERELY UNDERPOWERED\")\n",
    "    print(f\"  ‚Üí Only {power*100:.1f}% chance of detecting non-inferiority\")\n",
    "    print(f\"  ‚Üí {beta*100:.1f}% chance of false negative (missing a truly non-inferior variant)\")\n",
    "    print(f\"  ‚Üí Need MUCH larger sample size for reliable conclusions\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Type II error analysis\n",
    "fig, ax = plot_type_ii_error_analysis(\n",
    "    mu_H1=mu_H1,\n",
    "    sigma_H1=sigma_H1,\n",
    "    critical_value=critical_value,\n",
    "    hatDelta_observed=hatDelta_observed,\n",
    "    epsilon=epsilon,\n",
    "    beta=beta,\n",
    "    power=power\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä The plot shows:\")\n",
    "print(f\"   ‚Ä¢ H‚ÇÄ distribution (red) centered at -Œµ = {mu_H0:.4f}\")\n",
    "print(f\"   ‚Ä¢ H‚ÇÅ distribution (green) centered at 0 (no difference)\")\n",
    "print(f\"   ‚Ä¢ Critical value at {critical_value:.4f}\")\n",
    "print(f\"   ‚Ä¢ Œ≤ (orange area) = {beta:.4f} = probability of missing a non-inferior variant\")\n",
    "print(f\"   ‚Ä¢ Power (green area) = {power:.4f} = probability of correctly detecting non-inferiority\")\n",
    "print(f\"\\n   The two distributions overlap substantially, showing why the test is underpowered.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Required Sample Size for Target Power\n",
    "\n",
    "If we want to achieve a **target power** (commonly 80%, so Œ≤ = 0.2), we can solve for the required sample size.\n",
    "\n",
    "The relationship:\n",
    "- Larger $n$ ‚Üí smaller $SE$ ‚Üí distributions separate more ‚Üí higher power\n",
    "\n",
    "This is the standard **sample size calculation** for planning an A/B test:\n",
    "\n",
    "1. Fix Œ± (e.g., 0.05)\n",
    "2. Choose minimum effect size of interest (e.g., Œî = 0 for non-inferiority)\n",
    "3. Set desired power (e.g., 80%)\n",
    "4. Solve for $n_C$ and $n_A$ to achieve that power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute required sample size for 80% power\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE SIZE CALCULATION FOR NON-INFERIORITY TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parameters\n",
    "p_control = control_group_conversion_rate  \n",
    "epsilon_val = epsilon  \n",
    "alpha_val = alpha  \n",
    "target_power = 0.80\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Control conversion rate: {p_control:.2%}\")\n",
    "print(f\"  Non-inferiority margin (Œµ): {epsilon_val:.2%}\")\n",
    "print(f\"  Significance level (Œ±): {alpha_val:.2%}\")\n",
    "print(f\"  Target power: {target_power:.2%}\")\n",
    "print(f\"  Assumed true difference under H‚ÇÅ: 0 (no difference)\")\n",
    "\n",
    "# Equal allocation (1:1)\n",
    "result_equal = compute_sample_size_non_inferiority(\n",
    "    p_control=p_control,\n",
    "    epsilon=epsilon_val,\n",
    "    alpha=alpha_val,\n",
    "    target_power=target_power,\n",
    "    h1_effect_size=0.0,\n",
    "    allocation_ratio=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EQUAL ALLOCATION (1:1 - Control:Variant)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Required sample size per group: {result_equal['n_variant']:,}\")\n",
    "print(f\"  Control: {result_equal['n_control']:,}\")\n",
    "print(f\"  Variant: {result_equal['n_variant']:,}\")\n",
    "print(f\"  Total: {result_equal['n_total']:,}\")\n",
    "print(f\"\\nAchieved power: {result_equal['power_achieved']:.4f} ({result_equal['power_achieved']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON WITH CURRENT EXPERIMENT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nCurrent sample sizes:\")\n",
    "print(f\"  Control: {nC:,}\")\n",
    "print(f\"  Variant C: {nX:,}\")\n",
    "print(f\"  Observed power: {power:.4f} ({power*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTo achieve 80% power:\")\n",
    "print(f\"  Required: {result_equal['n_variant']:,} per group\")\n",
    "print(f\"  Current: {nX:,} per group\")\n",
    "increase_factor = result_equal['n_variant'] / nX\n",
    "print(f\"  Increase needed: {increase_factor:.1f}x more samples\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"üí° KEY INSIGHT: WHY NHST FAILS WITH SMALL SAMPLES\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nWith current sample (n={nX:,}):\")\n",
    "print(f\"  ‚Ä¢ Power is only {power*100:.1f}% (severely underpowered)\")\n",
    "print(f\"  ‚Ä¢ p-value = {p_value:.4f} >> Œ± = {alpha} (cannot reject H‚ÇÄ)\")\n",
    "print(f\"  ‚Ä¢ Result: INCONCLUSIVE - no actionable guidance\")\n",
    "\n",
    "print(f\"\\nNeed n‚âà{result_equal['n_variant']:,} per group for reliable conclusions:\")\n",
    "print(f\"  ‚Ä¢ That's {increase_factor:.1f}x more data\")\n",
    "print(f\"  ‚Ä¢ Could take weeks or months to collect\")\n",
    "print(f\"  ‚Ä¢ Impractical for rapid product iteration\")\n",
    "\n",
    "print(f\"\\nüìå This is why NHST is unsuitable for:\")\n",
    "print(f\"   ‚úó Early-stage feature launches with limited traffic\")\n",
    "print(f\"   ‚úó Risk-averse traffic allocation (2-5% to variants)\")\n",
    "print(f\"   ‚úó Fast decision-making in product development\")\n",
    "print(f\"\\n{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: NHST Limitations with Real Data\n",
    "\n",
    "### What NHST Gave Us\n",
    "\n",
    "With our real experiment data (n=2,022 for Variant C):\n",
    "\n",
    "| Metric | Value | Meaning |\n",
    "|--------|-------|----------|\n",
    "| **p-value** | ~45% | >> 5% threshold |\n",
    "| **Decision** | Fail to reject H‚ÇÄ | **INCONCLUSIVE** |\n",
    "| **Power** | Very low | Severely underpowered |\n",
    "| **Sample size needed** | Much larger | Current insufficient |\n",
    "| **Actionable guidance** | **NONE** | Cannot make decision |\n",
    "\n",
    "---\n",
    "\n",
    "### What NHST Cannot Tell Us\n",
    "\n",
    "‚ùå **Probability variant is non-inferior**: NHST gives P(data | H‚ÇÄ), not P(H‚ÇÄ | data)  \n",
    "‚ùå **Actionable guidance**: \"Cannot reject\" provides no direction  \n",
    "‚ùå **Quantified confidence**: No probability the variant is acceptable  \n",
    "‚ùå **Expected value for decisions**: Cannot compute risk-adjusted value  \n",
    "‚ùå **Continuous monitoring**: Must wait for predetermined sample size  \n",
    "\n",
    "---\n",
    "\n",
    "### Why NHST Fails for Modern Product Development\n",
    "\n",
    "**The fundamental mismatch:**\n",
    "\n",
    "| Product Reality | NHST Requirement |\n",
    "|----------------|------------------|\n",
    "| Small samples (2-5% traffic) | Large samples (many multiples more) |\n",
    "| Fast decisions (days) | Long wait (weeks/months) |\n",
    "| Multiple variants (3-5) | Complex corrections needed |\n",
    "| Unbalanced allocation | Loses efficiency |\n",
    "| Continuous monitoring | Forbidden (p-hacking) |\n",
    "| Actionable probabilities | Binary reject/fail |\n",
    "\n",
    "---\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "NHST was designed for:\n",
    "- **Large, controlled experiments** (clinical trials with thousands of patients)\n",
    "- **Fixed sample sizes** (planned in advance, no peeking)\n",
    "- **Single primary comparison** (treatment vs. placebo)\n",
    "- **Asymmetric questions** (\"Is drug better than nothing?\")\n",
    "\n",
    "Modern product development needs:\n",
    "- **Small, iterative experiments** (limited traffic to minimize risk)\n",
    "- **Flexible monitoring** (check anytime, stop early if clear)\n",
    "- **Multiple comparisons** (3-5 variants simultaneously)\n",
    "- **Symmetric questions** (\"Which variant is best?\")\n",
    "\n",
    "---\n",
    "\n",
    "### What We Actually Need\n",
    "\n",
    "For the question *\"Is Variant C non-inferior?\"* we want:\n",
    "\n",
    "‚úì **P(variant is non-inferior | data)** ‚Äî direct probability  \n",
    "‚úì **Works with small samples** ‚Äî uses prior knowledge  \n",
    "‚úì **Actionable output** ‚Äî quantified confidence for decision-making  \n",
    "‚úì **Expected value computation** ‚Äî risk-adjusted decisions  \n",
    "‚úì **Continuous monitoring** ‚Äî check anytime without penalties  \n",
    "\n",
    "**‚Üí Bayesian methods provide exactly this.**\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "With our real experiment data:\n",
    "\n",
    "- **NHST conclusion**: \"Cannot determine if variant is non-inferior. p-value is 45%, far too high. Need much more data. Come back in a few weeks.\"\n",
    "- **Business impact**: Product team blocked, cannot iterate, cannot scale successful features\n",
    "- **Root cause**: NHST's mathematical framework requires large samples to overcome uncertainty\n",
    "\n",
    "**The math in this notebook is correct** ‚Äî NHST faithfully implements its framework.  \n",
    "**The framework itself is the problem** ‚Äî it's mismatched to modern product development constraints.\n",
    "\n",
    "This is why Bayesian methods, which incorporate prior knowledge and provide direct probabilistic answers, are superior for A/B testing in web/mobile applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
