{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AB Test Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Structure of the test\n",
    "\n",
    "We have an existing jounrney which has a completion rate of x% , currently about 20%\n",
    "\n",
    "We are going to keep 70% of the traffic on the legacy/existing pages as a controld group and test n variants $A_1$, $A_2$,..$A_i$. We want to make sure the new proposed pages, do not degrade the journey convertion rates significantly, the goal here is not to improve it just not making it worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis will consider 2 Bernoulli Random Variables (RV) and their difference.\n",
    "\n",
    "$X_C$ being the control control group Bernoulli RV $X_A$ being the version A. They are both assumed to have for codomain $\\mathcal{X}_C=\\mathcal{X}_A=\\{0,1\\}$ which is really a boolean i.e. it converts or does not convert.\n",
    "\n",
    "But we are going to work  with a new random varaible we will call\"sample proportion\"  which is the sum to n Bernoulli ones divided by n and call them  $\\hat{p}_C$ and $\\hat{p}_A$ for the control and the variant respectively , defined as\n",
    "\n",
    " $ \\hat{p}=\\frac{1}{n}\\sum_1^n X_i$ each sample proportion is at the same time:\n",
    "\n",
    "* $\\hat{p}$ is a random variable with codomain $\\{0,\\frac{1}{n},\\frac{2}{n},...\\frac{n}{n}\\}$\n",
    "* $\\hat{p}$ is an estimator of the expected value $E(X)$, with $E(\\hat{p}) = E(X)$ because of the law of large numbers applied to a Bernoulli/Binomial.\n",
    "\n",
    "An estimator is a function from the n-cartesian product of the space of realizations of X (on itself n times)  to the domain of the parameter of interest, if we the notation codomain(X) = $\\mathcal{X}$, it is then a maaping from $\\mathcal{X}^n$ to some real value for a paraemter of original random variable X, in this case $\\hat{p}_A: \\mathcal{X}^n \\rightarrow \\{0,\\frac{1}{n},\\frac{2}{n},...,\\frac{n}{n}\\}$ withe binomial distribution which we can approximate to a gaussian as n grows\n",
    "\n",
    "Note that for a Bernoulli RV X $Var(X) = p (1-p)$ and therefore \n",
    "  \n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}Var(\\sum_1^n X_i)$\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}n(p(1-p))$\n",
    "\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n}(p(1-p))$\n",
    "\n",
    "so $\\boxed{Var(\\hat{p})= \\frac{p(1-p)}{n}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Null Hypothesis (H₀):\n",
    "The conversion rate for the variant is equal or better than the control (p_variant = 20% or better). Note that the null hypothesis is not really \"no effect\" but a \"view of the mind\" that actually no effect or a better better conversion is \"no effect\", \"some effect\" here mean the conversion is degrading ,this what we want to detect, a possible degradation.\n",
    "\n",
    "this can be modeled by stating the random variable $ D = p_A  - p_C $ has $E(D) \\geq 0 $\n",
    "\n",
    "#### Alternative Hypothesis (H₁):\n",
    " The conversion rate for the variant is stricly lower than the control. That is\n",
    "$E(D) \\lt 0 $\n",
    "\n",
    "\n",
    "#### Variance and Standard Deviation of the estimators\n",
    "\n",
    "The we define esimator which is also a random varaible $\\hat{D} = \\hat{p_A} - \\hat{p_C}$\n",
    "\n",
    "Each realization is given after each experience by the folowing counts\n",
    "\n",
    "$n_C$: Number of visitors in the control group per day (700)\n",
    "\n",
    "$x_C$ : Number of conversions in a given control group\n",
    "\n",
    "$n_A$: Number of visitors in each given group per day (100)\n",
    "\n",
    "$x_A$ : Number of conversions in the variant group\n",
    "\n",
    "\n",
    "Calculate the observed conversion rates (estimator), for the version, the control  and the 2 combine (the pool) as under the null hypothesis they can be combined in an estimator as the true conversion parameter is assumed identical\n",
    "\n",
    "the realization of $\\hat{p}_A$  is $(\\hat{p}_A(\\omega) = \\frac{x_A}{n_A})$\n",
    "\n",
    "the associated realization of $\\hat{p}_C$ is  $(\\hat{p}_C(\\omega) = \\frac{x_C}{n_C})$\n",
    "\n",
    "Then there is a theoritical estimator for the combine exeprience IF the null hypothesis was holding and there was not difference A and the control there woudl be a random variable for the estimator of both being pooled\n",
    "\n",
    "$\\hat{p}(\\omega_{pool}) = \\frac{x_A + x_V}{n_A + n_C}$\n",
    "\n",
    "Then we try to compute the variance standard deviation of the difference between the conversion rate of the control vs. the variance. The variance of the difference beteww the 2 proportion is the sum of their variance, so\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = Var(\\hat{p}_C) + Var(\\hat{p}_A)$\n",
    "\n",
    "Here there is the first \"sleight of hand\", we dont really know the true mean of those estimators, but appplying a \"trick\" called the plugs in (it is actually called the plug in principle) the *realization* of the pooled esimtator is used as if it was the true underlying value( so called population true parameter ) in the formula for the standard deviation and then we use the resulitng standard deviation it as THE standard deviation of the estimators, from that SD then we compute a z-score. The justification is a bit hand wavy and assume \"converge\" if many expeirments are run which is obviously questionnable on a single experiement in order to make a decision.\n",
    "\n",
    "Also because both $\\hat{p_C}$ and $\\hat{p_A}$ are Bernoulli RV, and using this \"plug in \" principal; we compute the sum of variance of the estimators also using the pooled sample reading as:\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_A} + \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_C} = \\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})$\n",
    "\n",
    "and the standard error which is just the standard deviation of the same quantity  also using the plug in principl the standard devation of the estimators that statisticain call \"standard error\" is\n",
    "\n",
    "$\\boxed{SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}}$\n",
    "\n",
    "this is sometimnes called \"pooled standard error\" or \"standard error of the difference between two independent proportions\"\n",
    "\n",
    "From this we conmpute the z-score whuich is a dimenionless value counting the number of standard devaition that the difference observed is from the theoritical difference which shoiuld be zero under the null hypothesis (approximated by the pooled one, sktechy). So a simple ivisino of the observed devation over teh standard deviation of hte diference\n",
    "\n",
    "$z= \\frac{\\frac{x_A}{n_A} - \\frac{x_C}{n_C}}{SE}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### P Value, Signifiance Level $\\alpha$ and critical Value\n",
    "\n",
    "the p value can be mechanically computed from the z score using a standard normal distribution\n",
    "\n",
    "Here we are doing a so called one tailed test that is we want to compute the probabilithy of see the given outcome or something \"worse\" given $H_0$ ,  that is with a  given the null hyptothesis and all the machinery we put in place (plug in, pooledl etc...).  Because in turn can be approximated by a gaussian. So the probablity of seeing an outcome as observed, or \"worse\" is eitehr the cumulative or survial function computed at the value observed in the current experiment depending on what we consider \"worse\".\n",
    "\n",
    "We can say that the estimator $\\hat{D} = \\hat{p}_A - \\hat{p}_C$ has for distribution $ N(0,SE)$\n",
    "\n",
    "given the choice of  H0and H1 we’re interested in negative values of z\n",
    "\n",
    "If we set the value observed as  $p_{observed}=\\frac{x_A}{n_A} - \\frac{x_C}{n_C}$. There  many ways to compute it:\n",
    "\n",
    "$p_{value} = \\phi(z)$ where $\\phi$ is the cmulative distribution function of the standard normal distribution and z the z score.\n",
    "\n",
    "a more direct computation is to simply integrate a gaussian representing tje distribution of the estimator, using stanrard deviation of  SE as the paramater for this guassian, that is the cumlative function:\n",
    "\n",
    "$pvalue = p(estimator observed value \\lt proportion observed) = \\int_{-\\infty }^{p_{observed}} \\frac{1}{\\sqrt{2 \\pi}(SE)} e^{-\\frac{x^2}{2(SE)^2}} dx$\n",
    "\n",
    "Note that if x is the difference in proportion it can only range from -1 to 1 in reality, so the guassian approximation can be questionable if the proportion are really close to 0 or 1 or -1\n",
    "\n",
    "The we pick an value $\\alpha$ for the so called \"significance level\" noted $\\alpha$ and in social science oftne setup at 0.05, the value for $\\hat{D}$ which make teh p value equal to $\\alpha$ is called criticxal value and  noted $\\hat{D}_{\\alpha}$, so $p(\\hat{D} < D_{\\alpha}) = \\alpha$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision based on p value and type 1 error (false positive)\n",
    "\n",
    "We will pick a significance level $\\alpha$ , if the probability of observing the data or worse (more negative value of the difference of proportion between A and C) \n",
    "\n",
    "IF $pvalue$ is less than $\\alpha$ we reject H0, that is we say that variant is degrading the experience, if it is above we accept H0 that is no effect that we care about (same or better conversion)\n",
    "\n",
    "$\\alpha$ is also called the proability of type 1 error that is rejecting H0, when it is actually true, also called false positive in this context. if we set $\\alpha$ at 0.05, the false postive rate is 5%, that is we say the variant make things worse while it was atually harmless. The value of $\\hat{p}$ where pvalue is reach is called \n",
    "\n",
    "In our current example the probability of saying that the new design is worse than the legacy/control while it was perfectsly fine will be 5%, we may want it tigher, i.e only accept a 0.05% of mistakenly not taking the new design has it has some other beneft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Type 2 Error (false negative) and Power\n",
    "\n",
    "Type 2 error is the probability of accepting H0  (failing to reject, double negation) when it is actually false, so that would be saying \"variant is harmless\" while it is actually degrading the experience. So in practice here it woudl be we observer a differnec ince conversion postive ot at leat larger than -0.1\n",
    "\n",
    "\n",
    " But the probability of type 2 error can only be assesssed after some kind of mimimum effect size is chose, as wityout it there is no way to model the new mean of estimator disitribution in the alternative hpothesis \n",
    "\n",
    "Let's assume that we only care if hte degradation is at least -0.1 (E(D) = -0.1), that is the variant degrades the conversion by at least 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the estimator under the alternative hypothesis H1, we use the gaussian:\n",
    "\n",
    "$\\hat{D} = \\hat{p}_A - \\hat{p}_C = N(\\delta, SE_{H1}) = N(-0.1, SE_{H1}) $ that is the guassian now in centerd on -0.1, no centered on 0\n",
    "\n",
    "with\n",
    "\n",
    "$SE_{H1} = \\sqrt{\\frac{p_A(1-p_A)}{n_A}+ \\frac{p_C(1-p_C)}{n_C}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we compute the probabilit of the esimator coming up \"higher\" but with the same critical value as it was established ahead of time with the significance level alpha.\n",
    "\n",
    "So the probabilit fo the estimator coming up higher than the significant level is\n",
    "\n",
    "$\\beta = p(\\hat{D} > D_{\\alpha} | E(D)=-0.1)$\n",
    "\n",
    "$ = \\int_{D_{\\alpha}}^{\\infty} \\frac{1}{\\sqrt{2\\pi}SE_{H1}} e^{-\\frac{(x+0.1)^2}{2(SE_{H1})^2}}dx$\n",
    "\n",
    "The Beta is the probability that even though the variant conversion rate is materially worse, we say \"it is fine\"\n",
    "\n",
    "The power $1-\\beta$ is the probability that we say the variant is worse when it happens to be actually worse.\n",
    "\n",
    "In many social since the power is set at 0.8, meaning 0.2 probability not spotting that we degraded the test bu at least  0.1.\n",
    "\n",
    "\n",
    "This computation will give su whatever it gives us, if we want to TARGET a Beta of 0.8, we can conpute the sample sizes (implied in SE) that will make us reach that level.\n",
    "\n",
    "Note there is no really close formed formula so some gaussian approximation and a bunch of plug in hacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
