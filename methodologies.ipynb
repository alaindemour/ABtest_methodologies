{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Test Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Structure of Standard AB test between a Control C and a Variant A\n",
    "\n",
    "We have an existing journey which has a completion rate of x% , currently x=20%\n",
    "\n",
    "We are going to keep 90% of the traffic on the legacy/existing pages as a controld group and test n variants $A$ We want to make sure the new proposed CX is not worse or better than the control, at which point we will switch all traffic to the variants, then rate the variant against each other.\n",
    "\n",
    "The type of AB test where one want to see whether somethig new is not \"making things worse\" i.e. somehow degrading the experience is usually called a \"non inferiority test\" (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST) Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing of the problem using Random Variables\n",
    "\n",
    "The analysis will consider 2 Bernoulli Random Variables $X_C$ and $X_A$ from which we will draw repeadetly (simulating presenting the new CX repeatedly to a bunhc of end users)\n",
    "\n",
    "$X_C$ being the UX and Bernoulli RV presented to control control group  $X_A$ being the variant A presented. They are both assumed to have for codomain $\\mathcal{X}_C=\\mathcal{X}_A=\\{0,1\\}$ that is a boolean i.e. it converts or does not convert. Here \"convert\" means the user perform the action they intended to do vs. abandoning the journey, e.g. create a passkey. There a possiblty with a techincal failure but this would not be their choice so it woudl be counted as a success for the purpose of this AB test.\n",
    "\n",
    "To assess this convertion rate we define a new random varaibles we will call\"sample proportions\"  which is the sum of n instances of the Bernoulli ones defined above, divided by n. For this \"sample proportion\" Let's using the notation  $\\hat{p}_C$ and $\\hat{p}_A$ for the control and the variant respectively , defined as\n",
    "\n",
    " $ \\hat{p_C}=\\frac{1}{n}\\sum_1^n X_{Ci}$ and \n",
    " $ \\hat{p_A}=\\frac{1}{n}\\sum_1^n X_{Ai}$\n",
    " \n",
    " each sample proportion can be interprested as 2 things :\n",
    "\n",
    "* $ \\hat{p_C}$ and  $ \\hat{p_A}$ can be interpresetd as ordinary random variables with codomain $\\{0,\\frac{1}{n},\\frac{2}{n},...\\frac{n}{n}\\}$\n",
    "* $\\hat{p_C}$ and  $ \\hat{p_A}$ can also be intepreteda as estimators of the expected value $E(p_C)$ and $E(p_A)$ , in general regardless of whether it is a control or a variant $E(\\hat{p}) \\rightarrow E(p)$ because of the law of large numbers applied to a Bernoulli/Binomial.\n",
    "\n",
    "An estimator is a function from the n-cartesian product of the space of realizations of X (on itself n times)  to the domain of the parameter of interest, if we  use the notation codomain(X) = $\\mathcal{X}$, it is then a maaping from $\\mathcal{X}^n$ to some real value for a paraemter of original random variable X, in this case $\\hat{p}_A: \\mathcal{X}^n \\rightarrow \\{0,\\frac{1}{n},\\frac{2}{n},...,\\frac{n}{n}\\}$ Because it it is a sum of Bernoullis, the estimator RV follows binomial distribution centered on the expected value. It is always true that a discrete binomial distirubtion can be approximated by a contunous gaussian as n grows (clipping the tails)\n",
    "\n",
    "Note that for a Bernoulli random variable $X$ we have $Var(X) = p (1-p)$ and therefore \n",
    "  \n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}Var(\\sum_1^n X_i)$\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}n(p(1-p))$\n",
    "\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n}(p(1-p))$\n",
    "\n",
    "so $\\boxed{Var(\\hat{p})= \\frac{p(1-p)}{n}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "\n",
    "To make the rest of the discussion easier to read  we are going to define the difference in estimator $\\hat{p_A}$ and $\\hat{p_C}$ as $\\hat\\Delta = \\hat{p_A} - \\hat{p_C}$ which is also an estimator of the expected value of random varaible $\\Delta = p_A - p_C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Null Hypothesis (H₀):\n",
    "\n",
    "\n",
    "As we said earlier the  first possible \"effect\" is if the new passkey creation UX degrades the overall UX so much that it may impacts our other business metrics. In the classical framing of a non-inferiotity test, the  Null Hypothesis is the \"bad thing\" we are tryigng to reject in our case it would mean : \"some degration\".\n",
    "\"some degration\" can be described with a simple inequality like  $E(p_A) \\le E(p_C) - \\epsilon $ where $\\epsilon$ is a tiny degradation we would find acceptable for instance 0.05 (5%).\n",
    "\n",
    "We will rewrite $H_0$ as $\\boxed{H_0 = E(\\Delta) \\leq -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    " The conversion rate for the variant is higher than $\\boxed{E(\\Delta) > -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Numerical example \n",
    "\n",
    "Each realization is given after each experience by the folowing counts\n",
    "\n",
    "$n_C$: Number of visitors in the control group\n",
    "\n",
    "$x_C$ : Number of conversions observed (realization) in a given control group\n",
    "\n",
    "$n_A$: Number of visitors in the variant group\n",
    "\n",
    "$x_A$ : Number of conversions observed (realization) in the variant group\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realization of difference in conversion rate estimator: -0.0400\n",
      "Control group realization of conversion rate estimator: 0.2000\n",
      "Treatment group realization of conversion rate estimator: 0.1600\n"
     ]
    }
   ],
   "source": [
    "control_group_conversion_rate = 0.2\n",
    "nC = 700\n",
    "xC_realization = nC * control_group_conversion_rate\n",
    "nA = 100\n",
    "xA_realization = 16\n",
    "hatpC_realization = xC_realization / nC\n",
    "hatpA_realization = xA_realization  / nA\n",
    "hatDelta_realization = hatpA_realization - hatpC_realization\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_realization:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_realization:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_realization:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the estimator $\\hat{\\Delta}$ (a.k.a Standard Error in frequentist/classic statistics)\n",
    "\n",
    "The NHST methodolgy involve computing (an estimatation of) the standard deviation for the estimator $\\hat{\\Delta}$, then to see how likely it then comparing it with the actual values we get from the difference in proportion in the experiment, then based on this decide if the difference observed is far enough (or close enough) to the theoritical value to reject or not reject the hypothesis based on some accpeted risks in the deicison rules (power, confidence internval)\n",
    "\n",
    "There are several methos to conpute the standard varatio of the estimator of the difference of proporition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### If the null hypotehsis is \"no effect\", use the Wald pooling method:\n",
    "\n",
    "If instead of \"non inferiority\" the null hypothesis $H-0 is \"no effect\", we can record the observed conversion rates (estimators) for each proportion then  combine (pool) as under this very specific null hypothesis they have the exact same expected value so a larger sample created through pooling is a better estimator of the true value, whatever it is.\n",
    "\n",
    "the realization of $\\hat{p}_A$  is $(\\hat{p}_A(\\omega) = \\frac{x_A}{n_A})$\n",
    "\n",
    "the associated realization of $\\hat{p}_C$ is  $(\\hat{p}_C(\\omega) = \\frac{x_C}{n_C})$\n",
    "\n",
    "Then there is a theoritical estimator for the combine exeprience IF the null hypothesis was holding and there was not difference A and the control there woudl be a random variable for the estimator of both being pooled\n",
    "\n",
    "$\\hat{p}(\\omega_{pool}) = \\frac{x_A + x_V}{n_A + n_C}$\n",
    "\n",
    "Then we try to compute the variance standard deviation of the difference between the conversion rate of the control vs. the variance. The variance of the difference beteww the 2 proportion is the sum of their variance, so\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = Var(\\hat{p}_C) + Var(\\hat{p}_A)$\n",
    "\n",
    "Here there is the first \"sleight of hand\", we dont really know the true mean of those estimators, but appplying a \"trick\" called the plugs in (it is actually called the plug in principle) the *realization* of the pooled esimtator is used as if it was the true underlying value( so called population true parameter ) in the formula for the standard deviation and then we use the resulitng standard deviation it as THE standard deviation of the estimators, from that SD then we compute a z-score. The justification is a bit hand wavy and assume \"converge\" if many expeirments are run which is obviously questionnable on a single experiement in order to make a decision.\n",
    "\n",
    "Also because both $\\hat{p_C}$ and $\\hat{p_A}$ are Bernoulli RV, and using this \"plug in \" principal; we compute the sum of variance of the estimators also using the pooled sample reading as:\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_A} + \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_C} = \\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})$\n",
    "\n",
    "and the standard error which is just the standard deviation of the same quantity  also using the plug in principl the standard devation of the estimators that statisticain call \"standard error\" is\n",
    "\n",
    "$\\boxed{WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}}$\n",
    "\n",
    "this is sometimnes called \"pooled standard error\" or \"standard error of the difference between two independent proportions\"\n",
    "\n",
    "From this we conmpute the z-score whuich is a dimenionless value counting the number of standard devaition that the difference observed is from the theoritical difference which shoiuld be zero under the null hypothesis (approximated by the pooled one, sktechy). So a simple ivisino of the observed devation over teh standard deviation of hte diference\n",
    "\n",
    "$z= \\frac{\\frac{x_A}{n_A} - \\frac{x_C}{n_C}}{SE}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When null hyopthesis is \"non inferiority\", use the Wald Unpooled Standard Error:\n",
    "\n",
    "If the null hypotheis is not \"no effect\" we cannot assume that each sample is drawn from teh same random variable, we actually explicitely say it is not because of the epsilon among other thing. So the first quick and dirt approch is just to sum the separate variances (see above variance of Bernoulli) as variances are additive when random variable are added  or substracted in our case, and are assymed to be independent, then take the square roo t pf that sum to get to a standard deviation:\n",
    "\n",
    "$\\hat{WaldUnpooledSE} = \\sqrt{\\frac{\\hat{p_A(w)}(1-\\hat{p_A(w)})}{n_A} + \\frac{\\hat{p_C(w)}(1-\\hat{p_C(w)})}{n_C}}$\n",
    "\n",
    "Note that we should be using hte expected value of $E(p_A)$ and $E(p_B)$ in the formula but we don't know them, so we use the so call \"plug in\" trick and replace them but whatever values we get. That is why this is a quick and dirty approach that can produce questionable results depending on the speicific phenomenon and sizes of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slightly Better for \"non inferiority\"  Newcombe (score-based / Wilson)\n",
    "\n",
    "Much better actual coverage than Wald, especially with imbalanced samples or extreme p.\n",
    "Still fairly simple to compute (closed-form formulas exist, or can be coded).\n",
    "\n",
    "Skips the standard error computation, goes straight to confidence interval TBD "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gold Standard:  Miettinen–Nurminen\n",
    "\n",
    "Very accurate coverage (close to nominal even with small n).\n",
    "Widely used in clinical trials and non-inferiority contexts.\n",
    "Handles one-sided bounds directly (great for NI).\n",
    "\n",
    "Skips the standard error computation, goes straight to confidence interval TBD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wald Pooled Standard Error: 0.0424\n",
      "Wald Unpooled Standard Error: 0.0397\n"
     ]
    }
   ],
   "source": [
    "p_pool_realization = (xC_realization + xA_realization) / (nC + nA)\n",
    "wald_pooled_SE = (p_pool_realization * (1 - p_pool_realization) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_realization * (1 - hatpC_realization) / nC) + (hatpA_realization * (1 - hatpA_realization) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newcombe CI: [-0.1068, 0.0487]\n",
      "Newcombe Effective SEs: 0.0341, 0.0453, 0.0397\n",
      "Miettinen-Nurminen CI: [-0.1042, 0.0483]\n",
      "Miettinen-Nurminen Effective SEs: 0.0328, 0.0451, 0.0389\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "L_newc, U_newc = confint_proportions_2indep(\n",
    "    count1=xA_realization, nobs1=nA, count2=xC_realization, nobs2=nC,\n",
    "    method=\"newcomb\", compare=\"diff\", alpha=alpha\n",
    ")\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "# “Effective” SEs derived from CI half-widths (pedagogical, not part of the method):\n",
    "z_two = norm.ppf(1 - alpha/2)\n",
    "SE_newc_lower = (hatDelta_realization - L_newc) / z_two\n",
    "SE_newc_upper = (U_newc - hatDelta_realization) / z_two\n",
    "SE_newc_sym   = (U_newc - L_newc) / (2*z_two)\n",
    "\n",
    "print(f\"Newcombe CI: [{L_newc:.4f}, {U_newc:.4f}]\")\n",
    "print(f\"Newcombe Effective SEs: {SE_newc_lower:.4f}, {SE_newc_upper:.4f}, {SE_newc_sym:.4f}\")\n",
    "\n",
    "\n",
    "L_mn, U_mn = confint_proportions_2indep(\n",
    "    count1=xA_realization, nobs1=nA, count2=int(xC_realization), nobs2=nC,\n",
    "    method=\"score\", compare=\"diff\", alpha=alpha\n",
    ")\n",
    "\n",
    "SE_mn_lower = (hatDelta_realization - L_mn) / z_two\n",
    "SE_mn_upper = (U_mn - hatDelta_realization) / z_two\n",
    "SE_mn_sym   = (U_mn - L_mn) / (2*z_two)\n",
    "\n",
    "print(f\"Miettinen-Nurminen CI: [{L_mn:.4f}, {U_mn:.4f}]\")\n",
    "print(f\"Miettinen-Nurminen Effective SEs: {SE_mn_lower:.4f}, {SE_mn_upper:.4f}, {SE_mn_sym:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### using the alpha cutoff\n",
    "\n",
    "to use compute the p value and use the alphas cuttoff, we need the probability, if the null were true, of observing a a smaple at least as extreme in the direction of H1 as what we saw. THe usual method in NHST is to first normalize the distribution to from general Gaussian to a standard normal N(0,1) to compute this probability.\n",
    "\n",
    "To normalize and center on zero, instead of the raw observation for a non inferiority test we can use a shifted and normalized random variable build from teh random variable of the basic difference in proporition $\\hat{\\Delta} = \\hat{p_C}-\\hat{p_A}$, that is using the classic way to reduce a general gaussian to a normal $Z = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "\n",
    "$Z_{NI} = \\frac{\\hat{\\Delta} - (\\Delta_0)}{\\hat{SE}} = \\boxed{\\frac{\\hat{\\Delta} - (-\\epsilon)}{\\hat{SE}}}$\n",
    "\n",
    "As they are all estinator this is not strictly true that this is a normal, espeically with finite samples, but it tends to a normal at infinity, this what the way the Wald plug in tricks  jusstify then computing probabilitu using N(0,1)\n",
    "\n",
    "\n",
    "We define it this by narrowing the null hyopthesis to being to $ = - \\epsilon$ not just $<  -\\epsilon$ because if we reject at the \"boundary\" $-\\epsilon$ we woudl also logically reject anywhere below. So the boundary is good enougha adn give us a single probabaility distribution to work with.\n",
    "\n",
    "$Z_{NI}$ treated as a random variable is distributed as N(0,1)\n",
    "\n",
    "Now the NHST methdology asked the question: Given H0, what is the probability of the observation we just made, if it is less than alpha (say the conventional alpha = 0.05) we refject the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### P Value, Signifiance Level $\\alpha$ (a.k.a confidence by some people) and Critical Value\n",
    "\n",
    "the p value can be mechanically computed from the z score using a standard normal distribution\n",
    "\n",
    "Here we are doing a so called one tailed test that is we want to compute the probabilithy of seeing the given outcome or something \"more extreme\" given $H_0$. Here more extreme means something that makes H1 even more true, that is the variant converting much worse than the control, while H0 is true that is the variant being actually the same or better  That is with a  given the null hyptothesis and all the machinery we put in place (plug in, pooledl etc...).  Because the binomial can be approximated by a Gaussiann the probablity of seeing an outcome as observed, or more extreme,  is either the cumulative or survial function computed at the value observed in the current experiment depending on in which direction go for \"more extreme\".\n",
    "\n",
    "We can say that the estimator $\\hat{D} = \\hat{p}_A - \\hat{p}_C$ has for distribution $ N(0,SE)$\n",
    "\n",
    "given the choice of  H0and H1 we’re interested in negative values of z\n",
    "\n",
    "If we set the value observed as  $p_{observed}=\\frac{x_A}{n_A} - \\frac{x_C}{n_C}$. There  many ways to compute it:\n",
    "\n",
    "$p_{value} = \\phi(z)$ where $\\phi$ is the cmulative distribution function of the standard normal distribution and z the z score.\n",
    "\n",
    "a more direct computation is to simply integrate a gaussian representing tje distribution of the estimator, using stanrard deviation of  SE as the paramater for this guassian, that is the cumlative function:\n",
    "\n",
    "$pvalue = p(estimator observed value \\lt proportion observed) = \\int_{-\\infty }^{p_{observed}} \\frac{1}{\\sqrt{2 \\pi}(SE)} e^{-\\frac{x^2}{2(SE)^2}} dx$\n",
    "\n",
    "Note that if x is the difference in proportion it can only range from -1 to 1 in reality, so the guassian approximation can be questionable if the proportion are really close to 0 or 1 or -1\n",
    "\n",
    "The we pick an value $\\alpha$ for the so called \"significance level\" noted $\\alpha$ and in social science oftne setup at 0.05, the value for $\\hat{D}$ which make teh p value equal to $\\alpha$ is called criticxal value and  noted $\\hat{D}_{\\alpha}$, so $p(\\hat{D} < D_{\\alpha}) = \\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.1187785931193237\n",
      "Fail to Reject H0: we will consider the variant \"not worse\", p-value (0.1188)is more than alpha (0.05). meaning the probability of seeing this result is under  the null hypothesis H0 is relatively high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import beta\n",
    "\n",
    "\n",
    "def compute_p_value(x_A, n_A, x_C, n_C):\n",
    "    \"\"\"\n",
    "    Compute the p-value for a one-tailed test where:\n",
    "      H0: p_A >= p_C  \n",
    "      H1: p_A < p_C\n",
    "    using the pooled conversion rate under H0.\n",
    "    \n",
    "    Parameters:\n",
    "        x_A (int): Number of conversions in the variant group.\n",
    "        n_A (int): Number of visitors in the variant group.\n",
    "        x_C (int): Number of conversions in the control group.\n",
    "        n_C (int): Number of visitors in the control group.\n",
    "        \n",
    "    Returns:\n",
    "        p_value (float): The computed one-tailed p-value.\n",
    "        z (float): The computed z-score (test statistic).\n",
    "    \"\"\"\n",
    "    # Calculate observed conversion rates for variant and control groups\n",
    "    p_A_hat = x_A / n_A\n",
    "    p_C_hat = x_C / n_C\n",
    "    \n",
    "    # Compute pooled conversion rate under the null hypothesis (p_A = p_C)\n",
    "    pooled_p = (x_A + x_C) / (n_A + n_C)\n",
    "    \n",
    "    # Compute the standard error under H0 using pooled conversion rate\n",
    "    SE = math.sqrt(pooled_p * (1 - pooled_p) * (1/n_A + 1/n_C))\n",
    "    \n",
    "    # Calculate z statistic for the difference in conversion rates.\n",
    "    # Under H0, z is computed as:\n",
    "    z = (p_A_hat - p_C_hat) / SE\n",
    "    \n",
    "    # Since our alternative H1 is p_A < p_C (variant is worse), we use a one-tailed test.\n",
    "    # The p-value is the probability that a standard normal variable is below the computed z.\n",
    "    p_value = norm.cdf(z)\n",
    "    \n",
    "    return p_value, z\n",
    "\n",
    "\n",
    "def decision(x_A, n_A, x_C, n_C, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Make a decision to reject or fail to reject the null hypothesis based on the computed p-value and a given alpha.\n",
    "    \n",
    "    Parameters:\n",
    "        x_A (int): Number of conversions in the variant group.\n",
    "        n_A (int): Number of visitors in the variant group.\n",
    "        x_C (int): Number of conversions in the control group.\n",
    "        n_C (int): Number of visitors in the control group.\n",
    "        alpha (float): Significance level, default to 0.05.\n",
    "        \n",
    "    Returns:\n",
    "        decision_str (str): A string with the decision and details.\n",
    "    \"\"\"\n",
    "    p_value, z = compute_p_value(x_A, n_A, x_C, n_C)\n",
    "    \n",
    "    # Decision based on the p-value compared with the significance level alpha.\n",
    "    # For a one-tailed test:\n",
    "    if p_value < alpha:\n",
    "        decision_str = (\n",
    "            f\"Reject H0: p-value ({p_value:.4f}) is less than alpha ({alpha}).\\n\"\n",
    "            f\"Test statistic (z) = {z:.4f} indicates the variant conversion rate is significantly lower.\"\n",
    "        )\n",
    "    else:\n",
    "        decision_str = (\n",
    "            f\"Fail to Reject H0: we will consider the variant \\\"not worse\\\", p-value ({p_value:.4f})\"  \n",
    "            f\"is more than alpha ({alpha}). meaning the probability of seeing this result is under  the null hypothesis H0 is relatively high\\n\"\n",
    "        )\n",
    "    \n",
    "    return decision_str\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "n_C = 2000  # control group visitors per day\n",
    "n_A = 200   # variant group visitors per day\n",
    "\n",
    "# Example conversion counts\n",
    "x_C = 600  # example: 600 conversions in the control group\n",
    "x_A = 52   # example: 52 conversions in the variant group\n",
    "\n",
    "alpha = 0.05 # significance level\n",
    "\n",
    "d = 0.1\n",
    "\n",
    "target_power = 0.8\n",
    "\n",
    "    \n",
    "\n",
    "p_val, z = compute_p_value(x_A, n_A, x_C, n_C)\n",
    "print(\"P-value:\", p_val)\n",
    "\n",
    "result = decision(x_A, n_A, x_C, n_C, alpha)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### online calcualtor for sanity checks\n",
    "\n",
    "https://www.surveymonkey.com/mp/ab-testing-significance-calculator/\n",
    "\n",
    "https://abtestguide.com/calc/\n",
    "\n",
    "https://vwo.com/tools/ab-test-significance-calculator/\n",
    "\n",
    "https://www.graphpad.com/quickcalcs/pvalue1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision based on p value and type 1 error (false positive)\n",
    "\n",
    "We will pick a significance level $\\alpha$ , if the probability of observing the data or worse (more negative value of the difference of proportion between A and C) \n",
    "\n",
    "IF $pvalue$ is less than $\\alpha$ we reject H0, that is we say that variant is degrading the experience, if it is above we accept H0 that is no effect that we care about (same or better conversion)\n",
    "\n",
    "$\\alpha$ is also called the proability of type 1 error that is rejecting H0, when it is actually true, also called false positive in this context. if we set $\\alpha$ at 0.05, the false postive rate is 5%, that is we say the variant make things worse while it was atually harmless. The value of $\\hat{p}$ where pvalue is reach is called \n",
    "\n",
    "In our current example the probability of saying that the new design is worse than the legacy/control while it was perfectsly fine will be 5%, we may want it tigher, i.e only accept a 0.05% of mistakenly not taking the new design has it has some other beneft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Type 2 Error (false negative) and Power\n",
    "\n",
    "Type 2 error is the probability of accepting H0  (failing to reject, double negation) when it is actually false, so that would be saying \"variant is harmless\" while it is actually degrading the experience. So in practice here it woudl be we observer a differnec ince conversion postive ot at leat larger than -0.1\n",
    "\n",
    "\n",
    " But the probability of type 2 error can only be assesssed after some kind of mimimum effect size is chose, as wityout it there is no way to model the new mean of estimator disitribution in the alternative hpothesis \n",
    "\n",
    "Let's assume that we only care if hte degradation is at least -0.1 (E(D) = -0.1), that is the variant degrades the conversion by at least 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the estimator under the alternative hypothesis H1, we use the gaussian:\n",
    "\n",
    "$\\hat{D} = \\hat{p}_A - \\hat{p}_C = N(\\delta, SE_{H1}) = N(-0.1, SE_{H1}) $ that is the guassian now in centerd on -0.1, no centered on 0\n",
    "\n",
    "with\n",
    "\n",
    "$SE_{H1} = \\sqrt{\\frac{p_A(1-p_A)}{n_A}+ \\frac{p_C(1-p_C)}{n_C}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we compute the probabilit of the esimator coming up \"higher\" but with the same critical value as it was established ahead of time with the significance level alpha.\n",
    "\n",
    "So the probabilit fo the estimator coming up higher than the significant level is\n",
    "\n",
    "$\\beta = p(\\hat{D} > D_{\\alpha} | E(D)=-0.1)$\n",
    "\n",
    "$ = \\int_{D_{\\alpha}}^{\\infty} \\frac{1}{\\sqrt{2\\pi}SE_{H1}} e^{-\\frac{(x+0.1)^2}{2(SE_{H1})^2}}dx$\n",
    "\n",
    "The Beta is the probability that even though the variant conversion rate is materially worse, we say \"it is fine\"\n",
    "\n",
    "The power $1-\\beta$ is the probability that we say the variant is worse when it happens to be actually worse.\n",
    "\n",
    "In many social since the power is set at 0.8, meaning 0.2 probability not spotting that we degraded the test bu at least  0.1.\n",
    "\n",
    "\n",
    "This computation will give su whatever it gives us, if we want to TARGET a Beta of 0.8, we can conpute the sample sizes (implied in SE) that will make us reach that level.\n",
    "\n",
    "Note there is no really close formed formula so some gaussian approximation and a bunch of plug in hacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current beta (Type II error rate): 0.0713\n",
      "Current power (1 - beta): 0.9287\n",
      "Minimum required visitors in variant group: 132\n",
      "Minimum required visitors in control group: 1319\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def compute_beta_power(x_A, n_A, x_C, n_C, d, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Computes the beta (Type II error rate) and power (1 - beta) for the one-tailed test:\n",
    "        H0: p_A >= p_C\n",
    "        H1: p_A < p_C\n",
    "    assuming a material effect of size d (i.e. the variant conversion rate is lower\n",
    "    than the control by at least d).\n",
    "    \n",
    "    Parameters:\n",
    "        x_A (int): Conversions in variant group.\n",
    "        n_A (int): Visitors in variant group.\n",
    "        x_C (int): Conversions in control group.\n",
    "        n_C (int): Visitors in control group.\n",
    "        d (float): Minimum meaningful difference (a positive number, e.g., 0.1).\n",
    "                   Under H1, we assume p_A_true = p_C - d.\n",
    "        alpha (float): Significance level (default 0.05).\n",
    "    \n",
    "    Returns:\n",
    "        beta (float): Estimated Type II error rate.\n",
    "        power (float): Estimated power (1 - beta).\n",
    "    \"\"\"\n",
    "    # Estimate control conversion rate from observed data.\n",
    "    p_C = x_C / n_C\n",
    "    # Under alternative, assume variant conversion rate is reduced by d.\n",
    "    p_A_true = p_C - d\n",
    "    \n",
    "    # Standard error under H0 (pooled estimate with p_A = p_C)\n",
    "    SE0 = math.sqrt(p_C * (1 - p_C) * (1/n_A + 1/n_C))\n",
    "    # Critical value from the null distribution (one-tailed; left tail)\n",
    "    # Note: norm.ppf(alpha) is negative (e.g., -1.645 for alpha=0.05)\n",
    "    D_alpha = norm.ppf(alpha) * SE0\n",
    "\n",
    "    # Standard error under H1 (using p_C for control and p_A_true for variant)\n",
    "    SE1 = math.sqrt( (p_C * (1 - p_C) / n_C) + (p_A_true * (1 - p_A_true) / n_A) )\n",
    "    \n",
    "    # Under H1, the test statistic D = p_A - p_C is distributed as N( -d, SE1^2 ).\n",
    "    # We reject H0 if D <= D_alpha.\n",
    "    # Thus, beta = P( D > D_alpha | H1 )\n",
    "    # Convert to a standard normal probability:\n",
    "    beta = 1 - norm.cdf((D_alpha + d) / SE1)\n",
    "    power = 1 - beta\n",
    "    return beta, power\n",
    "\n",
    "def compute_minimum_sample_sizes(x_A, n_A, x_C, n_C, d, target_power, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Estimates the minimum required sample sizes (for the variant and control groups)\n",
    "    to achieve a desired power, assuming that the ratio between the control and the variant group\n",
    "    stays constant (i.e., the variant group is a fraction of the control).\n",
    "    \n",
    "    The function uses the observed data to estimate the baseline control conversion rate p_C.\n",
    "    It then assumes that under H1: p_A = p_C - d.\n",
    "    \n",
    "    Parameters:\n",
    "        x_A (int): Conversions in variant group.\n",
    "        n_A (int): Visitors in variant group.\n",
    "        x_C (int): Conversions in control group.\n",
    "        n_C (int): Visitors in control group.\n",
    "        d (float): Minimum meaningful reduction in conversion rate (positive, e.g., 0.1).\n",
    "        target_power (float): Desired power (e.g., 0.8).\n",
    "        alpha (float): Significance level (default 0.05).\n",
    "        \n",
    "    Returns:\n",
    "        n_A_required (float): Minimum required visitors in the variant group.\n",
    "        n_C_required (float): Minimum required visitors in the control group.\n",
    "    \"\"\"\n",
    "    # Estimate p_C from the observed control group data.\n",
    "    p_C = x_C / n_C\n",
    "    # Under alternative, the variant rate is assumed to be:\n",
    "    p_A_true = p_C - d\n",
    "    \n",
    "    # Determine the ratio k = n_C / n_A (using current sample sizes)\n",
    "    k = n_C / n_A\n",
    "\n",
    "    # Under H0, the variance factor (without n) is:\n",
    "    SE0_term = math.sqrt( p_C * (1 - p_C) * ((k + 1) / k) )\n",
    "    \n",
    "    # Under H1, the variance factor is:\n",
    "    SE1_term = math.sqrt( p_C * (1 - p_C) / k + p_A_true * (1 - p_A_true) )\n",
    "    \n",
    "    # For a one-tailed test:\n",
    "    q_alpha = norm.ppf(alpha)           # This is a negative number (e.g., -1.645 for alpha=0.05)\n",
    "    q_power = norm.ppf(target_power)      # For target power (e.g., 0.84 for 80% power)\n",
    "    \n",
    "    # The critical value under H0 is D_alpha = q_alpha * (SE0_term / sqrt(n_A)).\n",
    "    # Under H1, to have power = target_power, we require:\n",
    "    #   q_alpha*(SE0_term/√n_A) = -d + q_power*(SE1_term/√n_A)\n",
    "    # Solving for √n_A yields:\n",
    "    #   √n_A = ( q_power * SE1_term - q_alpha * SE0_term ) / d\n",
    "    # (The subtraction sign becomes addition because q_alpha is negative.)\n",
    "    n_A_required = ((q_power * SE1_term - q_alpha * SE0_term) / d) ** 2\n",
    "    n_C_required = k * n_A_required\n",
    "    return n_A_required, n_C_required\n",
    "\n",
    "\n",
    "\n",
    "# Compute current beta and power using observed data:\n",
    "beta, power = compute_beta_power(x_A, n_A, x_C, n_C, d, alpha)\n",
    "print(f\"Current beta (Type II error rate): {beta:.4f}\")\n",
    "print(f\"Current power (1 - beta): {power:.4f}\")\n",
    "\n",
    "# Compute required minimum sample sizes to achieve the target power:\n",
    "n_A_req, n_C_req = compute_minimum_sample_sizes(x_A, n_A, x_C, n_C, d, target_power, alpha)\n",
    "print(f\"Minimum required visitors in variant group: {n_A_req:.0f}\")\n",
    "print(f\"Minimum required visitors in control group: {n_C_req:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume 2 prior distribution\n",
    "\n",
    "For the control we have plently of historical data so we can take the historical and/or recent conversion as the baseline, no need to perform statististical inference on it.\n",
    "\n",
    "For the Variant A, assuming we start with an non informative prior (a.k.a uniform prior of) of\n",
    "\n",
    "$Beta(1,1)$  \n",
    "\n",
    "With the beta distruction defined as \n",
    " \n",
    "representing the prior probability distribution of p_A, meaning that we have no idea, p_A could range from 0 to 1 with equal probability\n",
    " \n",
    "after n trials and k success the posteriod probability distribution due to the property of the Beta function is\n",
    " \n",
    "$Beta(k+1,n−k+1)$ \n",
    "\n",
    "This derived through Bayes Theorem and how the Beta function can be integrated\n",
    "\n",
    "The expected valye for $E(Beta(\\alpha,\\beta)) = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "which for a bayesian posterior becomes\n",
    "\n",
    "$E(Beta(k+1,n−k+1)) = \\frac{k+1}{k+1 + n−k+1} = \\frac{k+1}{n+2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value of posterior distribution for p_A: 0.2624\n"
     ]
    }
   ],
   "source": [
    "expected_value_posterior = (x_A + 1) / (n_A + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Credible Interval for p: [0.2042, 0.3250]\n"
     ]
    }
   ],
   "source": [
    "# Posterior parameters\n",
    "alpha = x_A + 1\n",
    "beta_param = n_A - x_A + 1\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% Credible Interval for p: [0.1876, 0.3459]\n"
     ]
    }
   ],
   "source": [
    "# compute the 99% credible interval (1th and 99th percentiles)\n",
    "p_L_99 = beta.ppf(0.005, alpha, beta_param)\n",
    "p_U_99 = beta.ppf(0.995, alpha, beta_param)\n",
    "# Output the result\n",
    "print(f\"99% Credible Interval for p: [{p_L_99:.4f}, {p_U_99:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "finance"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
