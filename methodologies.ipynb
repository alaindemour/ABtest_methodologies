{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AB Test Methodologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Structure of the test\n",
    "\n",
    "We have an existing jounrney which has a completion rate of x% , currently about 20%\n",
    "\n",
    "We are going to keep 70% of the traffic on the legacy/existing pages as a controld group and test n variants $A_1$, $A_2$,..$A_i$. We want to make sure the new proposed pages, do not degrade the journey convertion rates significantly, the goal here is not to improve it just not making it worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Hypothesis Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis will consider 2 Bernoulli Random Variables (RV) and their difference.\n",
    "\n",
    "$X_{control}$ being the control control group Bernoulli RV $X_A$ being the version A. They are both assumed to have for codomain $\\mathcal{X}_{control}=\\mathcal{X}_A=\\{0,1\\}$\n",
    "\n",
    "We are not going to try to use directly the random variable $X_A - X_{control}$ which is not a Bernoulli one has it can take 3 value -1, 0  and 1, but which  does capture the \"conversion rate differential\" between the control and a given variant.\n",
    "\n",
    "But we are going to work  with a new ran dom varaible we will call\"sample proportion\" which is the sum to n of the Bernoulli ones and call them  $\\hat{p}_C$ and $\\hat{p}_A$ for the control and the variant respectively , defined as $\\frac{1}{n}\\sum_1^n X_i$ each sample proportion is at the same time:\n",
    "\n",
    "* $\\hat{p_A}$ is a radom variable with the binomial codomain $\\{0,\\frac{1}{n},\\frac{2}{n},...n\\}$\n",
    "* $\\hat{p_A}$ is an estimator of the expected value $E(\\hat{p}_A)$  of $X_A$, with $E(\\hat{p}_A) = p$ becuase of teh laway of large number applied to a binomial. An estimator is a function from tjhe cartesian product of the space of realizations  to the domain of the parameter of ineterest (codomain(X) = $\\mathcal{X}$), so $\\mathcal{X}^n$ to some real value for a paraemter of origina random variable, in this case $\\hat{p}_A: \\mathcal{X}^n \\rightarrow \\{0,\\frac{1}{n},\\frac{2}{n},...n\\}$ withe binomial distribution which we can approximate to a gaussian as n grows\n",
    "\n",
    "Note that for a Bernoulli RV X $Var(X) = p (1-p)$ and therefore \n",
    "  \n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}Var(\\sum_1^n X_i)$\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}n(p(1-p))$\n",
    "\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n}(p(1-p))$\n",
    "\n",
    "so $\\boxed{Var(\\hat{p})= \\frac{p(1-p)}{n}}$\n",
    "\n",
    "\n",
    "#### Null Hypothesis (H₀):\n",
    "The conversion rate for the variant is equal or better than fthe control (p_variant = 20% or better). Note that hte null hypothesis is not really \"no effect\" but in \"view of mind\" that actuall no effect or better is \"no effect\" = good.\n",
    "\n",
    "this can be modeled by stating the radom variable $\\hat{p_A}  - \\hat{p_C} $ has $E(\\hat{p_A}  - \\hat{p_C}) \\geq 0 $\n",
    "\n",
    "#### Alternative Hypothesis (H₁):\n",
    " The conversion rate for the variant is stricly lower than the control.\n",
    "\n",
    "Each realization is given after each experience by the folowing counts\n",
    "\n",
    "$n_C$: Number of visitors in the control group per day (700)\n",
    "\n",
    "$x_C$ : Number of conversions in a given control group\n",
    "\n",
    "$n_A$: Number of visitors in each given group per day (100)\n",
    "\n",
    "$x_A$ : Number of conversions in the variant group\n",
    "\n",
    "\n",
    "Calculate the observed conversion rates (estimator), for the version, the control  and the 2 combine (the pool) as under the null hypothesis they can be combined in an estimator as the true conversion parameter is assumed identical\n",
    "\n",
    "the realization of $\\hat{p}_A$  is $(\\hat{p}_A(\\omega) = \\frac{x_A}{n_A})$\n",
    "\n",
    "the associated realization of $\\hat{p}_C$ is  $(\\hat{p}_C(\\omega) = \\frac{x_C}{n_C})$\n",
    "\n",
    "Then there is a theoritical estimator for the combine exeprience IF the null hypothesis was holding and there was not difference A and the control there woudl be a random variable for the estimator of both being pooled\n",
    "\n",
    "$\\hat{p}(\\omega_{pool}) = \\frac{x_A + x_V}{n_A + n_C}$\n",
    "\n",
    "Then we try to compute the variance standard deviation of the difference between the conversion rate of the control vs. the variance. The variance of the difference beteww the 2 proportion is the sum of their variance, so\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = Var(\\hat{p}_C) + Var(\\hat{p}_A)$\n",
    "\n",
    "Here there is the first \"sleight of hand\", we dont really know the true mean of those estimators, but appplying a \"trick\" called the plugs in (it is actually called the plug in principle) the *realization* of the pooled esimtator is used as if it was the true underlying value( so called population true parameter ) in the formula for the standard deviation and then we use the resulitng standard deviation it as THE standard deviation of the estimators, from that SD then we compute a z-score. The justification is a bit hand wavy and assume \"converge\" if many expeirments are run which is obviously questionnable on a single experiement in order to make a decision.\n",
    "\n",
    "Also because both $\\hat{p_C}$ and $\\hat{p_A}$ are Bernoulli RV, and using this \"plug in \" principal; we compute the sum of variance of the estimators also using the pooled sample reading as:\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_A} + \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_C} = \\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})$\n",
    "\n",
    "and the standard error which is just the standard deviation of the same quantity  also using the plug in principl the standard devation of the estimators that statisticain call \"standard error\" is\n",
    "\n",
    "$\\boxed{SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}}$\n",
    "\n",
    "this is sometimnes called \"pooled standard error\" or \"standard error of the difference between two independent proportions\"\n",
    "\n",
    "From this we conmpute the z-score whuich is a dimenionless value counting the number of standard devaition that the difference observed is from the theoritical difference which shoiuld be zero under the null hypothesis (approximated by the pooled one, sktechy). So a simple ivisino of the observed devation over teh standard deviation of hte diference\n",
    "\n",
    "$z= \\frac{\\hat{p}_A - \\hat{p_C}}{SE}$\n",
    "\n",
    "the p value can be mechanically computed from the z score using a standard normal distribution\n",
    "\n",
    "the minimum experiement size for a target effect size and confidence is another story, more complicated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Here we are doing a so called one tail test that is we want to compute the probabilithy of see the given outcome or something \"worse\" given the $H_0$ Hypothesis,  that is with a  given the null hyptothesis and all the machinery we put in place (plug in, pooledl etc...).  becuase the proportion random variable is a sum of Bernoullis by the central limite theorem we can assume it is following a binomial which in turn can be approximated by a gaussian. So the probablity of seeing an outcome as observed, or \"worse\" is really the survial function computed at the value observed in the current experiment. There  many ways to compute it:\n",
    "\n",
    "$p_{value} = 1 - \\phi(z)$ where $\\phi$ is the cmulative distributiin function of the standard normal distribution and z the z score.\n",
    "\n",
    "a more direct computation is to simply integrate the gaussian of stanrd devation SE that is the survial function\n",
    "\n",
    "$p(proportion > (\\hat{p}_V - \\hat{p}_A)) = \\int_{(\\hat{p}_V - \\hat{p}_A)}^\\infty \\frac{1}{(SE) 2 \\pi} e^{-\\frac{x^2}{2(SE)^2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
