{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB Test Methodologies Comparison : Null Hypothesis Significance Testing vs. Bayesian\n",
    "\n",
    "In the context of launching passkeys and comparing the abandonment rates and benefit of the UX variants we want to test, this notebook is a comparison of classic Null Hypothesis Significance Testing which is the method used by traditional statisticianss for decades but which are concpetually complex and hard to truly interpret,  with Baysian methods which are gaining more and more traction and they provide more flexibility and better direct interpretability for decision making like for decide when and how to switch more traffic from a control to a variant or between variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Structure of the test between : Control C and Variants A1, A2, A3 ...\n",
    "\n",
    "We have an existing journey to create customer digital identiy and create credentials which has a completion rate of x% , for the same of this discussion we will use an example success rate of 20% (that is 80% of user abandoning).\n",
    "\n",
    "We are going to keep 90% of the traffic on the legacy/existing pages as a controld group and test n variants $A_i$ We want to make sure the new proposed CX is not worse or better than the control, at which point we will switch all traffic to the variants, then rate the variants against each other.\n",
    "\n",
    "The type of AB test where one want to see whether somethig new is not \"making things worse\" i.e. somehow degrading the experience is usually called a \"non inferiority test\" (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST) Methodology\n",
    "\n",
    "At a high level the methdology is : assume what you don't want to see or heave, call it the null hypothesi for instanc teh drug has no effect or in our cuase the new experience singifianctly increase abandonment, run the test, look at the result if and if the result looks  unlikely enoug as if low enough probabitliy to happe (e.g. like that 5% to see what we observe) under the Null Hypothesis, then reject the hypothesis. Note 2 thing immedialy youi havent really said wether the oppostive hypothesis is true, just that your reject the null. Also \"unlikely enough\" is defined buy a somewhat arbitrary number (5% chance).\n",
    "\n",
    "The method boils down to compute the probability of the observation given the hypothesis, as we will see later the Bayesian approach is going the other way, it computes the probabilty of the hypothesis given the observation, which is a totally different metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing of the problem using Random Variables\n",
    "\n",
    "This abandonment rate of a  UX is usually modelled as 2 Bernoulli Random Variables $X_C$ and $X_A$ from which we will draw repeadetly. Bernoiulli just mean we present a choice with only 2 possible outcome success/failure, connvert/abandon, up/down etc.. presenting the new CX repeatedly to a bunch of end users.\n",
    "\n",
    "$X_C$ being the UX and Bernoulli RV presented to control control group  $X_A$ being the variant A presented. They are both assumed to have for codomain $\\mathcal{X}_C=\\mathcal{X}_A=\\{0,1\\}$ that is a boolean i.e. it converts or does not convert. Here \"convert\" means the user perform the action they intended to do vs. abandoning the journey, e.g. create a passkey. There a possiblty with a techincal failure but this would not be their choice so it woudl be counted as a success for the purpose of this AB test.\n",
    "\n",
    "The way NHST usually model the difference between a contrl and variant convertion rate is to define a new random varaibles called the \"sample proportions\" which is the sum of n instances of the Bernoulli ones defined above, divided by n. For this \"sample proportion\" Let's using the notation  $\\hat{p}_C$ and $\\hat{p}_A$ for the control and the variant respectively , defined as\n",
    "\n",
    " $ \\hat{p_C}=\\frac{1}{n}\\sum_1^n X_{Ci}$ and \n",
    " $ \\hat{p_A}=\\frac{1}{n}\\sum_1^n X_{Ai}$\n",
    " \n",
    " each sample proportion can be interprested as 2 things :\n",
    "\n",
    "* $ \\hat{p_C}$ and  $ \\hat{p_A}$ can be interpresetd as an ordinary random variables with codomain $\\{0,\\frac{1}{n},\\frac{2}{n},...\\frac{n}{n}\\}$\n",
    "* $\\hat{p_C}$ and  $ \\hat{p_A}$ can also be intepreted as estimators (in the technical sense of the word \"estimator\" used in classical statistics) of the expected value $E(p_C)$ and $E(p_A)$. In general regardless of whether it is a control or a variant $E(\\hat{p}) \\rightarrow E(p)$ because of the law of large numbers applied to a Bernoulli/Binomial.\n",
    "\n",
    "The notaiton with a \"little hat\" over the letter is the convention in statics for denote estimators.\n",
    "\n",
    "An estimator is a function from the n-cartesian product of the space of realizations of X (on itself n times)  to the domain of the parameter of interest, if we  use the notation codomain(X) = $\\mathcal{X}$, it is then a maaping from $\\mathcal{X}^n$ to some real value for a paraemter of original random variable X, in this case $\\hat{p}_A: \\mathcal{X}^n \\rightarrow \\{0,\\frac{1}{n},\\frac{2}{n},...,\\frac{n}{n}\\}$ Because it it is a sum of Bernoullis, the estimator RV follows binomial distribution centered on the expected value. It is always true that a discrete binomial distirubtion can be approximated by a contunous gaussian as n grows (clipping the tails)\n",
    "\n",
    "\n",
    "#### Variance and standard devation of a sum of Bernoulli or Binomial\n",
    "\n",
    "in all the rest of the discussion we are going to use a lot of the variance and standard devaition of the sample proporotion as it it used to defined what is called the \"standard error\" under various hypothesis. This is just a reminder of how tehy are computed\n",
    "\n",
    "Note that for a \"single\" Bernoulli random variable $X$ we have $Var(X) = p (1-p)$ and therefore \n",
    "  \n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}Var(\\sum_1^n X_i)$\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n^2}n(p(1-p))$\n",
    "\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_1^n X_i) = \\frac{1}{n}(p(1-p))$\n",
    "\n",
    "so $\\boxed{Var(\\hat{p})= \\frac{p(1-p)}{n}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Notation for the  Estimator of the difference in proportions\n",
    "\n",
    "To make the rest of the discussion easier to read  we are going to define the difference in estimator $\\hat{p_A}$ and $\\hat{p_C}$ as $\\hat\\Delta = \\hat{p_A} - \\hat{p_C}$ which is also an estimator of the expected value of random varaible $\\Delta = p_A - p_C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Null Hypothesis (H₀):\n",
    "\n",
    "\n",
    "As we said earlier the  \"effect\" we are trying to avoid is if the new passkey creation pages would degrade the overall UX  so much that it may impacts our other business metrics. In the classical framing of a non-inferiotity test, the  Null Hypothesis is the \"bad thing\" we are tryigng to reject in our case it would mean : \"some degration\".\n",
    "\n",
    "\"some degration\" can be described with a simple inequality like  $E(p_A) \\le E(p_C) - \\epsilon $ where $\\epsilon$ is a tiny degradation we would find acceptable for instance 0.03 (3%).\n",
    "\n",
    "We will rewrite\n",
    "\n",
    "$H_0$ as $\\boxed{H_0 = E(\\Delta) \\leq -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    " The conversion rate for the variant is higher than $\\boxed{E(\\Delta) > -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Hypothesis\n",
    "\n",
    "Below we will make use of the hyoptheis that the difference in conversion is exactly the snall accetable one  that is\n",
    "\n",
    "$\\boxed{E(\\Delta) = -\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Numerical example \n",
    "\n",
    "Each realization is given after each experience by the folowing counts\n",
    "\n",
    "$n_C$: Number of visitors in the control group\n",
    "\n",
    "$x_C$ : Number of conversions observed (realization) in a given control group\n",
    "\n",
    "$n_A$: Number of visitors in the variant group\n",
    "\n",
    "$x_A$ : Number of conversions observed (realization) in the variant group\n",
    "\n",
    "$\\hat{\\Delta_{obs}}$ the observed value of the difference between the proportions\n",
    "\n",
    "$\\epsilon$ : accetable degradation in the difference between the proportions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_conversion_rate = 0.2 # based on historical data\n",
    "nC = 7000\n",
    "xC_observed = nC * control_group_conversion_rate\n",
    "nA = 1000\n",
    "xA_observed = 200\n",
    "epsilon = 0.03\n",
    "alpha = 0.05\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed  / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_observed:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_observed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the estimator $\\hat{\\Delta}$ a.k.a Standard Error in frequentist/classic statistics.\n",
    "\n",
    "The NHST methodolgy involve computing first an estimatation of the standard deviation for the estimator $\\hat{\\Delta}$, then to see how likely it then comparing it with the actual values we get from the difference in proportion in the experiment, then based on this decide if the difference observed is far enough (or close enough) to the theoritical value to reject or not reject the hypothesis based on some accpeted risks in the deicison rules (power, confidence internval)\n",
    "\n",
    "THis is the first problem NHST has to tackle we have no idea what the true standar deviation is, so all varation on NHST use a \"hack\" they call the plug in principle, which basically use the current experiement results , to \"plug in\" some formula to esimate the unknown standard deviation. But note the circularity of the method:\n",
    "\n",
    "1. We want to test if the data is unusual under H₀\n",
    "2. To measure \"unusual,\" we need the standard error under H₀\n",
    "3. But SE depends on the unknown θ, so we plug in p̂ (the data itself!)\n",
    "4. We then use this data-derived SE to judge whether the data is unusual\n",
    "\n",
    "It's like: *Let me use my single measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising*\n",
    "\n",
    "NHST proponent justify it because if ou repeateded the same experiment many times, at infinity would converge to the true value \"on average\".But you only have ONE sample!\n",
    "You don't know if your particular p̂ is:\n",
    "Close to the true value (plug-in works well)\n",
    "An outlier (plug-in is terrible)\n",
    "\n",
    "Why Frequentists live with it:\n",
    "\n",
    "* Long-run frequency interpretation: \"If we repeat this procedure many times, it has correct coverage\"\n",
    "* Pragmatism: They need something computable\n",
    "* Simulation evidence: It works \"reasonably well\" in practice for moderate n, but \"reasonably well\" can be challeng see goole \"the crisis of replication\" to get a sense of the numerous failures.\n",
    "* No alternative: In the frequentist framework, parameters are fixed unknowns, not random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The various plugin Hacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### If hypotehsis is \"no effect\" the Wald pooling method:\n",
    "\n",
    "If instead of \"non inferiority\" the null hypothesis  is \"no effect\" ( we will have that later for H1), the plug in hack is to record the observed conversion rates (estimators) for each proportion then  combine (pool) as under this very specific hypothesis they have the exact same expected value so a larger sample created through pooling is a better estimator of the true value, whatever it is.\n",
    "\n",
    "the realization of $\\hat{p}_A$  is $(\\hat{p}_A(\\omega) = \\frac{x_A}{n_A})$\n",
    "\n",
    "the associated realization of $\\hat{p}_C$ is  $(\\hat{p}_C(\\omega) = \\frac{x_C}{n_C})$\n",
    "\n",
    "Then there is a theoritical estimator for the combine exeprience IF the null hypothesis was holding and there was not difference A and the control there woudl be a random variable for the estimator of both being pooled\n",
    "\n",
    "$\\hat{p}(\\omega_{pool}) = \\frac{x_A + x_V}{n_A + n_C}$\n",
    "\n",
    "Then we try to compute the variance standard deviation of the difference between the conversion rate of the control vs. the variance. The variance of the difference beteww the 2 proportion is the sum of their variance, so\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = Var(\\hat{p}_C) + Var(\\hat{p}_A)$\n",
    "\n",
    "Also because both $\\hat{p_C}$ and $\\hat{p_A}$ are Bernoulli RV, and using this \"plug in \" principal; we compute the sum of variance of the estimators also using the pooled sample reading as:\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_A} + \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_C} = \\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})$\n",
    "\n",
    "and the standard error which is just the standard deviation of the same quantity  also using the plug in principl the standard devation of the estimators that statisticain call \"standard error\" is\n",
    "\n",
    "$\\boxed{WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}}$\n",
    "\n",
    "this is sometimnes called \"pooled standard error\" or \"standard error of the difference between two independent proportions\"\n",
    "\n",
    "From this we conmpute the z-score whuich is a dimenionless value counting the number of standard devaition that the difference observed is from the theoritical difference which shoiuld be zero under the null hypothesis (approximated by the pooled one, sktechy). So a simple ivisino of the observed devation over teh standard deviation of hte diference\n",
    "\n",
    "$z= \\frac{\\frac{x_A}{n_A} - \\frac{x_C}{n_C}}{SE}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If hyopthesis is \"non inferiority\", use the Wald Unpooled Standard Error:\n",
    "\n",
    "If the hypotheis is not \"no effect\", but \"on inferioity\" we cannot assume that each sample is drawn from the same random variable, we actually explicitely say it is not because of the epsilon among other thing. So the first quick and dirt approch is just to sum the separate variances (see above variance of Bernoulli) as variances are additive when random variable are added  or substracted in our case, and are assymed to be independent, then take the square roo t pf that sum to get to a standard deviation:\n",
    "\n",
    "$\\hat{WaldUnpooledSE} = \\sqrt{\\frac{\\hat{p_A(w)}(1-\\hat{p_A(w)})}{n_A} + \\frac{\\hat{p_C(w)}(1-\\hat{p_C(w)})}{n_C}}$\n",
    "\n",
    "Note that we should be using hte expected value of $E(p_A)$ and $E(p_B)$ in the formula but we don't know them, so we use the so call \"plug in\" trick and replace them but whatever values we get. That is why this is a quick and dirty approach that can produce questionable results depending on the specific phenomenon and sizes of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slightly Better for \"non inferiority\"  Newcombe (score-based / Wilson)\n",
    "\n",
    "Much better actual coverage than Wald, especially with imbalanced samples or extreme p.\n",
    "Still fairly simple to compute (closed-form formulas exist, or can be coded).\n",
    "\n",
    "we wont' cover it here but it is still a plug-in hacks suffering from circularity argument and n=1 problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miettinen–Nurminen\n",
    "\n",
    "Widely used in clinical trials and non-inferiority contexts and in some regulated industry like pharmaceitical (recommended by the FDA)\n",
    "\n",
    "Very complex and hard to explain, and is still fundmentally a plug in hack with the same circular reasoning and  n=1 We wont cover it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Postive, pvalue, signifiance level $\\alpha$ (a.k.a confidence by some people) and critical value\n",
    "\n",
    "\n",
    "Once we approximated the  standard error SE (which is the standard devation of $\\hat{\\Delta}$) as explained above, teh NHST methodology then also fixed the expected value of $\\hat{\\Delta}$ under H0. The idea is that if you fix bothe the expected value and standar deviation , you can then work with a binomial or guassian probability distribution to compute whatever event probability you want. Even though the process is descrete and we should use a binomial continyusous gaussian are used in practice as htey are easier to manipulate in equations and are good approxmiation of binomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The other simplificaiton is that although H0 is an inquality $E(\\Delta) \\leq -\\epsilon$ we are going to actuall use $E(\\Delta) = -\\epsilon$ to get a single probability distribution to work with, with the rationale that the boundary is the least favorable case, that is if we reject H0 at the boundary $-\\epsilon$ logically we woudl reject it also at anythng with $E(\\Delta) \\leq -\\epsilon$, as our observation woudl be get an even lower probability if the mean shifted  to the left.So using the boundary value epsilon is that one that makes it the most difficult to cross the alpha threshold and the most difficult to reject $H_0$\n",
    "\n",
    "So with all that in place we can finally model H0 with a gaussian of mean $\\mu = -\\epsilon$ and standard deviation $\\sigma = \\hat{SE}$ \n",
    "\n",
    "$N(\\mu, \\sigma)$\n",
    "\n",
    "Now we can compute the p-value which is the probability of observing or something like the sample we got or more extreme in the direction of H1. In our case it woudl be probability of the sample being in $[\\hat{\\Delta_{obs}}, +\\infty]$ which can be comnputed by integrating the right tail of the gaussian from $\\hat{\\Delta_{obs}}$ to $+\\infty$. This integral of the right tail is called the survival function of the gaussian and can be computed direct in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The direct way of computing the survival function and therefore the pvalue  directly from the observation $\\hat{\\Delta_{obs}}$ would be to integrate\n",
    "\n",
    "pvalue $ = \\int_{ \\hat{\\Delta_{obs}} }^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}(SE)} e^{-\\frac{(x - (-\\epsilon))^2}{2(SE)^2}} dx$\n",
    "\n",
    "Using a common notation where $\\Phi$ is the cumulative distribtiuon of the standard normal N(0,1), the survival woudl altneratively expressed as this if one does not have something that conpute survial function directly\n",
    "\n",
    "pvalue = $1 - \\Phi(\\frac{\\hat{\\Delta_{obs}} - \\mu}{\\sigma})$\n",
    "\n",
    "that we will compare to the $\\alpha$, in our case 0.05, if the the probability of seeing what we observed or something even more favorable (the right tail) is 5% or less, we reject the null hypothesis H0 because we are seeing is very unlikely under H0. The critical value is the inverse of the right tail integratiojn, is the the value of hte observation that would make the probabilit of seeing the sample 5% or lower in our setup. The formula to inverse it from the $\\alpha$ is\n",
    "\n",
    "$c = \\mu  + \\sigma \\Phi^{-1}(1 - \\alpha)$\n",
    "\n",
    "Below is a full computation and a vizualization of the parameters along the right tail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided): {p_value:.4f}')\n",
    "\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for p-value={alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values; recompute to be robust\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H0 - 6 * sigma_HO\n",
    "right = mu_H0 + 6 * sigma_HO\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Right-tail probability\n",
    "p = norm.sf(x0, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Critical value at significance alpha (one-sided)\n",
    "crit_x = norm.ppf(1 - alpha, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade right tail\n",
    "mask = xs >= x0\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Right tail p = {p:.4g}\")\n",
    "\n",
    "# Vertical line at observed x\n",
    "ax.axvline(x0, color=\"C1\", ls=\"--\", lw=1.5, label=f\"observed delta = {x0:.4f}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(crit_x, color=\"C2\", ls=\"-.\", lw=1.5, label=f\"critical value c = {crit_x:.4f}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H0\n",
    "ax.axvline(-epsilon, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H0 = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H0:.4f}, σ={sigma_HO:.4f}) — Right-tail beyond x\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under the null H0\")\n",
    "ax.set_ylabel(\"Probability density under H0\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.show(), display handled by notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### traditional presentation of the same conmputation using z-scores\n",
    "\n",
    "The traditional ways for NHST to compute the p value is not to use the measurment $\\hat{\\Delta_{obs}}$ directly but to normalize it to a standard normal and centered on zero and on standard deviation 1 N(0,1). This mormalization is done by transforming the measurment into a so called z-score:\n",
    "\n",
    "The the z-score is just the way you can rescale an arbitrary Gaussian to the Normal by substracting the meand and dividing by the standard deviation $Z = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "$Z_{NI} = \\frac{\\hat{\\Delta} - (E(\\Delta_{H_{boundary}}))}{SE}$ \n",
    "\n",
    "$= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}$ \n",
    "\n",
    "$\\boxed{Z_{NI}= \\frac{\\hat{\\Delta} + \\epsilon}{SE}}$\n",
    "\n",
    "Then  $\\frac{\\hat{\\Delta} + \\epsilon}{SE}$ is a normal N(0,1) and \n",
    "\n",
    "$p(\\hat{\\Delta} \\ge \\hat{\\Delta(w)})$\n",
    "\n",
    "$ = p(\\frac{\\hat{\\Delta} + \\epsilon}{SE} \\ge \\frac{\\hat{\\Delta(w) +\\epsilon}}{SE })$\n",
    "\n",
    "Then we can compute the survial function but this timne on a standard normal and from the z-score value\n",
    "\n",
    "$\\int_{ Z_{NI} }^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}z^2} dz$\n",
    "\n",
    "which give the exact same results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positive\n",
    "In this NHST the pvalue is also the probabilit of a false positive that is the probability of rejecting the null hopythisis (saying htere is no degration) while there is acxtually a degradation. So it is setup our risk of make the wrong decision is 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Probability of having false negative a.k.a type 2 error , Power and sample size required to get to signficance under those conditions\n",
    "\n",
    "Conversely , \"False negative\" in the context of a non inferiority test is the probability of failing to reject given the fact that H1 is true, meaning we fail to clear the non inferiority test, while the new UX is actually non inferior (i.e. as good or better than the old one). So we have the same issue as for the false poistive computation, we have to pick a value for what expected value for the $\\Delta$ but this time under H1. As we cannot really work with a range of expecgted value for $\\Delta$  we need a single value to fix the gaussian we are going to integrate. Here possible choice is to also pick a boundary which is $E(\\Delta) = E(p_C)$ so whatever the control group mean was or is (depending on how much historical data we have). This is usally called \"minimum effect size we want to detect\" as is really a business decision, it is a bit arbitrary. Pickig our baseline is just one option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the estimator under the alternative hypothesis H1, we nedd to pick another expected value for the $\\Delta$, since this is for H1 which means the same or better, we pick the bare mimium we need to make that statement that is \"teh same\" and an expectd value of zero fro the Delta. Under these condition we can pool the samples to get an estimator of the variance and standard error as we are assuming they are distributed indentically\n",
    "\n",
    "$SE | H1 = WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we compute the probabilit of the esimator coming up \"higher\" but with the same critical value as it was established ahead of time with the significance level alpha.\n",
    "\n",
    "The Beta is the probability that we observed something lower and up to the the critical value defined but inverting the p-value but under H1, meaning the distribution is centered on the H1 expected value, in our case it is zero.\n",
    "\n",
    "This computation will give us whatever it gives us, if we want to TARGET a Beta of 0.2 (meaning a power of 0.9), we can conpute the sample sizes (implied in SE) that will make us reach that level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"probability of false negative a.k.a β a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Power (1 - β): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H1 - 6 * sigma_H1\n",
    "right = mu_H1 + 6 * sigma_H1\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H1, scale=sigma_H1)\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade left tail\n",
    "mask = xs <= critical_value\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Left tail p = {critical_value:.4g}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H1\n",
    "ax.axvline(mu_H1, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H1 = {mu_H1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST the Bayesian approach is concpetually simpler, instead of atificially considering onl 2 possible expected value bondaries for $\\Delta$ one for H0 and one for H1, we assume we just don't know that $E(\\Delta)$ can take any value between 0 and 1 , and this is just itself random variable but one that we can quantify accross all possible world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative approach assumimng we know nothing (a.k.a non informative prior)\n",
    "\n",
    "For the Variant A, assuming we start with an non informative prior meaning that we have no idea, p_A could range from 0 to 1 with equal probability model by the beta function $Beta(1,1)$  \n",
    " \n",
    "after n trials and k success the posteriod probability distribution due to the property of the Beta function is\n",
    " \n",
    "$Beta(xA +1,n_A − xA+1)$ \n",
    "\n",
    "This derived through Bayes Theorem and how the Beta function can be integrated\n",
    "\n",
    "The expected value for $E(Beta(\\alpha,\\beta)) = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "which for a bayesian posterior becomes\n",
    "\n",
    "$E(Beta(xA+1,nA−xA+1)) = \\frac{xA+1}{xA+1 + nA−xA+1} = \\frac{xA+1}{nA+2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "# Ensure we use the beta distribution from scipy.stats\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 99% credible interval (1th and 99th percentiles)\n",
    "p_L_99 = beta_dist.ppf(0.005, alpha, beta_param)\n",
    "p_U_99 = beta_dist.ppf(0.995, alpha, beta_param)\n",
    "# Output the result\n",
    "print(f\"99% Credible Interval for p: [{p_L_99:.4f}, {p_U_99:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
