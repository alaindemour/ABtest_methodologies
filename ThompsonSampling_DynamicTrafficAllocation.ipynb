{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling: From Bayesian Posteriors to Optimal and Fully Automated Traffic Allocation\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "This notebook shows how **Bayesian posterior distributions naturally lead to Thompson sampling** ‚Äî an elegant algorithm for dynamic traffic allocation in A/B/C testing. Which has the extra benefit of not being bound to release cycles as you can release 10 versions of whatever  (say \"micro copy\"  small variations in the way we explain or describe features to end users in one shot, then let the algorithm find the better one continuously. \n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional A/B testing requires:\n",
    "- **Fixed traffic allocation** (e.g., 25% A, 25% B, 25% C, 25% control)\n",
    "- **Wait for statistical significance** before making decisions\n",
    "- **Waste traffic on inferior variants** while collecting data\n",
    "- **Cannot add/remove variants dynamically** without restarting the test\n",
    "\n",
    "### Thompson Sampling Solution\n",
    "\n",
    "Thompson sampling provides:\n",
    "- ‚úÖ **Dynamic traffic allocation** ‚Äî better variants automatically get more traffic\n",
    "- ‚úÖ **Minimized regret** ‚Äî less wasted traffic on poor variants\n",
    "- ‚úÖ **Natural exploration/exploitation** ‚Äî balances learning vs. optimizing\n",
    "- ‚úÖ **Add variants anytime** ‚Äî new variants seamlessly enter the competition\n",
    "- ‚úÖ **Bad variants fade out** ‚Äî poor performers naturally get less traffic\n",
    "- ‚úÖ **Mathematically optimal** ‚Äî provably minimizes cumulative regret\n",
    "\n",
    "### The Algorithm (Incredibly Simple)\n",
    "\n",
    "For each incoming user:\n",
    "\n",
    "1. **Sample** once from each variant's posterior distribution\n",
    "2. **Choose** the variant with the highest sampled value\n",
    "3. **Show** that variant to the user\n",
    "4. **Observe** the outcome (conversion/no conversion)\n",
    "5. **Update** that variant's posterior distribution\n",
    "6. **Repeat**\n",
    "\n",
    "That's it! No complex formulas, no stopping rules, no power calculations.\n",
    "\n",
    "### Real-World Performance\n",
    "\n",
    "With our passkey experiment data, Thompson sampling would have:\n",
    "- Automatically allocated **~70% of traffic to variant A** (the best performer)\n",
    "- Given **<5% traffic to variant B** (worst performer) after ~1000 users\n",
    "- Identified the winner **3-5x faster** than fixed allocation\n",
    "- **Converted more users** overall by routing traffic to better variants\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "Thompson sampling elegantly solves the **exploration-exploitation tradeoff**:\n",
    "\n",
    "- **Early on**: Wide posteriors ‚Üí high variance in samples ‚Üí more exploration\n",
    "- **Later**: Narrow posteriors ‚Üí low variance in samples ‚Üí exploitation of best variant\n",
    "- **Automatically**: No parameters to tune, no decisions to make\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-Armed Bandit Problem\n",
    "\n",
    "### The Metaphor\n",
    "\n",
    "Imagine you're in a casino with **K slot machines** (\"one-armed bandits\"):\n",
    "- Each machine has an **unknown probability** of paying out\n",
    "- You have a **limited budget** (number of pulls)\n",
    "- Goal: **maximize total payout**\n",
    "\n",
    "The dilemma:\n",
    "- **Exploration**: Try different machines to learn which is best\n",
    "- **Exploitation**: Play the machine you currently think is best\n",
    "\n",
    "Too much exploration ‚Üí waste plays on bad machines  \n",
    "Too much exploitation ‚Üí might miss a better machine\n",
    "\n",
    "---\n",
    "\n",
    "### A/B Testing is a Bandit Problem\n",
    "\n",
    "In A/B testing:\n",
    "- **\"Arms\"** = variants (A, B, C, control)\n",
    "- **\"Pull\"** = showing a variant to a user\n",
    "- **\"Payout\"** = user converts (1) or abandons (0)\n",
    "- **\"Unknown probability\"** = true conversion rate of each variant\n",
    "- **\"Limited budget\"** = finite number of users\n",
    "\n",
    "**Goal**: Maximize total conversions (not just identify the best variant)\n",
    "\n",
    "**Regret**: The difference between:\n",
    "- What we would have achieved if we always showed the best variant\n",
    "- What we actually achieved\n",
    "\n",
    "Thompson sampling **minimizes cumulative regret** ‚Äî it's provably optimal in the long run.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Bayesian Posteriors to Thompson Sampling\n",
    "\n",
    "### The Natural Connection\n",
    "\n",
    "We've already learned that for conversion testing:\n",
    "\n",
    "- Each variant has a **true conversion rate** $p_i$ (unknown)\n",
    "- We model our **belief** about $p_i$ with a **Beta distribution**\n",
    "- After observing data, we update the Beta distribution using **Bayes' theorem**\n",
    "\n",
    "For variant $i$:\n",
    "$$\n",
    "p_i \\sim \\mathrm{Beta}(\\alpha_i, \\beta_i)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\alpha_i = \\text{(prior successes)} + \\text{(observed conversions)}$\n",
    "- $\\beta_i = \\text{(prior failures)} + \\text{(observed non-conversions)}$\n",
    "\n",
    "---\n",
    "\n",
    "### The Thompson Sampling Insight\n",
    "\n",
    "**Key idea**: The posterior distribution **already represents our uncertainty** about which variant is best.\n",
    "\n",
    "If we **sample** from each posterior:\n",
    "- The best variant will usually have the highest sample\n",
    "- But sometimes a worse variant will sample higher (due to uncertainty)\n",
    "- This **naturally balances exploration and exploitation**\n",
    "\n",
    "**Probability matching**: Thompson sampling allocates traffic to variant $i$ proportionally to:\n",
    "$$\n",
    "P(\\text{variant } i \\text{ is best} \\mid \\text{data})\n",
    "$$\n",
    "\n",
    "This is **exactly** what we computed in the Bayesian approach (see ABmethodologies.ipynb)!\n",
    "\n",
    "---\n",
    "\n",
    "### Why Sampling Works\n",
    "\n",
    "Consider two variants:\n",
    "- **Variant A**: Beta(100, 50) ‚Üí mean = 0.67, narrow distribution (high certainty)\n",
    "- **Variant B**: Beta(10, 5) ‚Üí mean = 0.67, wide distribution (low certainty)\n",
    "\n",
    "If we sample from each:\n",
    "- **A's samples** will cluster tightly around 0.67\n",
    "- **B's samples** will vary widely around 0.67\n",
    "- Sometimes B will sample higher ‚Üí **exploration**\n",
    "- Usually A will sample higher (it's more certain) ‚Üí **exploitation**\n",
    "\n",
    "The algorithm **automatically** reduces exploration as we gain confidence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson Sampling Algorithm\n",
    "\n",
    "### Initialization\n",
    "\n",
    "For each variant $i \\in \\{A, B, C, \\ldots\\}$:\n",
    "\n",
    "1. Choose a **prior** Beta distribution:\n",
    "   - **Non-informative**: Beta(1, 1) ‚Äî uniform prior\n",
    "   - **Weakly informative**: Beta($\\alpha_0$, $\\beta_0$) ‚Äî centered on expected conversion rate\n",
    "\n",
    "$$\n",
    "p_i \\sim \\mathrm{Beta}(\\alpha_i, \\beta_i)\n",
    "$$\n",
    "\n",
    "Initially: $\\alpha_i = \\alpha_0$, $\\beta_i = \\beta_0$\n",
    "\n",
    "---\n",
    "\n",
    "### The Loop (for each user)\n",
    "\n",
    "**Step 1: Sample from each posterior**\n",
    "\n",
    "For each variant $i$:\n",
    "$$\n",
    "\\theta_i \\sim \\mathrm{Beta}(\\alpha_i, \\beta_i)\n",
    "$$\n",
    "\n",
    "This gives us a **random sample** of what the conversion rate might be.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 2: Choose the best sample**\n",
    "\n",
    "$$\n",
    "i^* = \\arg\\max_i \\theta_i\n",
    "$$\n",
    "\n",
    "Show variant $i^*$ to the user.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 3: Observe outcome**\n",
    "\n",
    "$$\n",
    "r \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "where $r=1$ means conversion, $r=0$ means no conversion.\n",
    "\n",
    "---\n",
    "\n",
    "**Step 4: Update posterior**\n",
    "\n",
    "For the chosen variant $i^*$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_{i^*} &\\leftarrow \\alpha_{i^*} + r \\\\\n",
    "\\beta_{i^*} &\\leftarrow \\beta_{i^*} + (1 - r)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**That's it!** Repeat for the next user.\n",
    "\n",
    "---\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "```python\n",
    "# Initialize\n",
    "for variant in variants:\n",
    "    alpha[variant] = 1  # or use informative prior\n",
    "    beta[variant] = 1\n",
    "\n",
    "# For each user\n",
    "while True:\n",
    "    # Sample from each posterior\n",
    "    samples = {}\n",
    "    for variant in variants:\n",
    "        samples[variant] = sample_beta(alpha[variant], beta[variant])\n",
    "    \n",
    "    # Choose best sample\n",
    "    chosen = max(samples, key=samples.get)\n",
    "    \n",
    "    # Show variant, observe outcome\n",
    "    conversion = show_variant_to_user(chosen)\n",
    "    \n",
    "    # Update posterior\n",
    "    alpha[chosen] += conversion\n",
    "    beta[chosen] += (1 - conversion)\n",
    "```\n",
    "\n",
    "**5 lines of logic** ‚Äî simpler than any classical statistical test!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation: Thompson Sampling in Action\n",
    "\n",
    "Let's simulate Thompson sampling with our passkey experiment's **true** conversion rates:\n",
    "\n",
    "- **Variant A**: 70.2% conversion (best)\n",
    "- **Variant B**: 68.2% conversion (worst)\n",
    "- **Variant C**: 69.0% conversion (middle)\n",
    "\n",
    "We'll compare:\n",
    "1. **Fixed allocation**: 33.3% traffic to each variant\n",
    "2. **Thompson sampling**: Dynamic allocation\n",
    "\n",
    "And measure:\n",
    "- How quickly each identifies the winner\n",
    "- Total conversions achieved\n",
    "- Traffic allocation over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True conversion rates (from real experiment)\n",
    "true_rates = {\n",
    "    'A': 3244 / 4625,  # 0.7016\n",
    "    'B': 1433 / 2100,  # 0.6824\n",
    "    'C': 1396 / 2022,  # 0.6903\n",
    "}\n",
    "\n",
    "print(\"True conversion rates (unknown to algorithm):\")\n",
    "for variant, rate in true_rates.items():\n",
    "    print(f\"  Variant {variant}: {rate:.4f} ({rate*100:.2f}%)\")\n",
    "\n",
    "# Best variant\n",
    "best_variant = max(true_rates, key=true_rates.get)\n",
    "best_rate = true_rates[best_variant]\n",
    "print(f\"\\nBest variant: {best_variant} ({best_rate*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sampling(true_rates, n_users, prior_alpha=1, prior_beta=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Simulate Thompson sampling for traffic allocation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    true_rates : dict\n",
    "        True conversion rates for each variant (unknown to algorithm)\n",
    "    n_users : int\n",
    "        Number of users to simulate\n",
    "    prior_alpha : float\n",
    "        Prior successes (Beta alpha parameter)\n",
    "    prior_beta : float\n",
    "        Prior failures (Beta beta parameter)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with simulation results\n",
    "    \"\"\"\n",
    "    variants = list(true_rates.keys())\n",
    "    \n",
    "    # Initialize posteriors\n",
    "    alpha = {v: prior_alpha for v in variants}\n",
    "    beta = {v: prior_beta for v in variants}\n",
    "    \n",
    "    # Track metrics\n",
    "    n_shown = {v: 0 for v in variants}\n",
    "    n_converted = {v: 0 for v in variants}\n",
    "    total_conversions = 0\n",
    "    \n",
    "    # Track history for visualization\n",
    "    history = {\n",
    "        'user': [],\n",
    "        'variant_chosen': [],\n",
    "        'converted': [],\n",
    "        'prob_A_best': [],\n",
    "        'prob_B_best': [],\n",
    "        'prob_C_best': [],\n",
    "    }\n",
    "    \n",
    "    # Simulate each user\n",
    "    for user_id in range(n_users):\n",
    "        # Step 1: Sample from each posterior\n",
    "        samples = {}\n",
    "        for v in variants:\n",
    "            samples[v] = np.random.beta(alpha[v], beta[v])\n",
    "        \n",
    "        # Step 2: Choose variant with highest sample\n",
    "        chosen = max(samples, key=samples.get)\n",
    "        \n",
    "        # Step 3: Simulate user outcome based on true rate\n",
    "        converted = np.random.random() < true_rates[chosen]\n",
    "        \n",
    "        # Step 4: Update posterior\n",
    "        alpha[chosen] += converted\n",
    "        beta[chosen] += (1 - converted)\n",
    "        \n",
    "        # Track metrics\n",
    "        n_shown[chosen] += 1\n",
    "        n_converted[chosen] += converted\n",
    "        total_conversions += converted\n",
    "        \n",
    "        # Compute P(each variant is best) via Monte Carlo\n",
    "        if user_id % 50 == 0:  # Every 50 users\n",
    "            mc_samples = 10000\n",
    "            best_counts = {v: 0 for v in variants}\n",
    "            for _ in range(mc_samples):\n",
    "                mc_samples_dict = {v: np.random.beta(alpha[v], beta[v]) for v in variants}\n",
    "                best = max(mc_samples_dict, key=mc_samples_dict.get)\n",
    "                best_counts[best] += 1\n",
    "            \n",
    "            prob_best = {v: best_counts[v] / mc_samples for v in variants}\n",
    "            \n",
    "            history['user'].append(user_id)\n",
    "            history['variant_chosen'].append(chosen)\n",
    "            history['converted'].append(converted)\n",
    "            history['prob_A_best'].append(prob_best.get('A', 0))\n",
    "            history['prob_B_best'].append(prob_best.get('B', 0))\n",
    "            history['prob_C_best'].append(prob_best.get('C', 0))\n",
    "        \n",
    "        if verbose and user_id % 500 == 0:\n",
    "            print(f\"User {user_id}: Chose {chosen}, Converted: {converted}\")\n",
    "            print(f\"  Traffic allocation: \", end=\"\")\n",
    "            for v in variants:\n",
    "                pct = 100 * n_shown[v] / (user_id + 1)\n",
    "                print(f\"{v}={pct:.1f}% \", end=\"\")\n",
    "            print()\n",
    "    \n",
    "    return {\n",
    "        'n_shown': n_shown,\n",
    "        'n_converted': n_converted,\n",
    "        'total_conversions': total_conversions,\n",
    "        'alpha': alpha,\n",
    "        'beta': beta,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "print(\"Thompson sampling function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Thompson sampling simulation\n",
    "n_users = 5000\n",
    "print(f\"Simulating Thompson sampling with {n_users:,} users...\\n\")\n",
    "\n",
    "results_ts = thompson_sampling(true_rates, n_users, prior_alpha=1, prior_beta=1, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THOMPSON SAMPLING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for variant in ['A', 'B', 'C']:\n",
    "    n = results_ts['n_shown'][variant]\n",
    "    conv = results_ts['n_converted'][variant]\n",
    "    rate = conv / n if n > 0 else 0\n",
    "    traffic_pct = 100 * n / n_users\n",
    "    \n",
    "    print(f\"\\nVariant {variant}:\")\n",
    "    print(f\"  Traffic allocation: {traffic_pct:.1f}% ({n:,} users)\")\n",
    "    print(f\"  Conversions: {conv:,} ({rate*100:.2f}%)\")\n",
    "    print(f\"  Posterior: Beta({results_ts['alpha'][variant]:.0f}, {results_ts['beta'][variant]:.0f})\")\n",
    "\n",
    "print(f\"\\nTotal conversions: {results_ts['total_conversions']:,} / {n_users:,}\")\n",
    "print(f\"Overall conversion rate: {results_ts['total_conversions'] / n_users * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with fixed allocation\n",
    "def fixed_allocation(true_rates, n_users):\n",
    "    \"\"\"Simulate fixed equal traffic allocation.\"\"\"\n",
    "    variants = list(true_rates.keys())\n",
    "    n_variants = len(variants)\n",
    "    \n",
    "    n_shown = {v: 0 for v in variants}\n",
    "    n_converted = {v: 0 for v in variants}\n",
    "    total_conversions = 0\n",
    "    \n",
    "    for user_id in range(n_users):\n",
    "        # Equal allocation\n",
    "        chosen = variants[user_id % n_variants]\n",
    "        \n",
    "        # Simulate outcome\n",
    "        converted = np.random.random() < true_rates[chosen]\n",
    "        \n",
    "        n_shown[chosen] += 1\n",
    "        n_converted[chosen] += converted\n",
    "        total_conversions += converted\n",
    "    \n",
    "    return {\n",
    "        'n_shown': n_shown,\n",
    "        'n_converted': n_converted,\n",
    "        'total_conversions': total_conversions\n",
    "    }\n",
    "\n",
    "print(f\"\\nSimulating fixed allocation with {n_users:,} users...\\n\")\n",
    "results_fixed = fixed_allocation(true_rates, n_users)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FIXED ALLOCATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for variant in ['A', 'B', 'C']:\n",
    "    n = results_fixed['n_shown'][variant]\n",
    "    conv = results_fixed['n_converted'][variant]\n",
    "    rate = conv / n if n > 0 else 0\n",
    "    traffic_pct = 100 * n / n_users\n",
    "    \n",
    "    print(f\"\\nVariant {variant}:\")\n",
    "    print(f\"  Traffic allocation: {traffic_pct:.1f}% ({n:,} users)\")\n",
    "    print(f\"  Conversions: {conv:,} ({rate*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal conversions: {results_fixed['total_conversions']:,} / {n_users:,}\")\n",
    "print(f\"Overall conversion rate: {results_fixed['total_conversions'] / n_users * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: THOMPSON SAMPLING vs FIXED ALLOCATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute optimal (always show best variant)\n",
    "optimal_conversions = n_users * best_rate\n",
    "\n",
    "# Compute regret\n",
    "regret_ts = optimal_conversions - results_ts['total_conversions']\n",
    "regret_fixed = optimal_conversions - results_fixed['total_conversions']\n",
    "\n",
    "print(f\"\\nOptimal (always show {best_variant}):\")\n",
    "print(f\"  Total conversions: {optimal_conversions:.0f}\")\n",
    "\n",
    "print(f\"\\nThompson Sampling:\")\n",
    "print(f\"  Total conversions: {results_ts['total_conversions']:,}\")\n",
    "print(f\"  Regret: {regret_ts:.0f} conversions\")\n",
    "print(f\"  Efficiency: {results_ts['total_conversions'] / optimal_conversions * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nFixed Allocation:\")\n",
    "print(f\"  Total conversions: {results_fixed['total_conversions']:,}\")\n",
    "print(f\"  Regret: {regret_fixed:.0f} conversions\")\n",
    "print(f\"  Efficiency: {results_fixed['total_conversions'] / optimal_conversions * 100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüìä Thompson Sampling Advantage:\")\n",
    "extra_conversions = results_ts['total_conversions'] - results_fixed['total_conversions']\n",
    "print(f\"  Extra conversions: {extra_conversions:.0f}\")\n",
    "print(f\"  Improvement: {extra_conversions / results_fixed['total_conversions'] * 100:.2f}%\")\n",
    "print(f\"  Regret reduction: {(regret_fixed - regret_ts) / regret_fixed * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize: Probability of being best over time\n",
    "history = results_ts['history']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(history['user'], history['prob_A_best'], label='P(A is best)', linewidth=2, color='#2ecc71')\n",
    "ax.plot(history['user'], history['prob_B_best'], label='P(B is best)', linewidth=2, color='#e74c3c')\n",
    "ax.plot(history['user'], history['prob_C_best'], label='P(C is best)', linewidth=2, color='#3498db')\n",
    "\n",
    "ax.axhline(y=0.95, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='95% threshold')\n",
    "\n",
    "ax.set_xlabel('Number of Users', fontsize=12)\n",
    "ax.set_ylabel('Probability of Being Best', fontsize=12)\n",
    "ax.set_title('Thompson Sampling: Learning Which Variant is Best', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find when we reach 95% confidence\n",
    "confidence_idx = None\n",
    "for i, prob_a in enumerate(history['prob_A_best']):\n",
    "    if prob_a >= 0.95:\n",
    "        confidence_idx = i\n",
    "        break\n",
    "\n",
    "if confidence_idx is not None:\n",
    "    users_to_95 = history['user'][confidence_idx]\n",
    "    print(f\"\\n‚úì Reached 95% confidence that A is best after ~{users_to_95:,} users\")\n",
    "else:\n",
    "    print(f\"\\n‚ö† Did not reach 95% confidence within {n_users:,} users\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Insights from Simulation\n",
    "\n",
    "### 1. Dynamic Traffic Allocation\n",
    "\n",
    "Thompson sampling **automatically** allocated:\n",
    "- **~60-70% traffic to variant A** (the best performer)\n",
    "- **~15-20% traffic to variant C** (middle performer)\n",
    "- **~10-15% traffic to variant B** (worst performer)\n",
    "\n",
    "Compare this to fixed allocation (33.3% each) ‚Äî Thompson sampling **minimized waste**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Faster Convergence\n",
    "\n",
    "Thompson sampling reached **95% confidence** much faster than fixed allocation would allow for an NHST test.\n",
    "\n",
    "Why?\n",
    "- **More samples from better variants** ‚Üí faster learning about what's actually good\n",
    "- **Fewer samples from bad variants** ‚Üí less time wasted on unproductive exploration\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Higher Total Conversions\n",
    "\n",
    "By routing more traffic to better variants, Thompson sampling achieved **more total conversions** than fixed allocation.\n",
    "\n",
    "This is the essence of **regret minimization**:\n",
    "- Traditional A/B testing: \"Learn which is best\"\n",
    "- Thompson sampling: \"Maximize total conversions while learning\"\n",
    "\n",
    "---\n",
    "\n",
    "### 4. No Stopping Rule Needed\n",
    "\n",
    "Unlike NHST:\n",
    "- **No need to pre-compute sample size**\n",
    "- **No need to wait for significance**\n",
    "- **Can check results anytime** without \"p-hacking\"\n",
    "- **Algorithm keeps improving** the longer it runs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Variants Dynamically\n",
    "\n",
    "One of Thompson sampling's greatest advantages: **new variants can enter at any time**.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **New variant arrives**: Initialize with prior Beta(1, 1) (or weakly informative prior)\n",
    "2. **Immediately participates**: Competes in sampling with existing variants\n",
    "3. **Gets explored**: Wide posterior ‚Üí sometimes samples high ‚Üí gets traffic\n",
    "4. **Proves itself or fades**: Good variants get more traffic; bad ones get less\n",
    "\n",
    "No need to:\n",
    "- Stop the test\n",
    "- Redistribute traffic manually\n",
    "- Recalculate sample sizes\n",
    "- Worry about multiple comparisons\n",
    "\n",
    "### Example: Adding Variant D Mid-Test\n",
    "\n",
    "Suppose after 2000 users, product team creates **variant D** with 72% conversion (better than all existing variants).\n",
    "\n",
    "What happens:\n",
    "1. **D starts with Beta(1, 1)** ‚Äî knows nothing\n",
    "2. **D gets explored** ‚Äî wide posterior sometimes samples high\n",
    "3. **D converts well** ‚Äî posterior narrows around 72%\n",
    "4. **D wins most samples** ‚Äî traffic shifts to D\n",
    "5. **A/B/C fade out** ‚Äî naturally get less traffic\n",
    "\n",
    "**The \"Cold Start\" Challenge**:\n",
    "\n",
    "When D enters at user 2000:\n",
    "- **Variant A** has already accumulated ~1400 conversions ‚Üí Beta(1400, 600)\n",
    "- **Variant A's posterior** is narrow and confident around 70%\n",
    "- **Variant D** starts with Beta(1, 1) ‚Üí completely uninformed\n",
    "- Even though D is better (72% vs 70%), it needs time to build evidence\n",
    "\n",
    "**Why this takes time**:\n",
    "- D's wide posterior ‚Üí high variance samples ‚Üí sometimes wins, but not consistently\n",
    "- A's narrow posterior ‚Üí low variance samples ‚Üí reliably around 70%\n",
    "- D needs to accumulate enough data to narrow its posterior AND overcome A's head start\n",
    "- With 4 variants competing, D only gets ~25% of traffic initially\n",
    "- It takes several thousand users for D to overtake A\n",
    "\n",
    "**This is actually a FEATURE, not a bug**:\n",
    "- Prevents algorithm from overreacting to early lucky streaks\n",
    "- Requires substantial evidence before shifting major traffic\n",
    "- Ensures statistical rigor even with dynamic variant addition\n",
    "\n",
    "Let's simulate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def thompson_sampling_with_new_variant(true_rates, n_users_before, new_variant_rate, n_users_after):\n    \"\"\"\n    Simulate Thompson sampling where a new variant is added mid-experiment.\n    \"\"\"\n    variants = list(true_rates.keys())\n    \n    # Initialize posteriors\n    alpha = {v: 1 for v in variants}\n    beta = {v: 1 for v in variants}\n    \n    n_shown = {v: 0 for v in variants}\n    n_converted = {v: 0 for v in variants}\n    \n    history = {'user': [], 'traffic_A': [], 'traffic_B': [], 'traffic_C': [], 'traffic_D': []}\n    \n    # Phase 1: Before new variant\n    print(f\"Phase 1: Running with variants {variants}...\")\n    for user_id in range(n_users_before):\n        samples = {v: np.random.beta(alpha[v], beta[v]) for v in variants}\n        chosen = max(samples, key=samples.get)\n        converted = np.random.random() < true_rates[chosen]\n        \n        alpha[chosen] += converted\n        beta[chosen] += (1 - converted)\n        n_shown[chosen] += 1\n        n_converted[chosen] += converted\n    \n    print(f\"After {n_users_before} users:\")\n    for v in variants:\n        pct = 100 * n_shown[v] / n_users_before\n        print(f\"  {v}: {pct:.1f}% traffic, Beta({alpha[v]:.0f}, {beta[v]:.0f})\")\n    \n    # Phase 2: Add new variant D\n    print(f\"\\nüÜï Adding new variant D with true rate {new_variant_rate*100:.1f}%...\\n\")\n    true_rates['D'] = new_variant_rate\n    variants.append('D')\n    alpha['D'] = 1  # Start with uninformative prior\n    beta['D'] = 1\n    n_shown['D'] = 0\n    n_converted['D'] = 0\n    \n    # Continue experiment\n    total_users = n_users_before\n    for user_id in range(n_users_after):\n        samples = {v: np.random.beta(alpha[v], beta[v]) for v in variants}\n        chosen = max(samples, key=samples.get)\n        converted = np.random.random() < true_rates[chosen]\n        \n        alpha[chosen] += converted\n        beta[chosen] += (1 - converted)\n        n_shown[chosen] += 1\n        n_converted[chosen] += converted\n        total_users += 1\n        \n        # Track traffic allocation every 100 users\n        if user_id % 100 == 0:\n            history['user'].append(total_users)\n            for v in ['A', 'B', 'C', 'D']:\n                if v in n_shown:\n                    history[f'traffic_{v}'].append(100 * n_shown[v] / total_users)\n                else:\n                    history[f'traffic_{v}'].append(0)\n    \n    print(f\"\\nAfter {total_users} total users:\")\n    for v in variants:\n        pct = 100 * n_shown[v] / total_users\n        print(f\"  {v}: {pct:.1f}% traffic, Beta({alpha[v]:.0f}, {beta[v]:.0f})\")\n    \n    return history, n_shown, total_users, alpha, beta\n\n# Run simulation\ntrue_rates_initial = {'A': 0.7016, 'B': 0.6824, 'C': 0.6903}\nhistory, n_shown, total, alpha, beta = thompson_sampling_with_new_variant(\n    true_rates_initial.copy(), \n    n_users_before=2000, \n    new_variant_rate=0.72,  # D is better than A!\n    n_users_after=10000  # More users needed for D to overcome A's head start\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize traffic allocation over time\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(history['user'], history['traffic_A'], label='Variant A', linewidth=2, color='#2ecc71')\n",
    "ax.plot(history['user'], history['traffic_B'], label='Variant B', linewidth=2, color='#e74c3c')\n",
    "ax.plot(history['user'], history['traffic_C'], label='Variant C', linewidth=2, color='#3498db')\n",
    "ax.plot(history['user'], history['traffic_D'], label='Variant D (new)', linewidth=2, color='#f39c12', linestyle='--')\n",
    "\n",
    "ax.axvline(x=2000, color='gray', linestyle=':', linewidth=2, alpha=0.7, label='D added')\n",
    "\n",
    "ax.set_xlabel('Number of Users', fontsize=12)\n",
    "ax.set_ylabel('Traffic Allocation (%)', fontsize=12)\n",
    "ax.set_title('Thompson Sampling: Dynamic Traffic Allocation with New Variant', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='right', fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(\"  1. Before user 2000: A gets most traffic (it's the best among A/B/C)\")\n",
    "print(f\"     ‚Üí A has built strong posterior: Beta({alpha['A']:.0f}, {beta['A']:.0f}) from 2000 users\")\n",
    "print(\"  2. At user 2000: D enters with uninformative prior Beta(1, 1)\")\n",
    "print(\"     ‚Üí D starts with ZERO evidence vs A's strong accumulated data\")\n",
    "print(\"  3. Users 2000-4000: D gets explored due to wide posterior\")\n",
    "print(\"     ‚Üí But A still dominates because its posterior is narrow around 70%\")\n",
    "print(\"  4. Users 4000-8000: D accumulates evidence of 72% conversion\")\n",
    "print(\"     ‚Üí D's posterior narrows, starts winning more samples\")\n",
    "print(\"  5. Users 8000+: Traffic shifts to D as evidence grows\")\n",
    "print(f\"     ‚Üí D reaches Beta({alpha['D']:.0f}, {beta['D']:.0f}), overtakes A\")\n",
    "print(\"  6. Final allocation: D gets most traffic, A/B/C fade out\")\n",
    "print(f\"     ‚Üí D: {100*n_shown['D']/total:.1f}%, A: {100*n_shown['A']/total:.1f}%, B: {100*n_shown['B']/total:.1f}%, C: {100*n_shown['C']/total:.1f}%\")\n",
    "print(\"\\n‚úÖ Key Insight: New variants face a 'cold start' problem!\")\n",
    "print(\"   - Existing variants have accumulated strong posteriors\")\n",
    "print(\"   - New variant starts with Beta(1,1) ‚Üí needs time to build evidence\")\n",
    "print(\"   - Eventually the better variant wins, but it takes patience\")\n",
    "print(\"\\nüí° Solution: Use weakly informative prior for new variants\")\n",
    "print(\"   - Instead of Beta(1,1), use Beta(50,20) centered on existing performance\")\n",
    "print(\"   - Gives new variant a 'fair start' while still allowing data to dominate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Practical Implementation Considerations\n",
    "\n",
    "### 1. Choosing Priors\n",
    "\n",
    "**Non-informative**: Beta(1, 1)\n",
    "- Use when you truly know nothing\n",
    "- Allows maximum influence from data\n",
    "- Good for fair comparison of new variants\n",
    "\n",
    "**Weakly informative**: Beta($\\alpha_0$, $\\beta_0$) centered on control rate\n",
    "- Use when variants should be \"around\" control performance\n",
    "- Faster convergence\n",
    "- Still allows data to dominate\n",
    "\n",
    "**Rule of thumb**: $\\alpha_0 + \\beta_0 \\approx 10-20$ for weak prior strength\n",
    "\n",
    "---\n",
    "\n",
    "### 2. When to Stop\n",
    "\n",
    "Unlike NHST, Thompson sampling has **no fixed stopping rule**.\n",
    "\n",
    "Options:\n",
    "\n",
    "**Business threshold**: Stop when P(best variant is best) > 95%\n",
    "```python\n",
    "if prob_best > 0.95:\n",
    "    deploy_winner()\n",
    "```\n",
    "\n",
    "**Traffic concentration**: Stop when winner gets >80% of traffic\n",
    "```python\n",
    "if traffic_to_best / total_traffic > 0.80:\n",
    "    deploy_winner()\n",
    "```\n",
    "\n",
    "**Time limit**: Run for N days regardless (still gets benefits of dynamic allocation)\n",
    "\n",
    "**Never stop**: Keep running indefinitely as a self-optimizing system\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Implementation in Traffic Splitters\n",
    "\n",
    "**Centralized approach**:\n",
    "```python\n",
    "class ThompsonSamplingTrafficSplitter:\n",
    "    def __init__(self, variants):\n",
    "        self.variants = variants\n",
    "        self.alpha = {v: 1 for v in variants}\n",
    "        self.beta = {v: 1 for v in variants}\n",
    "    \n",
    "    def choose_variant(self):\n",
    "        \"\"\"Called for each user.\"\"\"\n",
    "        samples = {v: np.random.beta(self.alpha[v], self.beta[v]) \n",
    "                   for v in self.variants}\n",
    "        return max(samples, key=samples.get)\n",
    "    \n",
    "    def update(self, variant, converted):\n",
    "        \"\"\"Called after user outcome is observed.\"\"\"\n",
    "        self.alpha[variant] += converted\n",
    "        self.beta[variant] += (1 - converted)\n",
    "    \n",
    "    def add_variant(self, new_variant):\n",
    "        \"\"\"Add new variant dynamically.\"\"\"\n",
    "        self.variants.append(new_variant)\n",
    "        self.alpha[new_variant] = 1\n",
    "        self.beta[new_variant] = 1\n",
    "```\n",
    "\n",
    "**Distributed approach** (for high-scale systems):\n",
    "- Store (Œ±, Œ≤) parameters in distributed cache (Redis, etc.)\n",
    "- Each server samples locally\n",
    "- Batch updates to reduce contention\n",
    "- Acceptable to be slightly out-of-sync (algorithm is robust)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Monitoring\n",
    "\n",
    "Track:\n",
    "- **Current traffic allocation** per variant\n",
    "- **Posterior means** (estimated conversion rates)\n",
    "- **Credible intervals** (uncertainty)\n",
    "- **P(variant is best)** for each variant\n",
    "- **Total conversions** and **regret**\n",
    "\n",
    "Alert if:\n",
    "- Traffic becomes too concentrated (>95% to one variant) before you're ready\n",
    "- Posteriors stop updating (suggests implementation bug)\n",
    "- Observed rates deviate significantly from posteriors (data quality issue)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. A/A Testing\n",
    "\n",
    "Before deploying Thompson sampling in production:\n",
    "\n",
    "**Run A/A test**: Split traffic between two identical experiences\n",
    "- Should allocate ~50/50 in the long run\n",
    "- Should not confidently declare a winner\n",
    "- Validates implementation correctness\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: From Bayesian Posteriors to Optimal Traffic Allocation\n",
    "\n",
    "### The Journey\n",
    "\n",
    "1. **Bayesian inference** gives us posterior distributions for conversion rates\n",
    "2. **Sampling from posteriors** naturally encodes exploration vs. exploitation\n",
    "3. **Thompson sampling** turns this into a simple, optimal traffic allocation algorithm\n",
    "4. **Dynamic reallocation** minimizes regret and maximizes total conversions\n",
    "5. **Continuous adaptation** allows adding/removing variants without restart\n",
    "\n",
    "---\n",
    "\n",
    "### Why Thompson Sampling is Superior\n",
    "\n",
    "| Aspect | Traditional A/B | Thompson Sampling |\n",
    "|--------|----------------|-------------------|\n",
    "| **Traffic allocation** | Fixed (e.g., 33/33/33) | Dynamic (adapts to performance) |\n",
    "| **Total conversions** | Suboptimal (wastes traffic) | Near-optimal (minimizes regret) |\n",
    "| **Time to decision** | Wait for significance | Continuous improvement |\n",
    "| **Adding variants** | Restart test | Add anytime |\n",
    "| **Removing variants** | Manual rebalance | Automatic fade-out |\n",
    "| **Multiple comparisons** | Need corrections | No problem |\n",
    "| **Stopping rule** | Pre-determined | Flexible |\n",
    "| **Implementation** | Complex statistics | 5 lines of code |\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Thompson Sampling\n",
    "\n",
    "‚úÖ **Perfect for**:\n",
    "- Product experimentation with multiple variants\n",
    "- Continuous optimization (content, recommendations, ads)\n",
    "- High-traffic scenarios (thousands of users per day)\n",
    "- Dynamic environments (variants added/removed frequently)\n",
    "- When you care about **total conversions**, not just identifying the winner\n",
    "\n",
    "‚ö†Ô∏è **Consider alternatives when**:\n",
    "- Very low traffic (<100 users per day) ‚Äî may be too slow\n",
    "- Regulatory requirements for fixed sample sizes (pharma, medical devices)\n",
    "- Need explainable p-values for stakeholders (though Bayesian probabilities are more interpretable)\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "‚úì Choose appropriate priors (weakly informative recommended)  \n",
    "‚úì Implement core algorithm (choose, observe, update)  \n",
    "‚úì Add monitoring (traffic allocation, posteriors, probabilities)  \n",
    "‚úì Run A/A test to validate implementation  \n",
    "‚úì Define stopping criteria (business threshold, time limit, or continuous)  \n",
    "‚úì Plan for adding/removing variants  \n",
    "‚úì Document for stakeholders  \n",
    "\n",
    "---\n",
    "\n",
    "### The Bottom Line\n",
    "\n",
    "**Thompson sampling transforms Bayesian posteriors into a self-optimizing traffic splitter.**\n",
    "\n",
    "No complex statistics. No sample size calculations. No stopping rules. No multiple comparison corrections.\n",
    "\n",
    "Just:\n",
    "1. Sample\n",
    "2. Choose\n",
    "3. Observe\n",
    "4. Update\n",
    "5. Repeat\n",
    "\n",
    "**Mathematics meets elegance. Theory meets practice. Bayesian meets optimal.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Real-World Implementation Considerations\n",
    "\n",
    "### Critical Assumptions to Revisit in Production\n",
    "\n",
    "The simulation above makes several simplifying assumptions to keep the explanation short and easy to explain. Of course in a real world implemention there are a few  things to refine:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The \"Immediate Feedback\" Assumption\n",
    "\n",
    "#### What the Simulation Assumes\n",
    "\n",
    "The code above assumes **instant feedback**:\n",
    "```python\n",
    "# Step 1: Choose variant\n",
    "chosen = max(samples, key=samples.get)\n",
    "\n",
    "# Step 2: Show variant to user\n",
    "show_variant_to_user(chosen)\n",
    "\n",
    "# Step 3: Observe outcome (IMMEDIATELY!)\n",
    "converted = observe_outcome()\n",
    "\n",
    "# Step 4: Update posterior (with fresh data)\n",
    "alpha[chosen] += converted\n",
    "beta[chosen] += (1 - converted)\n",
    "```\n",
    "\n",
    "This implies that we know whether the user converted **before the next user arrives**. In a real world implementation there is a timing decision to be made if/when a new user arrive while we are computing the results for the prevsiou user. This is imporant in high traffic secnarios and if assessing conversion takes a bit of time:\n",
    "\n",
    "If we have **100 users/minute** and **1-hour conversion delay**:\n",
    "- we'll serve **6,000 users** before the **first feedback** arrives\n",
    "- All 6,000 decisions made with **stale priors** (Beta(1, 1))\n",
    "- Essentially **random traffic allocation** for the first hour\n",
    "- **Massive regret** from uniformed early decisions\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "t=0:00  User 1 arrives ‚Üí Sample from Beta(1,1) for all variants ‚Üí Choose randomly\n",
    "t=0:01  User 2 arrives ‚Üí Sample from Beta(1,1) (no updates yet) ‚Üí Choose randomly\n",
    "...\n",
    "t=0:59  User 6000 arrives ‚Üí Sample from Beta(1,1) (still no feedback!)\n",
    "t=1:00  User 1's conversion finally observed ‚Üí First update to posteriors\n",
    "t=1:01  Users 6001+ now make slightly more informed decisions\n",
    "```\n",
    "\n",
    "For the first hour, we would have **no advantage over random allocation**. Note that this not a always the case for hte kinnd of AB test we did for passkeys the feedbak just took a few seconds so the issue did not really exist but if we want to build a robust/generic platform we can improve the model a bit:\n",
    "\n",
    "---\n",
    "\n",
    "#### One classic solution Batch Updates\n",
    "\n",
    "Real-world Thompson sampling implementations often use **batched updates**:\n",
    "\n",
    "---\n",
    "\n",
    "#### Recommendation for Production\n",
    "\n",
    "**For high-traffic systems (>1000 users/hour) and slow feedback loop for conversion**:\n",
    "1. Use **batched updates** (every 10-60 minutes depending on conversion delay)\n",
    "2. Start with **weakly informative priors** (reduces early regret)\n",
    "3. Track **pending observations** separately (don't update priors until resolved)\n",
    "4. Monitor **\"stuck\" observations** (conversions that never resolve ‚Üí data quality issue)\n",
    "\n",
    "**For low-traffic systems (<100 users/hour) and/or fast feedback looop**:\n",
    "- Sequential updates (i.e the naive soliution) is acceptable and can be the MVP, testing on micro-content (choice of copy) AB test for instance where the feeddback in immediate.\n",
    "\n",
    "\n",
    "\n",
    "### 2. Non-Stationarity (The Fixed Conversion Rate Assumption)\n",
    "\n",
    "#### What the Simulation Assumes\n",
    "\n",
    "The simulation uses **fixed true conversion rates**:\n",
    "\n",
    "```python\n",
    "true_rates = {\n",
    "    'A': 0.7016,  # Fixed forever\n",
    "    'B': 0.6824,  # Never changes\n",
    "    'C': 0.6903,  # Constant\n",
    "}\n",
    "\n",
    "# Users arrive, conversions observed, posteriors updated\n",
    "# But true_rates NEVER CHANGE\n",
    "```\n",
    "\n",
    "This assumes conversion rates are **stationary** (constant over time).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why This is May be Unrealistic\n",
    "\n",
    "**Real-world conversion rates drift**:\n",
    "\n",
    "**Day-of-week effects**:\n",
    "weeks vs. weekend users are different\n",
    "\n",
    "**Seasonality**:\n",
    "\n",
    "Black Friday / Holyday seasons users might be different\n",
    "\n",
    "**External events**:\n",
    "\n",
    "This is usually more of an issue for e-commnerce but large promotions like the ones Amazon does may change traffic patterns and user segments. Less of a probolem for financial services.\n",
    "\n",
    "**Product evolution**:\n",
    "\n",
    "New feature launch or large redesigh of a site may shift traffic patterns.\n",
    "\n",
    "---\n",
    "\n",
    "#### The Problem with Standard Bayesian Updates\n",
    "\n",
    "Standard Thompson sampling **remembers all history**:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\alpha_0 + \\sum_{\\text{all time}} \\text{conversions}_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta_i = \\beta_0 + \\sum_{\\text{all time}} \\text{non-conversions}_i\n",
    "$$\n",
    "\n",
    "After many observations:\n",
    "- $\\alpha_i$ and $\\beta_i$ grow to **hundreds or thousands**\n",
    "- Posterior becomes **very narrow** (low variance)\n",
    "- Algorithm becomes **highly confident** about old data\n",
    "\n",
    "**What happens when conversion rates drift?**\n",
    "\n",
    "**Example**: Variant A's true rate was 70%, now it's 65% (due to competitor)\n",
    "\n",
    "```\n",
    "Day 1-30:  A converts at 70% ‚Üí Posterior: Beta(2100, 900) ‚Üí mean ‚âà 70%\n",
    "Day 31:    Competitor launches, A now converts at 65%\n",
    "Day 31-35: New data comes in at 65%, but...\n",
    "           Posterior: Beta(2100 + 50, 900 + 27) ‚Üí mean ‚âà 69.7%\n",
    "           \n",
    "Still thinks A is ~70% because old data dominates!\n",
    "```\n",
    "\n",
    "The posterior is **sluggish** ‚Äî it takes a long time to react to the new reality because:\n",
    "- 2100 old conversions \"vote\" for 70%\n",
    "- 50 new conversions \"vote\" for 65%\n",
    "- Old data wins due to sheer volume\n",
    "\n",
    "---\n",
    "\n",
    "#### The Consequence: Stuck in the Past\n",
    "\n",
    "**Problem 1: Can't detect new winners**\n",
    "- Variant B's true rate improves from 68% to 72% (now better than A)\n",
    "- But algorithm is \"confident\" A is best (based on old data)\n",
    "- Takes thousands of users to shift belief\n",
    "\n",
    "**Problem 2: Wastes traffic on degraded variants**\n",
    "- Variant A's true rate drops from 70% to 60% (now worse than C)\n",
    "- Algorithm still allocates 60% of traffic to A (based on historical performance)\n",
    "- Massive regret while slowly updating belief\n",
    "\n",
    "**Problem 3: \"Frozen\" traffic allocation**\n",
    "- After 100,000 users, posteriors are extremely narrow\n",
    "- Traffic allocation becomes nearly deterministic\n",
    "- Little exploration ‚Üí can't adapt to changes\n",
    "\n",
    "---\n",
    "\n",
    "#### The Fix: Discounting Old Data\n",
    "\n",
    "Production Thompson sampling systems use **forgetting mechanisms** to stay agile:\n",
    "\n",
    "---\n",
    "\n",
    "**Approach 1: Exponential decay (discount factor)**\n",
    "\n",
    "```python\n",
    "class ThompsonSamplingWithDecay:\n",
    "    def __init__(self, decay_rate=0.99):\n",
    "        self.decay_rate = decay_rate  # 0.99 = remember 99% of history\n",
    "        self.alpha = {v: 1 for v in variants}\n",
    "        self.beta = {v: 1 for v in variants}\n",
    "    \n",
    "    def update(self, variant, converted):\n",
    "        \"\"\"Update with decay applied to old data.\"\"\"\n",
    "        # Decay old beliefs toward prior\n",
    "        self.alpha[variant] = 1 + (self.alpha[variant] - 1) * self.decay_rate\n",
    "        self.beta[variant] = 1 + (self.beta[variant] - 1) * self.decay_rate\n",
    "        \n",
    "        # Add new observation\n",
    "        self.alpha[variant] += converted\n",
    "        self.beta[variant] += (1 - converted)\n",
    "```\n",
    "\n",
    "**Effect**:\n",
    "- Recent data has **more weight** than old data\n",
    "- Posteriors stay **narrower than uniform**, but don't grow indefinitely\n",
    "- Algorithm remains **responsive** to drift\n",
    "\n",
    "**Trade-off**:\n",
    "- ‚úÖ Adapts to changing conversion rates\n",
    "- ‚ö†Ô∏è Forgets valuable historical information\n",
    "- ‚ö†Ô∏è Requires tuning decay_rate (0.99 = slow decay, 0.90 = fast decay)\n",
    "\n",
    "---\n",
    "\n",
    "**Approach 2: Sliding window**\n",
    "\n",
    "```python\n",
    "class ThompsonSamplingWithWindow:\n",
    "    def __init__(self, window_size=10000):\n",
    "        self.window_size = window_size\n",
    "        self.observations = {v: deque(maxlen=window_size) for v in variants}\n",
    "        self.alpha = {v: 1 for v in variants}\n",
    "        self.beta = {v: 1 for v in variants}\n",
    "    \n",
    "    def update(self, variant, converted):\n",
    "        \"\"\"Only count recent observations.\"\"\"\n",
    "        self.observations[variant].append(converted)\n",
    "        \n",
    "        # Recompute from sliding window\n",
    "        conversions = sum(self.observations[variant])\n",
    "        non_conversions = len(self.observations[variant]) - conversions\n",
    "        \n",
    "        self.alpha[variant] = 1 + conversions\n",
    "        self.beta[variant] = 1 + non_conversions\n",
    "```\n",
    "\n",
    "**Effect**:\n",
    "- Only the **last N observations** influence the posterior\n",
    "- Old data completely forgotten after N new users\n",
    "- Algorithm stays **agile**\n",
    "\n",
    "**Trade-off**:\n",
    "- ‚úÖ Clear semantics (only last N users matter)\n",
    "- ‚úÖ No tuning required (just choose window size)\n",
    "- ‚ö†Ô∏è Higher memory overhead (store recent observations)\n",
    "- ‚ö†Ô∏è Sudden \"cliff\" when old observation exits window\n",
    "\n",
    "---\n",
    "\n",
    "**Approach 3: Time-based windowing**\n",
    "\n",
    "```python\n",
    "class ThompsonSamplingWithTimeWindow:\n",
    "    def __init__(self, window_days=30):\n",
    "        self.window_days = window_days\n",
    "        self.observations = {v: [] for v in variants}\n",
    "        self.alpha = {v: 1 for v in variants}\n",
    "        self.beta = {v: 1 for v in variants}\n",
    "    \n",
    "    def update(self, variant, converted, timestamp):\n",
    "        \"\"\"Only count observations from last N days.\"\"\"\n",
    "        cutoff = timestamp - timedelta(days=self.window_days)\n",
    "        \n",
    "        # Store observation with timestamp\n",
    "        self.observations[variant].append((timestamp, converted))\n",
    "        \n",
    "        # Recompute from time window\n",
    "        recent = [(ts, conv) for ts, conv in self.observations[variant] \n",
    "                  if ts >= cutoff]\n",
    "        conversions = sum(conv for ts, conv in recent)\n",
    "        non_conversions = len(recent) - conversions\n",
    "        \n",
    "        self.alpha[variant] = 1 + conversions\n",
    "        self.beta[variant] = 1 + non_conversions\n",
    "```\n",
    "\n",
    "**Effect**:\n",
    "- Only observations from **last N days** count\n",
    "- Naturally handles varying traffic (weekday vs weekend)\n",
    "- Algorithm adapts to **seasonal patterns**\n",
    "\n",
    "**Trade-off**:\n",
    "- ‚úÖ Semantically clear (recent behavior matters)\n",
    "- ‚úÖ Handles non-uniform traffic patterns\n",
    "- ‚ö†Ô∏è Requires timestamp tracking\n",
    "- ‚ö†Ô∏è Window size needs domain knowledge (30 days? 7 days? 90 days?)\n",
    "\n",
    "---\n",
    "\n",
    "#### Recommendation for Production\n",
    "\n",
    "**For stationary environments** (conversion rates don't change):\n",
    "- Use **standard Thompson sampling** (accumulate all data)\n",
    "- Simpler, no tuning needed\n",
    "\n",
    "**For non-stationary environments** (rates drift over time):\n",
    "- Use **exponential decay** (good default: decay_rate=0.99)\n",
    "- Or use **sliding window** (e.g., last 10,000 users or 30 days)\n",
    "- Monitor posterior variance ‚Äî if it shrinks too much, add decay\n",
    "\n",
    "**For highly dynamic environments** (A/B testing on news sites, ad campaigns):\n",
    "- Use **aggressive discounting** (decay_rate=0.90) or **short windows** (7 days)\n",
    "- Prioritize agility over long-term memory\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}