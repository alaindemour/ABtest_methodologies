{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function\n",
    "from scipy.stats import beta as beta_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Methodologies: Null Hypothesis Significance Testing vs. Bayesian Approaches\n",
    "\n",
    "When evaluating new user experiences (UX) — such as launching **passkeys** and measuring their impact on abandonment rates — we need a way to decide whether a new design is better, worse, or equivalent to the current one.  \n",
    "This notebook compares two major approaches:\n",
    "\n",
    "- **Null Hypothesis Significance Testing (NHST)** — the long-standing statistical framework, widely used but often conceptually tricky and difficult to interpret in practical decision-making.\n",
    "- **Bayesian methods** — increasingly popular because they offer more flexibility and produce results that are often easier to interpret directly when deciding actions (e.g., when to shift more traffic from a control to a new variant).\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "We assume an existing digital identity creation flow with a **completion rate of ~20%** (meaning ~80% of users abandon).  \n",
    "Our test design:\n",
    "\n",
    "- Keep **90%** of traffic on the current experience as the **control group**.\n",
    "- Send the remaining traffic to one or more **variants** $A, B, C$.\n",
    "\n",
    "The test will be conducted in 2 steps, first we need to determine that each new experience is **no worse** than the current one, accepting small degradaition as unavoidable as we are adding more pages and clicks  — then after establishing we haven't degraded the experience, shifting more traffic to the variants and decidng which is the better-performing variant.\n",
    "\n",
    "The type of A/B test — where the first goal is to ensure a new design does **not degrade** the experience — is called a **non-inferiority test** (explained below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST)\n",
    "\n",
    "At a high level, the **NHST** workflow is:\n",
    "\n",
    "1. **Assume what you *don’t* want to see** — this is the **null hypothesis**.  \n",
    "   - Example in medicine: *“the drug has no effect.”*  \n",
    "   - Example here: *“the new experience increases abandonment.”*\n",
    "2. **Run the experiment** and compute a test statistic.\n",
    "3. **Ask:** *If the null were true, how likely is it that we would observe a result at least this extreme?*  \n",
    "   - If that probability (the **p-value**) is very low — e.g., below a conventional threshold such as 5% — we **reject the null**.\n",
    "\n",
    "Two immediate caveats:\n",
    "- Rejecting the null does **not** prove the opposite is true; it only says the data would be unlikely *if* the null were correct.\n",
    "- “Unlikely enough” is arbitrary — thresholds like 5% are conventions, not laws of nature.\n",
    "\n",
    "A key point: NHST computes **$P(\\text{data} \\mid \\text{hypothesis})$**.  \n",
    "Later we’ll see that the Bayesian approach instead computes **$P(\\text{hypothesis} \\mid \\text{data})$** — a fundamentally different quantity.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Conversion as Random Variables\n",
    "\n",
    "The abandonment or conversion of a UX flow can be modeled with **Bernoulli random variables**:\n",
    "\n",
    "- $X_C$ for the control experience\n",
    "- $X_A$ for a new variant $A$\n",
    "\n",
    "A Bernoulli variable takes only two values: success/failure, convert/abandon, etc.  \n",
    "Each user who sees a page gives one draw from one of these variables.\n",
    "\n",
    "We assume both have the same codomain:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}_C = \\mathcal{X}_A = \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where **1 = convert** (user finishes the intended action, e.g., creating a passkey) and **0 = abandon**.  \n",
    "Technical failures are treated as *success* here because the user attempted the action.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Proportions\n",
    "\n",
    "NHST usually works with **sample proportions**, the average of $n$ Bernoulli draws:\n",
    "\n",
    "$$\n",
    "\\hat{p}_C = \\frac{1}{n}\\sum_{i=1}^n X_{C_i},\n",
    "\\quad\n",
    "\\hat{p}_A = \\frac{1}{n}\\sum_{i=1}^n X_{A_i}.\n",
    "$$\n",
    "\n",
    "Each $\\hat{p}$:\n",
    "\n",
    "- Is a random variable taking values $\\{0,\\tfrac1n,\\tfrac2n,\\ldots,1\\}$.\n",
    "- Is also an **estimator** of the true expected value $p = E[X]$.  \n",
    "  By the law of large numbers, $\\hat{p} \\to p$ as $n$ grows.\n",
    "\n",
    "(Statisticians use a “hat” to denote an estimator.)\n",
    "\n",
    "Formally, an estimator maps $n$ realizations of $X$ ($\\mathcal{X}^n$) to a real number:\n",
    "\n",
    "$$\n",
    "\\hat{p}: \\mathcal{X}^n \\to [0,1].\n",
    "$$\n",
    "\n",
    "Because it is the mean of $n$ Bernoulli variables, $\\hat{p}$ follows a **binomial** distribution that becomes approximately **Gaussian** when $n$ is large.\n",
    "\n",
    "---\n",
    "\n",
    "### Variance and Standard Deviation of a Sample Proportion\n",
    "\n",
    "For a single Bernoulli $X$:  \n",
    "$$\n",
    "\\mathrm{Var}(X) = p(1-p).\n",
    "$$\n",
    "\n",
    "For the sample proportion:\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\tfrac1n \\sum_{i=1}^n X_i\\right)\n",
    "= \\tfrac1{n^2} n p(1-p)\n",
    "= \\tfrac{p(1-p)}{n}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The square root of this variance is the **standard error** — a quantity we’ll use later.\n",
    "\n",
    "---\n",
    "\n",
    "### Difference in Proportions\n",
    "\n",
    "We’ll often look at the **difference** between variant and control:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} = \\hat{p}_A - \\hat{p}_C\n",
    "$$\n",
    "\n",
    "This estimates the true difference\n",
    "\n",
    "$$\n",
    "\\Delta = p_A - p_C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis $H_0$** — the “bad” scenario we want to reject:  \n",
    "  the new UX **degrades** conversion by at least some small amount $\\epsilon$ we consider unacceptable (e.g., $3\\%$):\n",
    "\n",
    "  $$\n",
    "  H_0: E[\\Delta] \\le -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis $H_1$** — the new UX is **not worse** than control (possibly better):\n",
    "\n",
    "  $$\n",
    "  H_1: E[\\Delta] > -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Boundary Hypothesis** — used in test construction:  \n",
    "  assume the difference is exactly at the acceptable degradation limit:\n",
    "\n",
    "  $$\n",
    "  E[\\Delta] = -\\epsilon\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Example\n",
    "\n",
    "For a concrete example, we define the following counts and quantities for each experience:\n",
    "\n",
    "- $n_C$ — number of visitors in the **control** group  \n",
    "- $x_C$ — number of **conversions** observed in the control group\n",
    "\n",
    "- $n_A$ — number of visitors in the **variant** group  \n",
    "- $x_A$ — number of **conversions** observed in the variant group\n",
    "\n",
    "- $\\hat{\\Delta}_{\\mathrm{obs}}$ — the **observed difference** in conversion proportions between variant and control\n",
    "\n",
    "- $\\epsilon$ — the **acceptable degradation margin**, i.e., the smallest decrease in conversion we are willing to tolerate for the new variant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realization of difference in conversion rate estimator: 0.0200\n",
      "Control group realization of conversion rate estimator: 0.2000\n",
      "Treatment group realization of conversion rate estimator: 0.2200\n"
     ]
    }
   ],
   "source": [
    "control_group_conversion_rate = 0.2 # based on historical data\n",
    "nC = 7000\n",
    "xC_observed = nC * control_group_conversion_rate\n",
    "nA = 150\n",
    "xA_observed = 33\n",
    "epsilon = 0.03\n",
    "alpha = 0.05\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed  / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_observed:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_observed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the Estimator $\\hat{\\Delta}$ (a.k.a. Standard Error in Frequentist Statistics)\n",
    "\n",
    "In NHST, the first step is to **estimate the standard deviation** of the estimator $\\hat{\\Delta}$ (often called the **standard error**, SE).  \n",
    "We then compare the observed difference in proportions from the experiment to this estimated variability to decide whether the observed effect is “far enough” from what we would expect under the null hypothesis $H_0$.\n",
    "\n",
    "This is a key pain point for NHST:\n",
    "\n",
    "- We **do not know** the true standard deviation — it depends on the unknown underlying conversion probabilities.\n",
    "- Frequentist methods therefore use the **plug-in principle**: estimate the unknown variance by “plugging in” the sample estimates (the data you just observed).\n",
    "\n",
    "But note the circularity:\n",
    "\n",
    "1. We want to know if the data are unusual under $H_0$.\n",
    "2. To measure “unusual,” we need the standard error assuming $H_0$.\n",
    "3. SE depends on the unknown true rates, so we **plug in** $\\hat{p}$ (from the data!).\n",
    "4. We then use this data-derived SE to judge whether the data are unusual.\n",
    "\n",
    "It’s like saying: *“Use my one measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising.”*\n",
    "\n",
    "Frequentists accept this because:\n",
    "\n",
    "- **Long-run frequency view:** if we repeated the procedure many times, it would have correct average properties.\n",
    "- **Pragmatism:** it’s computable from one experiment.\n",
    "- **Simulation evidence:** works “reasonably well” for moderate $n$, though “reasonably well” is debatable (see the *replication crisis*).\n",
    "- **Framework limitation:** in classical stats, parameters are fixed unknowns, so no natural way to treat them as random.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Plug-In Approaches (a.k.a. “Standard Error Hacks”)\n",
    "\n",
    "#### 1. Wald **Pooled** Standard Error (for “No Effect” Null Hypothesis)\n",
    "\n",
    "If the null is **no effect** ($p_A = p_C$), we can pool data from control and variant, because under $H_0$ they are assumed to come from the same distribution.\n",
    "\n",
    "Realizations of sample proportions are:\n",
    "\n",
    "$$\n",
    "\\hat{p}_A = \\frac{x_A}{n_A}, \\qquad\n",
    "\\hat{p}_C = \\frac{x_C}{n_C}.\n",
    "$$\n",
    "\n",
    "If $H_0$ were true (no difference), a pooled estimator is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{pool}} = \\frac{x_A + x_C}{n_A + n_C}.\n",
    "$$\n",
    "\n",
    "Variance of the difference between two independent proportions is the sum of their variances:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\mathrm{Var}(\\hat{p}_A) + \\mathrm{Var}(\\hat{p}_C).\n",
    "$$\n",
    "\n",
    "Plugging in the pooled estimate:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right).\n",
    "$$\n",
    "\n",
    "So the **pooled standard error** is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{WaldPooled SE} =\n",
    "\\sqrt{\\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right)} }\n",
    "$$\n",
    "\n",
    "This is the classic **standard error of the difference between two independent proportions**.\n",
    "\n",
    "Once SE is computed, we calculate a **z-score** (number of SEs the observed difference is away from the null value 0):\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p}_A - \\hat{p}_C}{\\text{SE}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wald **Unpooled** Standard Error (for Non-Inferiority)\n",
    "\n",
    "If the null is **non-inferiority** (allowing a margin $\\epsilon$), we **cannot** assume $p_A = p_C$, so we don’t pool.\n",
    "\n",
    "Instead we sum the individual variances (using the plug-in trick separately for each group):\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{WaldUnpooled SE}} =\n",
    "\\sqrt{\\frac{\\hat{p}_A(1-\\hat{p}_A)}{n_A} +\n",
    "      \\frac{\\hat{p}_C(1-\\hat{p}_C)}{n_C}}.\n",
    "$$\n",
    "\n",
    "Ideally, the true $p_A$ and $p_C$ should be used here, but we don’t know them — so we substitute $\\hat{p}_A$ and $\\hat{p}_C$.  \n",
    "This works but can be inaccurate if sample sizes are small or rates are extreme.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Newcombe / Score-Based (Wilson)\n",
    "\n",
    "- **Better coverage** than Wald, especially with imbalanced sample sizes or very high/low $p$.\n",
    "- Still closed-form and relatively easy to compute.\n",
    "- Still uses plug-in estimates and suffers from the same “single-sample” limitation.\n",
    "\n",
    "*(Not detailed here — but recommended over Wald when possible.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Miettinen–Nurminen\n",
    "\n",
    "- Widely used in **clinical trials** and regulated industries (e.g., FDA guidance).\n",
    "- Provides improved accuracy for non-inferiority tests.\n",
    "- However: mathematically complex, still plug-in based, and still fundamentally relies on one sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Frequentist methods **must estimate variance from the data itself** — leading to circularity and potential miscalibration, especially for small samples or edge cases.  \n",
    "Later we’ll see how Bayesian methods avoid this by modeling uncertainty about the true conversion rates directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Positive, *p*-value, Significance Level $\\alpha$ (sometimes called “confidence”), and Critical Value\n",
    "\n",
    "Once we have an estimate of the standard error $SE$ (the standard deviation of $\\hat{\\Delta}$), NHST assumes a **sampling distribution** for the estimator under the null hypothesis $H_0$.\n",
    "\n",
    "The idea is:\n",
    "\n",
    "- If we know (or assume) the **mean** and **standard deviation** of $\\hat{\\Delta}$ under $H_0$,  \n",
    "- we can model it with a known probability distribution and calculate how likely any observed result is.\n",
    "\n",
    "Although the true process is discrete (binomial), in practice we often approximate it by a **normal (Gaussian)** distribution. This is mathematically simpler and is a good approximation for moderate or large sample sizes.\n",
    "\n",
    "---\n",
    "\n",
    "##### Using the “Boundary” Case\n",
    "\n",
    "The null hypothesis is technically an inequality:\n",
    "\n",
    "$$\n",
    "H_0: E[\\Delta] \\le -\\epsilon.\n",
    "$$\n",
    "\n",
    "But to get a single distribution to work with, we typically use the **boundary value**:\n",
    "\n",
    "$$\n",
    "E[\\Delta] = -\\epsilon.\n",
    "$$\n",
    "\n",
    "Why?  \n",
    "- The boundary is the **least favorable** case for rejecting $H_0$.  \n",
    "- If we can reject $H_0$ at $E[\\Delta] = -\\epsilon$, then we would also reject any stronger null (mean further left).  \n",
    "- This makes the test conservative.\n",
    "\n",
    "So under $H_0$, we model $\\hat{\\Delta}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = -\\epsilon, \\qquad \\sigma = SE.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Computing the *p*-Value\n",
    "\n",
    "The **p-value** is the probability (under $H_0$) of observing a result **as extreme or more extreme** than what we got, in the direction of the alternative $H_1$.\n",
    "\n",
    "In this one-sided non-inferiority test, that means the right tail probability:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P_{H_0}\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\text{obs}}\\big]\n",
    "= \\int_{\\hat{\\Delta}_{\\text{obs}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n",
    "\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,dx.\n",
    "$$\n",
    "\n",
    "This tail integral is the **survival function** of the normal distribution.\n",
    "\n",
    "Using the standard normal CDF $\\Phi$:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = 1 - \\Phi\\!\\left(\\frac{\\hat{\\Delta}_{\\text{obs}}-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Significance Level $\\alpha$ and Critical Value\n",
    "\n",
    "- We choose a **significance level** $\\alpha$ (often $0.05$).  \n",
    "- If $p\\text{-value} \\le \\alpha$, we reject $H_0$ — our result is unlikely under the null.\n",
    "\n",
    "The **critical value** $c$ is the smallest observed difference that would lead to rejection at level $\\alpha$. It is obtained by inverting the right-tail probability:\n",
    "\n",
    "$$\n",
    "c = \\mu + \\sigma \\,\\Phi^{-1}(1 - \\alpha).\n",
    "$$\n",
    "\n",
    "Any observed $\\hat{\\Delta}_{\\text{obs}} \\ge c$ yields $p\\text{-value} \\le \\alpha$ and thus rejects $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "Below we’ll compute the p-value and critical value explicitly and visualize how the right tail behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided): {p_value:.4f}')\n",
    "\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for p-value={alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values; recompute to be robust\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H0 - 6 * sigma_HO\n",
    "right = mu_H0 + 6 * sigma_HO\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Right-tail probabilityy\n",
    "p = norm.sf(x0, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Critical value at significance alpha (one-sided)\n",
    "crit_x = norm.ppf(1 - alpha, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade right tail\n",
    "mask = xs >= x0\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Right tail p = {p:.4g}\")\n",
    "\n",
    "# Vertical line at observed x\n",
    "ax.axvline(x0, color=\"C1\", ls=\"--\", lw=1.5, label=f\"observed delta = {x0:.4f}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(crit_x, color=\"C2\", ls=\"-.\", lw=1.5, label=f\"critical value c = {crit_x:.4f}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H0\n",
    "ax.axvline(-epsilon, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H0 = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H0:.4f}, σ={sigma_HO:.4f}) — Right-tail beyond x\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under the null H0\")\n",
    "ax.set_ylabel(\"Probability density under H0\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.show(), display handled by notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### traditional presentation of the same conmputation using z-scores\n",
    "\n",
    "The traditional ways for NHST to compute the p value is not to use the measurment $\\hat{\\Delta_{obs}}$ directly but to normalize it to a standard normal and centered on zero and on standard deviation 1 N(0,1). This mormalization is done by transforming the measurment into a so called z-score:\n",
    "\n",
    "The the z-score is just the way you can rescale an arbitrary Gaussian to the Normal by substracting the meand and dividing by the standard deviation $Z = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "$Z_{NI} = \\frac{\\hat{\\Delta} - (E(\\Delta_{H_{boundary}}))}{SE}$ \n",
    "\n",
    "$= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}$ \n",
    "\n",
    "$\\boxed{Z_{NI}= \\frac{\\hat{\\Delta} + \\epsilon}{SE}}$\n",
    "\n",
    "Then  $\\frac{\\hat{\\Delta} + \\epsilon}{SE}$ is a normal N(0,1) and \n",
    "\n",
    "$p(\\hat{\\Delta} \\ge \\hat{\\Delta(w)})$\n",
    "\n",
    "$ = p(\\frac{\\hat{\\Delta} + \\epsilon}{SE} \\ge \\frac{\\hat{\\Delta(w) +\\epsilon}}{SE })$\n",
    "\n",
    "Then we can compute the survial function but this timne on a standard normal and from the z-score value\n",
    "\n",
    "$\\int_{ Z_{NI} }^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}z^2} dz$\n",
    "\n",
    "which give the exact same results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positive\n",
    "In this NHST the pvalue is also the probability of a false positive that is the probabilityy of rejecting the null hopythisis (saying htere is no degradation) while there is acxtually a degradation. So it is setup our risk of make the wrong decision is 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Probability of having false negative a.k.a type 2 error , Power and sample size required to get to signficance under those conditions\n",
    "\n",
    "Conversely , \"False negative\" in the context of a non inferiority test is the probabilityy of failing to reject given the fact that H1 is true, meaning we fail to clear the non inferiority test, while the new UX is actually non inferior (i.e. as good or better than the old one). So we have the same issue as for the false poistive computation, we have to pick a value for what expected value for the $\\Delta$ but this time under H1. As we cannot really work with a range of expecgted value for $\\Delta$  we need a single value to fix the gaussian we are going to integrate. Here possible choice is to also pick a boundary which is $E(\\Delta) = E(p_C)$ so whatever the control group mean was or is (depending on how much historical data we have). This is usally called \"minimum effect size we want to detect\" as is really a business decision, it is a bit arbitrary. Pickig our baseline is just one option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the estimator under the alternative hypothesis H1, we need to pick another expected value for the $\\Delta$, since this is for H1 which means the same or better, we pick the bare minimum we need to make that statement that is \"the same\" and an expected value of zero for the Delta. Under these condition we can pool the samples to get an estimator of the variance and standard error as we are assuming they are distributed indentically\n",
    "\n",
    "$SE | H1 = WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we compute the probability of the estimator coming up \"higher\" but with the same critical value as it was established ahead of time with the significance level alpha.\n",
    "\n",
    "The Beta is the probability that we observed something lower and up to the critical value defined by inverting the p-value but under H1, meaning the distribution is centered on the H1 expected value, in our case it is zero.\n",
    "\n",
    "This computation will give us whatever it gives us. If we want to TARGET a Power of 0.8 (which means β = 0.2, or 20% Type II error rate), we can compute the sample sizes (implied in SE) that will make us reach that level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"probabilityy of false negative a.k.a β a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Power (1 - β): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H1 - 6 * sigma_H1\n",
    "right = mu_H1 + 6 * sigma_H1\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H1, scale=sigma_H1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density under H₁\")\n",
    "\n",
    "# Shade left tail (Type II error region)\n",
    "mask = xs <= critical_value\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, \n",
    "                label=f\"Type II error β = {beta:.4g}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(critical_value, color=\"C2\", ls=\"-.\", lw=1.5, \n",
    "           label=f\"critical value c = {critical_value:.4f}\")\n",
    "\n",
    "# Vertical line at observed delta\n",
    "ax.axvline(hatDelta_observed, color=\"C1\", ls=\"--\", lw=1.5, \n",
    "           label=f\"observed delta = {hatDelta_observed:.4f}\")\n",
    "\n",
    "# Vertical line at the mean under H1\n",
    "ax.axvline(mu_H1, color=\"k\", ls=\":\", lw=1.5, \n",
    "           label=f\"mean under H₁ = {mu_H1:.4f}\")\n",
    "\n",
    "# Vertical line at the H0 boundary (for reference)\n",
    "ax.axvline(-epsilon, color=\"gray\", ls=\":\", lw=1.5, alpha=0.7,\n",
    "           label=f\"H₀ boundary = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H1:.4f}, σ={sigma_H1:.4f}) — Type II Error (β) and Power\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under H₁\")\n",
    "ax.set_ylabel(\"Probability density under H₁\")\n",
    "ax.legend(loc=\"best\", fontsize=9)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "\n",
    "# Add text annotation for power\n",
    "power_text = f\"Power = 1 - β = {power:.4f}\"\n",
    "ax.text(0.98, 0.95, power_text, transform=ax.transAxes, \n",
    "        fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST the Bayesian approach is conceptually simpler, instead of artificially considering only 2 possible expected value boundaries for $\\Delta$ one for H0 and one for H1, we assume we just don't know that $E(\\Delta)$ can take any value between 0 and 1 , and this is just itself random variable but one that we can quantify accross all possible world\n",
    "\n",
    "#### Using Beta distribution as a tool to model our 'pior belief\"\n",
    "\n",
    "For kind of Bernouilli trial we are dealin wiht (i.e. success/failure, convert/abandom etc...) the most convienent model for our prior belive is what is know as a Beta distribution, note that the anme \"Beta\" is unfortunate it as it sonds like the Beta in NHST, but it  has nothint ot with it, is is a probability distribution depending on 2 parameters which depening on how they are set can model \"we know nothing a piorir\" a.k.a an uniformative prior, or  \"we don't know much but just that the coversion rate should be aropund 20% but we may be surprised\" to  very strong prior based on tons of historical data \"the conversion shold be ver close to 20%\"\n",
    "\n",
    "Note tha the Beta distribution is deined on the interval [0,1] as it is modeling a probability, so that'teh probability of a probability (meta probability)\n",
    "\n",
    "Here are  few example of priors that coud be used, in cour case we will try with a totally unfornative priot (we know nothing) and weakly one (shoub be around 17% but we are not sure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Four cases:\n",
    "cases = [\n",
    "    (\"Uninformative (flat)\", (\"single\", (1, 1))),\n",
    "    (\"Weakly informative (centered, high entropy)\", (\"single\", (3, 12))),   # mean=0.2\n",
    "    (\"Strong conviction (centered, low entropy)\", (\"single\", (200, 800))),  # mean=0.2\n",
    "    (\"Bi-modal (mixture of Betas)\", (\"mixture\", ((5, 20, 0.5), (20, 5, 0.5))))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (title, (kind, params)) in zip(axes.ravel(), cases):\n",
    "    if kind == \"single\":\n",
    "        a, b = params\n",
    "        y = beta_dist.pdf(x, a, b)\n",
    "        mean = a / (a + b)\n",
    "        ci_low, ci_high = beta_dist.ppf([0.025, 0.975], a, b)\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2)\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        ax.axvline(mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mean={mean:.3f}\")\n",
    "        ax.axvline(ci_low, color=\"C1\", ls=\"--\", lw=1, label=\"95% CI\")\n",
    "        ax.axvline(ci_high, color=\"C1\", ls=\"--\", lw=1)\n",
    "        ax.set_title(f\"{title}\\nBeta(α={a}, β={b})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "    else:\n",
    "        # Mixture of two Betas: (a1, b1, w1), (a2, b2, w2)\n",
    "        (a1, b1, w1), (a2, b2, w2) = params\n",
    "        y = w1 * beta_dist.pdf(x, a1, b1) + w2 * beta_dist.pdf(x, a2, b2)\n",
    "        m1, m2 = a1 / (a1 + b1), a2 / (a2 + b2)\n",
    "        mix_mean = w1 * m1 + w2 * m2\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2, label=\"mixture pdf\")\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        # Component means\n",
    "        ax.axvline(m1, color=\"C2\", ls=\"--\", lw=1, label=f\"mean₁={m1:.2f}\")\n",
    "        ax.axvline(m2, color=\"C3\", ls=\"--\", lw=1, label=f\"mean₂={m2:.2f}\")\n",
    "        # Mixture mean\n",
    "        ax.axvline(mix_mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mixture mean={mix_mean:.2f}\")\n",
    "        ax.set_title(f\"{title}\\n{w1:.1f}·Beta({a1},{b1}) + {w2:.1f}·Beta({a2},{b2})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"p\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"density\")\n",
    "\n",
    "fig.suptitle(\"Four Beta Priors: flat, weakly centered, strongly centered, bi-modal\", fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative approach assuming we know nothing (a.k.a non informative prior)\n",
    "\n",
    "For the Variant A, assuming we start with an non informative prior meaning that we have no idea, p_A could range from 0 to 1 with equal probabilityy model by the beta function $Beta(1,1)$  \n",
    " \n",
    "after n trials and k success the posterior probabilityy distribution due to the property of the Beta function is\n",
    " \n",
    "$Beta(xA+1, n_A - xA+1)$ \n",
    "\n",
    "This derived through Bayes Theorem and how the Beta function can be integrated\n",
    "\n",
    "The expected value for $E(Beta(\\alpha,\\beta)) = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "which for a bayesian posterior becomes\n",
    "\n",
    "$E(Beta(xA+1,nA−xA+1)) = \\frac{xA+1}{xA+1 + nA−xA+1} = \\frac{xA+1}{nA+2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the credible intervals and visualize the prior and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "# Ensure we use the beta distribution from scipy.stats\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 99% credible interval (1th and 99th percentiles)\n",
    "p_L_99 = beta_dist.ppf(0.005, alpha, beta_param)\n",
    "p_U_99 = beta_dist.ppf(0.995, alpha, beta_param)\n",
    "# Output the result\n",
    "print(f\"99% Credible Interval for p: [{p_L_99:.4f}, {p_U_99:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-informative prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_noninformative_pdf = beta_dist.pdf(x_range, 1, 1)  # Beta(1,1) - uniform\n",
    "posterior_noninformative_pdf = beta_dist.pdf(x_range, alpha, beta_param)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_noninformative_pdf, 'b--', lw=2, label='Prior: Beta(1, 1) - Non-informative')\n",
    "ax.plot(x_range, posterior_noninformative_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha:.1f}, {beta_param:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L) & (x_range <= p_U)\n",
    "ax.fill_between(x_range[mask], posterior_noninformative_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Non-informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probability that variant is non-inferior using non-informative prior\n",
    "prob_non_inferior_noninformative = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                                       alpha, beta_param)\n",
    "print(f\"Probability that variant is non-inferior (non-informative prior): {prob_non_inferior_noninformative:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior_noninformative*100:.2f}% probability that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Informative prior approach using historical data\n",
    "\n",
    "Instead of using a non-informative prior Beta(1,1), we can incorporate our historical knowledge. We know the control group has a conversion rate of 0.2, and we want to test for non-inferiority with margin epsilon = 0.03. \n",
    "\n",
    "For the variant, we'll use a prior centered at 0.2 - 0.03 = 0.17 (the boundary of non-inferiority), but with high entropy to reflect our uncertainty. A Beta distribution with mean μ and relatively small α and β values will have high entropy while still being informative about the center.\n",
    "\n",
    "For a Beta(α, β) distribution:\n",
    "- Mean: $\\mu = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "- To get high entropy (concave distribution), we want small α and β values (both < 1 makes it U-shaped, but values around 2-5 give high entropy while remaining unimodal)\n",
    "\n",
    "Let's use α = 3, and solve for β to get mean = 0.17:\n",
    "- $0.17 = \\frac{3}{3 + \\beta}$\n",
    "- $\\beta = \\frac{3}{0.17} - 3 \\approx 14.65$\n",
    "\n",
    "After observing the data, the posterior will be Beta(xA + α, nA - xA + β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative prior parameters\n",
    "target_prior_mean = control_group_conversion_rate - epsilon  # 0.17\n",
    "alpha_prior = 3  # Small value for high entropy\n",
    "beta_prior = (alpha_prior / target_prior_mean) - alpha_prior  # Solve for beta given mean\n",
    "\n",
    "print(f\"Prior: Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "print(f\"Prior mean: {alpha_prior / (alpha_prior + beta_prior):.4f}\")\n",
    "print(f\"Prior variance: {(alpha_prior * beta_prior) / ((alpha_prior + beta_prior)**2 * (alpha_prior + beta_prior + 1)):.6f}\")\n",
    "\n",
    "# Posterior parameters after observing data\n",
    "alpha_posterior = xA_observed + alpha_prior\n",
    "beta_posterior = (nA - xA_observed) + beta_prior\n",
    "\n",
    "print(f\"\\nPosterior: Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Compute 95% credible interval\n",
    "p_L_informative = beta_dist.ppf(0.025, alpha_posterior, beta_posterior)\n",
    "p_U_informative = beta_dist.ppf(0.975, alpha_posterior, beta_posterior)\n",
    "print(f\"95% Credible Interval: [{p_L_informative:.4f}, {p_U_informative:.4f}]\")\n",
    "\n",
    "# Compute 99% credible interval\n",
    "p_L_99_informative = beta_dist.ppf(0.005, alpha_posterior, beta_posterior)\n",
    "p_U_99_informative = beta_dist.ppf(0.995, alpha_posterior, beta_posterior)\n",
    "print(f\"99% Credible Interval: [{p_L_99_informative:.4f}, {p_U_99_informative:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_pdf = beta_dist.pdf(x_range, alpha_prior, beta_prior)\n",
    "posterior_pdf = beta_dist.pdf(x_range, alpha_posterior, beta_posterior)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_pdf, 'b--', lw=2, label=f'Prior: Beta({alpha_prior:.1f}, {beta_prior:.1f})')\n",
    "ax.plot(x_range, posterior_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha_posterior:.1f}, {beta_posterior:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L_informative) & (x_range <= p_U_informative)\n",
    "ax.fill_between(x_range[mask], posterior_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilityy that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "prob_non_inferior = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                       alpha_posterior, beta_posterior)\n",
    "print(f\"Probability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probabilityy that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
