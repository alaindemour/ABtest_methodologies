{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function\n",
    "from scipy.stats import beta as beta_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Methodologies: Null Hypothesis Significance Testing vs. Bayesian Approaches\n",
    "\n",
    "When evaluating new user experiences (UX) — such as launching **passkeys** and measuring their impact on abandonment rates — we need a way to decide whether a new design is better, worse, or equivalent to the current one.  \n",
    "This notebook compares two major approaches:\n",
    "\n",
    "- **Null Hypothesis Significance Testing (NHST)** — the long-standing statistical framework, widely used but often conceptually tricky and difficult to interpret in practical decision-making.\n",
    "- **Bayesian methods** — increasingly popular because they offer more flexibility and produce results that are often easier to interpret directly when deciding actions (e.g., when to shift more traffic from a control to a new variant).\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "We assume an existing digital identity creation flow with a **completion rate of ~20%** (meaning ~80% of users abandon).  \n",
    "Our test design:\n",
    "\n",
    "- Keep **90%** of traffic on the current experience as the **control group**.\n",
    "- Send the remaining traffic to one or more **variants** $A, B, C$.\n",
    "\n",
    "The test will be conducted in 2 steps, first we need to determine that each new experience is **no worse** than the current one, accepting small degradaition as unavoidable as we are adding more pages and clicks  — then after establishing we haven't degraded the experience, shifting more traffic to the variants and decidng which is the better-performing variant.\n",
    "\n",
    "The type of A/B test — where the first goal is to ensure a new design does **not degrade** the experience — is called a **non-inferiority test** (explained below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST) Methodology\n",
    "\n",
    "At a high level the methodology is : assume what you don't want to see or have, call it the null hypothesis, for instance, the drug has no effect or in our case the new experience significantly increase abandonment, run the test, look at the result if and if the result looks  unlikely enough, as if low enough probabilityy to happen (e.g. like that 5% to see what we observe) under the Null Hypothesis, then reject the hypothesis. Note 2 thing immediately, you havent really said whether the opposite hypothesis is true, just that your reject the null. Also \"unlikely enough\" is defined by a somewhat arbitrary number (5% chance).\n",
    "\n",
    "The method boils down to compute the probabilityy of the observation given the hypothesis, as we will see later the Bayesian approach is going the other way, it computes the probabilty of the hypothesis given the observation, which is a totally different metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framing of the problem using Random Variables\n",
    "\n",
    "This abandonment rate of a  UX is usually modelled as 2 Bernoulli Random Variables $X_C$ and $X_A$ from which we will draw repeatedly. Bernoulli just mean we present a choice with onlyy 2 possible outcome success/failure, convert/abandon, up/down etc.. presenting the new CX repeatedly to a bunch of end users.\n",
    "\n",
    "$X_C$ being the UX and Bernoulli RV presented to control control group  $X_A$ being the variant A presented. They are both assumed to have for codomain $\\mathcal{X}_C=\\mathcal{X}_A=\\{0,1\\}$ that is a boolean i.e. it converts or does not convert. Here \"convert\" means the user perform the action they intended to do vs. abandoning the journey, e.g. create a passkey. There a possibility with a techincal failure but this would not be their choice so it would be counted as a success for the purpose of this AB test.\n",
    "\n",
    "The way NHST usually model the difference between a control and variant convertion rate is to define a new random variables called the \"sample proportions\" which is the sum of n instances of the Bernoulli ones defined above, divided by n. For this \"sample proportion\" Let's using the notation  $\\hat{p}_C$ and $\\hat{p}_A$ for the control and the variant respectively , defined as\n",
    "\n",
    " $ \\hat{p_C}=\\frac{1}{n}\\sum_{i=1}^n X_{Ci}$ and \n",
    " $ \\hat{p_A}=\\frac{1}{n}\\sum_{i=1}^n X_{Ai}$\n",
    " \n",
    " each sample proportion can be interprested as 2 things :\n",
    "\n",
    "* $ \\hat{p_C}$ and  $ \\hat{p_A}$ can be interpreted as an ordinary random variables with codomain $\\{0,\\frac{1}{n},\\frac{2}{n},...\\frac{n}{n}\\}$\n",
    "* $\\hat{p_C}$ and  $ \\hat{p_A}$ can also be interpreted as estimators (in the technical sense of the word \"estimator\" used in classical statistics) of the expected value $E(p_C)$ and $E(p_A)$. In general regardless of whether it is a control or a variant $E(\\hat{p}) \\rightarrow E(p)$ because of the law of large numbers applied to a Bernoulli/Binomial.\n",
    "\n",
    "The notation with a \"little hat\" over the letter is the convention in statistics to denote estimators.\n",
    "\n",
    "An estimator is a function from the n-cartesian product of the space of realizations of X (on itself n times)  to the domain of the parameter of interest, if we  use the notation codomain(X) = $\\mathcal{X}$, it is then a mapping from $\\mathcal{X}^n$ to some real value for a parameter of original random variable X, in this case $\\hat{p}_A: \\mathcal{X}^n \\rightarrow \\{0,\\frac{1}{n},\\frac{2}{n},...,\\frac{n}{n}\\}$ Because it it is a sum of Bernoullis, the estimator RV follows binomial distribution centered on the expected value. It is always true that a discrete binomial distribution can be approximated by a continuous gaussian as n grows (clipping the tails)\n",
    "\n",
    "\n",
    "#### Variance and standard devation of a sum of Bernoulli or Binomial\n",
    "\n",
    "in all the rest of the discussion we are going to use a lot of the variance and standard deviation of the sample proportion as it it used to defined what is called the \"standard error\" under various hypothesis. This is just a reminder of how they are computed\n",
    "\n",
    "Note that for a \"single\" Bernoulli random variable $X$ we have $Var(X) = p (1-p)$ and therefore \n",
    "  \n",
    "$Var(\\frac{1}{n}\\sum_{i=1}^n X_i) = \\frac{1}{n^2}Var(\\sum_{i=1}^n X_i)$\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_{i=1}^n X_i) = \\frac{1}{n^2}n(p(1-p))$\n",
    "\n",
    "\n",
    "$Var(\\frac{1}{n}\\sum_{i=1}^n X_i) = \\frac{1}{n}(p(1-p))$\n",
    "\n",
    "so $\\boxed{Var(\\hat{p})= \\frac{p(1-p)}{n}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Notation for the  Estimator of the difference in proportions\n",
    "\n",
    "To make the rest of the discussion easier to read  we are going to define the difference in estimator $\\hat{p_A}$ and $\\hat{p_C}$ as $\\hat\\Delta = \\hat{p_A} - \\hat{p_C}$ which is also an estimator of the expected value of random varaible $\\Delta = p_A - p_C$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Null Hypothesis (H₀):\n",
    "\n",
    "\n",
    "As we said earlier the  \"effect\" we are trying to avoid is if the new passkey creation pages would degrade the overall UX  so much that it may impacts our other business metrics. In the classical framing of a non-inferiority test, the  Null Hypothesis is the \"bad thing\" we are trying to reject in our case it would mean : \"some degradation\".\n",
    "\n",
    "\"some degradation\" can be described with a simple inequality like  $E(p_A) \\le E(p_C) - \\epsilon $ where $\\epsilon$ is a tiny degradation we would find acceptable for instance 0.03 (3%).\n",
    "\n",
    "We will rewrite\n",
    "\n",
    "$H_0$ as $\\boxed{H_0 = E(\\Delta) \\leq -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Alternative Hypothesis (H₁):\n",
    " The conversion rate for the variant is higher than $\\boxed{E(\\Delta) > -\\epsilon}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary Hypothesis\n",
    "\n",
    "Below we will make use of the hypothesis that the difference in conversion is exactly the small acceptable one  that is\n",
    "\n",
    "$\\boxed{E(\\Delta) = -\\epsilon}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Numerical example \n",
    "\n",
    "Each realization is given after each experience by the folowing counts\n",
    "\n",
    "$n_C$: Number of visitors in the control group\n",
    "\n",
    "$x_C$ : Number of conversions observed (realization) in a given control group\n",
    "\n",
    "$n_A$: Number of visitors in the variant group\n",
    "\n",
    "$x_A$ : Number of conversions observed (realization) in the variant group\n",
    "\n",
    "$\\hat{\\Delta_{obs}}$ the observed value of the difference between the proportions\n",
    "\n",
    "$\\epsilon$ : acceptable degradation in the difference between the proportions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_conversion_rate = 0.2 # based on historical data\n",
    "nC = 7000\n",
    "xC_observed = nC * control_group_conversion_rate\n",
    "nA = 150\n",
    "xA_observed = 33\n",
    "epsilon = 0.03\n",
    "alpha = 0.05\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed  / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_observed:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_observed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the estimator $\\hat{\\Delta}$ a.k.a Standard Error in frequentist/classic statistics.\n",
    "\n",
    "The NHST methodolgy involve computing first an estimatation of the standard deviation for the estimator $\\hat{\\Delta}$, then to see how likely it then comparing it with the actual values we get from the difference in proportion in the experiment, then based on this decide if the difference observed is far enough (or close enough) to the theoritical value to reject or not reject the hypothesis based on some accpeted risks in the deicison rules (power, confidence internval)\n",
    "\n",
    "THis is the first problem NHST has to tackle we have no idea what the true standar deviation is, so all varation on NHST use a \"hack\" they call the plug in principle, which basically use the current experiement results , to \"plug in\" some formula to esimate the unknown standard deviation. But note the circularity of the method:\n",
    "\n",
    "1. We want to test if the data is unusual under H₀\n",
    "2. To measure \"unusual,\" we need the standard error under H₀\n",
    "3. But SE depends on the unknown θ, so we plug in p̂ (the data itself!)\n",
    "4. We then use this data-derived SE to judge whether the data is unusual\n",
    "\n",
    "It's like: *Let me use my single measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising*\n",
    "\n",
    "NHST proponent justify it because if ou repeateded the same experiment many times, at infinity would converge to the true value \"on average\".But you onlyy have ONE sample!\n",
    "You don't know if your particular p̂ is:\n",
    "Close to the true value (plug-in works well)\n",
    "An outlier (plug-in is terrible)\n",
    "\n",
    "Why Frequentists live with it:\n",
    "\n",
    "* Long-run frequency interpretation: \"If we repeat this procedure many times, it has correct coverage\"\n",
    "* Pragmatism: They need something computable\n",
    "* Simulation evidence: It works \"reasonably well\" in practice for moderate n, but \"reasonably well\" can be challeng see goole \"the crisis of replication\" to get a sense of the numerous failures.\n",
    "* No alternative: In the frequentist framework, parameters are fixed unknowns, not random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The various plugin Hacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### If hypotehsis is \"no effect\" the Wald pooling method:\n",
    "\n",
    "If instead of \"non inferiority\" the null hypothesis  is \"no effect\" ( we will have that later for H1), the plug in hack is to record the observed conversion rates (estimators) for each proportion then  combine (pool) as under this very specific hypothesis they have the exact same expected value so a larger sample created through pooling is a better estimator of the true value, whatever it is.\n",
    "\n",
    "the realization of $\\hat{p}_A$  is $(\\hat{p}_A(\\omega) = \\frac{x_A}{n_A})$\n",
    "\n",
    "the associated realization of $\\hat{p}_C$ is  $(\\hat{p}_C(\\omega) = \\frac{x_C}{n_C})$\n",
    "\n",
    "Then there is a theoritical estimator for the combine exeprience IF the null hypothesis was holding and there was not difference A and the control there would be a random variable for the estimator of both being pooled\n",
    "\n",
    "$\\hat{p}(\\omega_{pool}) = \\frac{x_A + x_V}{n_A + n_C}$\n",
    "\n",
    "Then we try to compute the variance standard deviation of the difference between the conversion rate of the control vs. the variance. The variance of the difference beteww the 2 proportion is the sum of their variance, so\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = Var(\\hat{p}_C) + Var(\\hat{p}_A)$\n",
    "\n",
    "Also because both $\\hat{p_C}$ and $\\hat{p_A}$ are Bernoulli RV, and using this \"plug in \" principal; we compute the sum of variance of the estimators also using the pooled sample reading as:\n",
    "\n",
    "$Var(\\hat{p}_A - \\hat{p}_C ) = \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_A} + \\frac{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool}))}{n_C} = \\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})$\n",
    "\n",
    "and the standard error which is just the standard deviation of the same quantity  also using the plug in principl the standard devation of the estimators that statisticain call \"standard error\" is\n",
    "\n",
    "$\\boxed{WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}}$\n",
    "\n",
    "this is sometimnes called \"pooled standard error\" or \"standard error of the difference between two independent proportions\"\n",
    "\n",
    "From this we conmpute the z-score whuich is a dimenionlyess value counting the number of standard deviation that the difference observed is from the theoritical difference which shoiuld be zero under the null hypothesis (approximated by the pooled one, sktechy). So a simple ivisino of the observed devation over teh standard deviation of hte diference\n",
    "\n",
    "$z= \\frac{\\frac{x_A}{n_A} - \\frac{x_C}{n_C}}{SE}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If hyopthesis is \"non inferiority\", use the Wald Unpooled Standard Error:\n",
    "\n",
    "If the hypotheis is not \"no effect\", but \"on inferioity\" we cannot assume that each sample is drawn from the same random variable, we actually explicitely say it is not because of the epsilon among other thing. So the first quick and dirt approch is just to sum the separate variances (see above variance of Bernoulli) as variances are additive when random variable are added  or substracted in our case, and are assymed to be independent, then take the square roo t pf that sum to get to a standard deviation:\n",
    "\n",
    "$\\hat{WaldUnpooledSE} = \\sqrt{\\frac{\\hat{p_A(w)}(1-\\hat{p_A(w)})}{n_A} + \\frac{\\hat{p_C(w)}(1-\\hat{p_C(w)})}{n_C}}$\n",
    "\n",
    "Note that we should be using hte expected value of $E(p_A)$ and $E(p_B)$ in the formula but we don't know them, so we use the so call \"plug in\" trick and replace them but whatever values we get. That is why this is a quick and dirty approach that can produce questionable results depending on the specific phenomenon and sizes of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slightly Better for \"non inferiority\"  Newcombe (score-based / Wilson)\n",
    "\n",
    "Much better actual coverage than Wald, especially with imbalanced samples or extreme p.\n",
    "Still fairly simple to compute (closed-form formulas exist, or can be coded).\n",
    "\n",
    "we wont' cover it here but it is still a plug-in hacks suffering from circularity argument and n=1 problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Miettinen–Nurminen\n",
    "\n",
    "Widely used in clinical trials and non-inferiority contexts and in some regulated industry like pharmaceitical (recommended by the FDA)\n",
    "\n",
    "Very complex and hard to explain, and is still fundmentally a plug in hack with the same circular reasoning and  n=1 We wont cover it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Postive, pvalue, signifiance level $\\alpha$ (a.k.a confidence by some people) and critical value\n",
    "\n",
    "\n",
    "Once we approximated the  standard error SE (which is the standard devation of $\\hat{\\Delta}$) as explained above, teh NHST methodology then also fixed the expected value of $\\hat{\\Delta}$ under H0. The idea is that if you fix bothe the expected value and standar deviation , you can then work with a binomial or guassian probabilityy distribution to compute whatever event probabilityy you want. Even though the process is descrete and we should use a binomial continyusous gaussian are used in practice as htey are easier to manipulate in equations and are good approxmiation of binomial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The other simplificaiton is that although H0 is an inquality $E(\\Delta) \\leq -\\epsilon$ we are going to actuall use $E(\\Delta) = -\\epsilon$ to get a single probabilityy distribution to work with, with the rationale that the boundary is the least favorable case, that is if we reject H0 at the boundary $-\\epsilon$ logically we would reject it also at anythng with $E(\\Delta) \\leq -\\epsilon$, as our observation would be get an even lower probabilityy if the mean shifted  to the left.So using the boundary value epsilon is that one that makes it the most difficult to cross the alpha threshold and the most difficult to reject $H_0$\n",
    "\n",
    "So with all that in place we can finally model H0 with a gaussian of mean $\\mu = -\\epsilon$ and standard deviation $\\sigma = \\hat{SE}$ \n",
    "\n",
    "$N(\\mu, \\sigma)$\n",
    "\n",
    "Now we can compute the p-value which is the probabilityy of observing or something like the sample we got or more extreme in the direction of H1. In our case it would be probabilityy of the sample being in $[\\hat{\\Delta_{obs}}, +\\infty]$ which can be comnputed by integrating the right tail of the gaussian from $\\hat{\\Delta_{obs}}$ to $+\\infty$. This integral of the right tail is called the survival function of the gaussian and can be computed direct in python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The direct way of computing the survival function and therefore the pvalue  directly from the observation $\\hat{\\Delta_{obs}}$ would be to integrate\n",
    "\n",
    "pvalue $ = \\int_{ \\hat{\\Delta_{obs}} }^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}(SE)} e^{-\\frac{(x - (-\\epsilon))^2}{2(SE)^2}} dx$\n",
    "\n",
    "Using a common notation where $\\Phi$ is the cumulative distribtiuon of the standard normal N(0,1), the survival would altneratively expressed as this if one does not have something that compute survial function directly\n",
    "\n",
    "pvalue = $1 - \\Phi(\\frac{\\hat{\\Delta_{obs}} - \\mu}{\\sigma})$\n",
    "\n",
    "that we will compare to the $\\alpha$, in our case 0.05, if the the probabilityy of seeing what we observed or something even more favorable (the right tail) is 5% or less, we reject the null hypothesis H0 because we are seeing is very unlikely under H0. The critical value is the inverse of the right tail integratiojn, is the the value of hte observation that would make the probability of seeing the sample 5% or lower in our setup. The formula to inverse it from the $\\alpha$ is\n",
    "\n",
    "$c = \\mu  + \\sigma \\Phi^{-1}(1 - \\alpha)$\n",
    "\n",
    "Below is a full computation and a vizualization of the parameters along the right tail\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided): {p_value:.4f}')\n",
    "\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for p-value={alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values; recompute to be robust\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H0 - 6 * sigma_HO\n",
    "right = mu_H0 + 6 * sigma_HO\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Right-tail probabilityy\n",
    "p = norm.sf(x0, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Critical value at significance alpha (one-sided)\n",
    "crit_x = norm.ppf(1 - alpha, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade right tail\n",
    "mask = xs >= x0\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Right tail p = {p:.4g}\")\n",
    "\n",
    "# Vertical line at observed x\n",
    "ax.axvline(x0, color=\"C1\", ls=\"--\", lw=1.5, label=f\"observed delta = {x0:.4f}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(crit_x, color=\"C2\", ls=\"-.\", lw=1.5, label=f\"critical value c = {crit_x:.4f}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H0\n",
    "ax.axvline(-epsilon, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H0 = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H0:.4f}, σ={sigma_HO:.4f}) — Right-tail beyond x\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under the null H0\")\n",
    "ax.set_ylabel(\"Probability density under H0\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.show(), display handled by notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### traditional presentation of the same conmputation using z-scores\n",
    "\n",
    "The traditional ways for NHST to compute the p value is not to use the measurment $\\hat{\\Delta_{obs}}$ directly but to normalize it to a standard normal and centered on zero and on standard deviation 1 N(0,1). This mormalization is done by transforming the measurment into a so called z-score:\n",
    "\n",
    "The the z-score is just the way you can rescale an arbitrary Gaussian to the Normal by substracting the meand and dividing by the standard deviation $Z = \\frac{X-\\mu}{\\sigma}$\n",
    "\n",
    "$Z_{NI} = \\frac{\\hat{\\Delta} - (E(\\Delta_{H_{boundary}}))}{SE}$ \n",
    "\n",
    "$= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}$ \n",
    "\n",
    "$\\boxed{Z_{NI}= \\frac{\\hat{\\Delta} + \\epsilon}{SE}}$\n",
    "\n",
    "Then  $\\frac{\\hat{\\Delta} + \\epsilon}{SE}$ is a normal N(0,1) and \n",
    "\n",
    "$p(\\hat{\\Delta} \\ge \\hat{\\Delta(w)})$\n",
    "\n",
    "$ = p(\\frac{\\hat{\\Delta} + \\epsilon}{SE} \\ge \\frac{\\hat{\\Delta(w) +\\epsilon}}{SE })$\n",
    "\n",
    "Then we can compute the survial function but this timne on a standard normal and from the z-score value\n",
    "\n",
    "$\\int_{ Z_{NI} }^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{1}{2}z^2} dz$\n",
    "\n",
    "which give the exact same results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positive\n",
    "In this NHST the pvalue is also the probability of a false positive that is the probabilityy of rejecting the null hopythisis (saying htere is no degradation) while there is acxtually a degradation. So it is setup our risk of make the wrong decision is 5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Probability of having false negative a.k.a type 2 error , Power and sample size required to get to signficance under those conditions\n",
    "\n",
    "Conversely , \"False negative\" in the context of a non inferiority test is the probabilityy of failing to reject given the fact that H1 is true, meaning we fail to clear the non inferiority test, while the new UX is actually non inferior (i.e. as good or better than the old one). So we have the same issue as for the false poistive computation, we have to pick a value for what expected value for the $\\Delta$ but this time under H1. As we cannot really work with a range of expecgted value for $\\Delta$  we need a single value to fix the gaussian we are going to integrate. Here possible choice is to also pick a boundary which is $E(\\Delta) = E(p_C)$ so whatever the control group mean was or is (depending on how much historical data we have). This is usally called \"minimum effect size we want to detect\" as is really a business decision, it is a bit arbitrary. Pickig our baseline is just one option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To model the estimator under the alternative hypothesis H1, we need to pick another expected value for the $\\Delta$, since this is for H1 which means the same or better, we pick the bare minimum we need to make that statement that is \"the same\" and an expected value of zero for the Delta. Under these condition we can pool the samples to get an estimator of the variance and standard error as we are assuming they are distributed indentically\n",
    "\n",
    "$SE | H1 = WaldPooled SE = \\sqrt{\\hat{p}(\\omega_{pool})(1- \\hat{p}(\\omega_{pool})) (\\frac{1}{n_C} + \\frac{1}{n_A})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place we compute the probability of the estimator coming up \"higher\" but with the same critical value as it was established ahead of time with the significance level alpha.\n",
    "\n",
    "The Beta is the probability that we observed something lower and up to the critical value defined by inverting the p-value but under H1, meaning the distribution is centered on the H1 expected value, in our case it is zero.\n",
    "\n",
    "This computation will give us whatever it gives us. If we want to TARGET a Power of 0.8 (which means β = 0.2, or 20% Type II error rate), we can compute the sample sizes (implied in SE) that will make us reach that level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"probabilityy of false negative a.k.a β a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Power (1 - β): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H1 - 6 * sigma_H1\n",
    "right = mu_H1 + 6 * sigma_H1\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H1, scale=sigma_H1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density under H₁\")\n",
    "\n",
    "# Shade left tail (Type II error region)\n",
    "mask = xs <= critical_value\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, \n",
    "                label=f\"Type II error β = {beta:.4g}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(critical_value, color=\"C2\", ls=\"-.\", lw=1.5, \n",
    "           label=f\"critical value c = {critical_value:.4f}\")\n",
    "\n",
    "# Vertical line at observed delta\n",
    "ax.axvline(hatDelta_observed, color=\"C1\", ls=\"--\", lw=1.5, \n",
    "           label=f\"observed delta = {hatDelta_observed:.4f}\")\n",
    "\n",
    "# Vertical line at the mean under H1\n",
    "ax.axvline(mu_H1, color=\"k\", ls=\":\", lw=1.5, \n",
    "           label=f\"mean under H₁ = {mu_H1:.4f}\")\n",
    "\n",
    "# Vertical line at the H0 boundary (for reference)\n",
    "ax.axvline(-epsilon, color=\"gray\", ls=\":\", lw=1.5, alpha=0.7,\n",
    "           label=f\"H₀ boundary = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H1:.4f}, σ={sigma_H1:.4f}) — Type II Error (β) and Power\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under H₁\")\n",
    "ax.set_ylabel(\"Probability density under H₁\")\n",
    "ax.legend(loc=\"best\", fontsize=9)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "\n",
    "# Add text annotation for power\n",
    "power_text = f\"Power = 1 - β = {power:.4f}\"\n",
    "ax.text(0.98, 0.95, power_text, transform=ax.transAxes, \n",
    "        fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST the Bayesian approach is conceptually simpler, instead of artificially considering only 2 possible expected value boundaries for $\\Delta$ one for H0 and one for H1, we assume we just don't know that $E(\\Delta)$ can take any value between 0 and 1 , and this is just itself random variable but one that we can quantify accross all possible world\n",
    "\n",
    "#### Using Beta distribution as a tool to model our 'pior belief\"\n",
    "\n",
    "For kind of Bernouilli trial we are dealin wiht (i.e. success/failure, convert/abandom etc...) the most convienent model for our prior belive is what is know as a Beta distribution, note that the anme \"Beta\" is unfortunate it as it sonds like the Beta in NHST, but it  has nothint ot with it, is is a probability distribution depending on 2 parameters which depening on how they are set can model \"we know nothing a piorir\" a.k.a an uniformative prior, or  \"we don't know much but just that the coversion rate should be aropund 20% but we may be surprised\" to  very strong prior based on tons of historical data \"the conversion shold be ver close to 20%\"\n",
    "\n",
    "Note tha the Beta distribution is deined on the interval [0,1] as it is modeling a probability, so that'teh probability of a probability (meta probability)\n",
    "\n",
    "Here are  few example of priors that coud be used, in cour case we will try with a totally unfornative priot (we know nothing) and weakly one (shoub be around 17% but we are not sure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Four cases:\n",
    "cases = [\n",
    "    (\"Uninformative (flat)\", (\"single\", (1, 1))),\n",
    "    (\"Weakly informative (centered, high entropy)\", (\"single\", (3, 12))),   # mean=0.2\n",
    "    (\"Strong conviction (centered, low entropy)\", (\"single\", (200, 800))),  # mean=0.2\n",
    "    (\"Bi-modal (mixture of Betas)\", (\"mixture\", ((5, 20, 0.5), (20, 5, 0.5))))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (title, (kind, params)) in zip(axes.ravel(), cases):\n",
    "    if kind == \"single\":\n",
    "        a, b = params\n",
    "        y = beta_dist.pdf(x, a, b)\n",
    "        mean = a / (a + b)\n",
    "        ci_low, ci_high = beta_dist.ppf([0.025, 0.975], a, b)\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2)\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        ax.axvline(mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mean={mean:.3f}\")\n",
    "        ax.axvline(ci_low, color=\"C1\", ls=\"--\", lw=1, label=\"95% CI\")\n",
    "        ax.axvline(ci_high, color=\"C1\", ls=\"--\", lw=1)\n",
    "        ax.set_title(f\"{title}\\nBeta(α={a}, β={b})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "    else:\n",
    "        # Mixture of two Betas: (a1, b1, w1), (a2, b2, w2)\n",
    "        (a1, b1, w1), (a2, b2, w2) = params\n",
    "        y = w1 * beta_dist.pdf(x, a1, b1) + w2 * beta_dist.pdf(x, a2, b2)\n",
    "        m1, m2 = a1 / (a1 + b1), a2 / (a2 + b2)\n",
    "        mix_mean = w1 * m1 + w2 * m2\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2, label=\"mixture pdf\")\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        # Component means\n",
    "        ax.axvline(m1, color=\"C2\", ls=\"--\", lw=1, label=f\"mean₁={m1:.2f}\")\n",
    "        ax.axvline(m2, color=\"C3\", ls=\"--\", lw=1, label=f\"mean₂={m2:.2f}\")\n",
    "        # Mixture mean\n",
    "        ax.axvline(mix_mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mixture mean={mix_mean:.2f}\")\n",
    "        ax.set_title(f\"{title}\\n{w1:.1f}·Beta({a1},{b1}) + {w2:.1f}·Beta({a2},{b2})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"p\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"density\")\n",
    "\n",
    "fig.suptitle(\"Four Beta Priors: flat, weakly centered, strongly centered, bi-modal\", fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative approach assuming we know nothing (a.k.a non informative prior)\n",
    "\n",
    "For the Variant A, assuming we start with an non informative prior meaning that we have no idea, p_A could range from 0 to 1 with equal probabilityy model by the beta function $Beta(1,1)$  \n",
    " \n",
    "after n trials and k success the posterior probabilityy distribution due to the property of the Beta function is\n",
    " \n",
    "$Beta(xA+1, n_A - xA+1)$ \n",
    "\n",
    "This derived through Bayes Theorem and how the Beta function can be integrated\n",
    "\n",
    "The expected value for $E(Beta(\\alpha,\\beta)) = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "\n",
    "which for a bayesian posterior becomes\n",
    "\n",
    "$E(Beta(xA+1,nA−xA+1)) = \\frac{xA+1}{xA+1 + nA−xA+1} = \\frac{xA+1}{nA+2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the credible intervals and visualize the prior and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "# Ensure we use the beta distribution from scipy.stats\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the 99% credible interval (1th and 99th percentiles)\n",
    "p_L_99 = beta_dist.ppf(0.005, alpha, beta_param)\n",
    "p_U_99 = beta_dist.ppf(0.995, alpha, beta_param)\n",
    "# Output the result\n",
    "print(f\"99% Credible Interval for p: [{p_L_99:.4f}, {p_U_99:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-informative prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_noninformative_pdf = beta_dist.pdf(x_range, 1, 1)  # Beta(1,1) - uniform\n",
    "posterior_noninformative_pdf = beta_dist.pdf(x_range, alpha, beta_param)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_noninformative_pdf, 'b--', lw=2, label='Prior: Beta(1, 1) - Non-informative')\n",
    "ax.plot(x_range, posterior_noninformative_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha:.1f}, {beta_param:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L) & (x_range <= p_U)\n",
    "ax.fill_between(x_range[mask], posterior_noninformative_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Non-informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probability that variant is non-inferior using non-informative prior\n",
    "prob_non_inferior_noninformative = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                                       alpha, beta_param)\n",
    "print(f\"Probability that variant is non-inferior (non-informative prior): {prob_non_inferior_noninformative:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior_noninformative*100:.2f}% probability that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Informative prior approach using historical data\n",
    "\n",
    "Instead of using a non-informative prior Beta(1,1), we can incorporate our historical knowledge. We know the control group has a conversion rate of 0.2, and we want to test for non-inferiority with margin epsilon = 0.03. \n",
    "\n",
    "For the variant, we'll use a prior centered at 0.2 - 0.03 = 0.17 (the boundary of non-inferiority), but with high entropy to reflect our uncertainty. A Beta distribution with mean μ and relatively small α and β values will have high entropy while still being informative about the center.\n",
    "\n",
    "For a Beta(α, β) distribution:\n",
    "- Mean: $\\mu = \\frac{\\alpha}{\\alpha + \\beta}$\n",
    "- To get high entropy (concave distribution), we want small α and β values (both < 1 makes it U-shaped, but values around 2-5 give high entropy while remaining unimodal)\n",
    "\n",
    "Let's use α = 3, and solve for β to get mean = 0.17:\n",
    "- $0.17 = \\frac{3}{3 + \\beta}$\n",
    "- $\\beta = \\frac{3}{0.17} - 3 \\approx 14.65$\n",
    "\n",
    "After observing the data, the posterior will be Beta(xA + α, nA - xA + β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative prior parameters\n",
    "target_prior_mean = control_group_conversion_rate - epsilon  # 0.17\n",
    "alpha_prior = 3  # Small value for high entropy\n",
    "beta_prior = (alpha_prior / target_prior_mean) - alpha_prior  # Solve for beta given mean\n",
    "\n",
    "print(f\"Prior: Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "print(f\"Prior mean: {alpha_prior / (alpha_prior + beta_prior):.4f}\")\n",
    "print(f\"Prior variance: {(alpha_prior * beta_prior) / ((alpha_prior + beta_prior)**2 * (alpha_prior + beta_prior + 1)):.6f}\")\n",
    "\n",
    "# Posterior parameters after observing data\n",
    "alpha_posterior = xA_observed + alpha_prior\n",
    "beta_posterior = (nA - xA_observed) + beta_prior\n",
    "\n",
    "print(f\"\\nPosterior: Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Compute 95% credible interval\n",
    "p_L_informative = beta_dist.ppf(0.025, alpha_posterior, beta_posterior)\n",
    "p_U_informative = beta_dist.ppf(0.975, alpha_posterior, beta_posterior)\n",
    "print(f\"95% Credible Interval: [{p_L_informative:.4f}, {p_U_informative:.4f}]\")\n",
    "\n",
    "# Compute 99% credible interval\n",
    "p_L_99_informative = beta_dist.ppf(0.005, alpha_posterior, beta_posterior)\n",
    "p_U_99_informative = beta_dist.ppf(0.995, alpha_posterior, beta_posterior)\n",
    "print(f\"99% Credible Interval: [{p_L_99_informative:.4f}, {p_U_99_informative:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_pdf = beta_dist.pdf(x_range, alpha_prior, beta_prior)\n",
    "posterior_pdf = beta_dist.pdf(x_range, alpha_posterior, beta_posterior)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_pdf, 'b--', lw=2, label=f'Prior: Beta({alpha_prior:.1f}, {beta_prior:.1f})')\n",
    "ax.plot(x_range, posterior_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha_posterior:.1f}, {beta_posterior:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L_informative) & (x_range <= p_U_informative)\n",
    "ax.fill_between(x_range[mask], posterior_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilityy that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "prob_non_inferior = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                       alpha_posterior, beta_posterior)\n",
    "print(f\"Probability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probabilityy that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
