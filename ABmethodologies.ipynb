{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec Summary\n",
    "\n",
    "THis notebook goes into a lot details and present all the math to justify using a Bayesian approach, but if you want to skip the math and just get the benefit read this, and to simply access the code that can analuze the AB test amd make decision go to the end of the notebook\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "When launching new web or mobile features, engineering teams face a common dilemma:\n",
    "\n",
    "- **Limited traffic allocation**: New features get only 5-10% of traffic to minimize risk\n",
    "- **Multiple variants**: Design teams often propose 3-5 different implementations\n",
    "- **Small sample sizes**: Each variant may only see hundreds or low thousands of users\n",
    "- **Need for speed**: Business wants fast decisions to iterate or scale\n",
    "\n",
    "**Traditional NHST fails here**: With small, unbalanced samples (e.g., 7,000 control vs. 150 per variant), statistical tests either:\n",
    "- Fail to reach significance (underpowered, β > 80%)\n",
    "- Require weeks of data collection\n",
    "- Cannot reliably compare multiple variants\n",
    "\n",
    "## The Bayesian Solution\n",
    "\n",
    "Bayesian methods excel precisely where NHST struggles:\n",
    "\n",
    "### 1. **Works with Small Samples**\n",
    "- Incorporates prior knowledge (historical conversion rates)\n",
    "- Updates beliefs incrementally as data arrives\n",
    "- Provides meaningful conclusions even with n=150 per variant\n",
    "- No arbitrary \"minimum sample size\" requirement\n",
    "\n",
    "### 2. **Handles Unbalanced Allocation Naturally**\n",
    "- 90% control, 10% variants? No problem.\n",
    "- Each variant can have different sample sizes\n",
    "- No need for equal allocation or \"balanced designs\"\n",
    "- Protects existing user experience while testing\n",
    "\n",
    "### 3. **Scales to Many Variants Effortlessly**\n",
    "- Compare 3, 5, 10, or 100 variants simultaneously\n",
    "- Single coherent analysis—no multiple comparison penalties\n",
    "- Direct answer: P(A is best) = 31%, P(B is best) = 47%, P(C is best) = 22%\n",
    "\n",
    "### 4. **Provides Actionable Probabilities**\n",
    "- NHST says: \"Cannot reject H₀\" (not actionable)\n",
    "- Bayesian says: \"47% chance B is best, 22% it's worse than control\" (actionable)\n",
    "- Direct business decision: Deploy B with quantified risk\n",
    "\n",
    "### 5. **Allows Continuous Monitoring**\n",
    "- Can check results anytime without \"p-hacking\" concerns\n",
    "- Update posteriors as new data arrives\n",
    "- Stop early if clear winner emerges\n",
    "- Continue if more certainty needed—mathematically rigorous\n",
    "\n",
    "\n",
    "## Key Benefits Summary\n",
    "\n",
    "| Aspect | Traditional NHST | Bayesian Approach |\n",
    "|--------|-----------------|-------------------|\n",
    "| Small samples | Underpowered, inconclusive | Works well with prior knowledge |\n",
    "| Unbalanced allocation | Loses efficiency | No problem |\n",
    "| Multiple variants | Complex corrections needed | Natural single analysis |\n",
    "| Interpretation | p-value (hard to explain) | Probability (intuitive) |\n",
    "| Decision making | Binary reject/fail | Quantified risk/confidence |\n",
    "| Continuous monitoring | Forbidden (p-hacking) | Allowed and rigorous |\n",
    "| Time to decision | Weeks (need larger n) | Days (works with small n) |\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "For modern product development with:\n",
    "- **Rapid iteration cycles**\n",
    "- **Risk-averse traffic allocation**\n",
    "- **Multiple design options**\n",
    "- **Small initial samples**\n",
    "\n",
    "**Bayesian methods provide:**\n",
    "- Faster decisions (days vs. weeks)\n",
    "- Better use of limited data\n",
    "- Clear, business-friendly outputs\n",
    "- Quantified confidence for risk management\n",
    "\n",
    "This enables product teams to launch confidently, iterate quickly, and scale successful features—all while protecting the existing user experience.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The two standalone Python functions in this notebook provide:\n",
    "1. `test_non_inferiority()` - Verify new features don't degrade experience\n",
    "2. `select_best_variant()` - Choose winning variant with probability\n",
    "\n",
    "Both work with any sample sizes and scale to any number of variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function\n",
    "from scipy.stats import beta as beta_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth Analysis of the 2 methodologies\n",
    "\n",
    "When evaluating new user experiences (UX) — such as launching **passkeys** and measuring their impact on abandonment rates — our CX team has provided us with 3 variants A, B and C for the passkey creation experience. After launch we need to make sure that adding passkey creation does not degrade the success rate of our current CX sigificantly comparing the variant to a control group that will remain on our legacy pages, then we have to decide which of the variant perform best.\n",
    "  \n",
    "This notebook compares two major approaches:\n",
    "\n",
    "- **Null Hypothesis Significance Testing (NHST)** — the long-standing statistical framework, widely used but often conceptually tricky and difficult to interpret in practical decision-making.\n",
    "- **Bayesian methods** — increasingly popular because they offer more flexibility, better ability to work on small  samples and produce results that are often easier to interpret directly when deciding actions (e.g., when to shift more traffic from a control to a new variant).\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "For the sake of the discussion lets' assume an existing digital identity creation flow with a **completion rate of ~20%** (meaning ~80% of users abandon).  \n",
    "\n",
    "Our test design:\n",
    "\n",
    "- Keep **x%** of traffic on the current experience as the **control group**.\n",
    "- Send the remaining traffic to one or more **variants** $A, B, C$.\n",
    "\n",
    "The test will be conducted in 2 steps, first we need to determine that each new experience is **no worse** than the current one, accepting small degradaition as unavoidable as we are adding more pages and clicks  — then after establishing we haven't degraded the experience, shifting more traffic to the variants and decidng which is the better-performing variant.\n",
    "\n",
    "The type of A/B test — where the first goal is to ensure a new design does **not degrade** the experience — is called a **non-inferiority test** (explained below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST)\n",
    "\n",
    "At a high level, the **NHST** workflow is:\n",
    "\n",
    "1. **Assume what you *don’t* want to see** — this is the **null hypothesis**.  \n",
    "   - Example in medicine: *“the drug has no effect.”*  \n",
    "   - Example here: *“the new experience increases abandonment.”*\n",
    "2. **Run the experiment** and compute a test statistic.\n",
    "3. **Ask:** *If the null were true, how likely is it that we would observe a result at least this extreme?*  \n",
    "   - If that probability (the **p-value**) is very low — e.g., below a conventional threshold such as 5% — we **reject the null**.\n",
    "\n",
    "Two immediate caveats:\n",
    "- Rejecting the null does **not** prove the opposite is true; it only says the data would be unlikely *if* the null were correct.\n",
    "- “Unlikely enough” is arbitrary — thresholds like 5% are conventions, not laws of nature.\n",
    "\n",
    "A key point: NHST computes **$P(\\text{data} \\mid \\text{hypothesis})$**.  \n",
    "Later we’ll see that the Bayesian approach instead computes **$P(\\text{hypothesis} \\mid \\text{data})$** — a fundamentally different quantity.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Conversion as Random Variables\n",
    "\n",
    "The abandonment or conversion of a UX flow can be modeled with **Bernoulli random variables**:\n",
    "\n",
    "- $X_C$ for the control experience\n",
    "- $X_A$ for a new variant $A$\n",
    "\n",
    "A Bernoulli variable takes only two values: success/failure, convert/abandon, etc.  \n",
    "Each user who sees a page gives one draw from one of these variables.\n",
    "\n",
    "We assume both have the same codomain:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}_C = \\mathcal{X}_A = \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where **1 = convert** (user finishes the intended action, e.g., creating a passkey) and **0 = abandon**.  \n",
    "Technical failures are treated as *success* here because the user attempted the action.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Proportions\n",
    "\n",
    "NHST usually works with **sample proportions**, the average of $n$ Bernoulli draws:\n",
    "\n",
    "$$\n",
    "\\hat{p}_C = \\frac{1}{n}\\sum_{i=1}^n X_{C_i},\n",
    "\\quad\n",
    "\\hat{p}_A = \\frac{1}{n}\\sum_{i=1}^n X_{A_i}.\n",
    "$$\n",
    "\n",
    "Each $\\hat{p}$:\n",
    "\n",
    "- Is a random variable taking values $\\{0,\\tfrac1n,\\tfrac2n,\\ldots,1\\}$.\n",
    "- Is also an **estimator** of the true expected value $p = E[X]$.  \n",
    "  By the law of large numbers, $\\hat{p} \\to p$ as $n$ grows.\n",
    "\n",
    "(Statisticians use a “hat” to denote an estimator.)\n",
    "\n",
    "Formally, an estimator maps $n$ realizations of $X$ ($\\mathcal{X}^n$) to a real number:\n",
    "\n",
    "$$\n",
    "\\hat{p}: \\mathcal{X}^n \\to [0,1].\n",
    "$$\n",
    "\n",
    "Because it is the mean of $n$ Bernoulli variables, $\\hat{p}$ follows a **binomial** distribution that becomes approximately **Gaussian** when $n$ is large.\n",
    "\n",
    "---\n",
    "\n",
    "### Variance and Standard Deviation of a Sample Proportion\n",
    "\n",
    "For a single Bernoulli $X$:  \n",
    "$$\n",
    "\\mathrm{Var}(X) = p(1-p).\n",
    "$$\n",
    "\n",
    "For the sample proportion:\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\tfrac1n \\sum_{i=1}^n X_i\\right)\n",
    "= \\tfrac1{n^2} n p(1-p)\n",
    "= \\tfrac{p(1-p)}{n}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The square root of this variance is the **standard error** — a quantity we’ll use later.\n",
    "\n",
    "$$\n",
    "\\boxed{SE = SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Difference in Proportions\n",
    "\n",
    "We’ll often look at the **difference** between variant and control:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} = \\hat{p}_A - \\hat{p}_C\n",
    "$$\n",
    "\n",
    "This estimates the true difference\n",
    "\n",
    "$$\n",
    "\\Delta = p_A - p_C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis $H_0$** — the “bad” scenario we want to reject:  \n",
    "  the new UX **degrades** conversion by at least some small amount $\\epsilon$ we consider unacceptable (e.g., $3\\%$):\n",
    "\n",
    "  $$\n",
    "  H_0: E[\\Delta] \\le -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis $H_1$** — the new UX is **not worse** than control (possibly better):\n",
    "\n",
    "  $$\n",
    "  H_1: E[\\Delta] > -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Boundary Hypothesis** — used in test construction:  \n",
    "  assume the difference is exactly at the acceptable degradation limit:\n",
    "\n",
    "  $$\n",
    "  E[\\Delta] = -\\epsilon\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Example\n",
    "\n",
    "For a concrete example, we define the following counts and quantities for each experience:\n",
    "\n",
    "- $n_C$ — number of visitors in the **control** group  \n",
    "- $x_C$ — number of **conversions** observed in the control group\n",
    "\n",
    "- $n_A$ — number of visitors in the **variant** group  \n",
    "- $x_A$ — number of **conversions** observed in the variant group\n",
    "\n",
    "- $\\hat{\\Delta}_{\\mathrm{obs}}$ — the **observed difference** in conversion proportions between variant and control\n",
    "\n",
    "- $\\epsilon$ — the **acceptable degradation margin**, i.e., the smallest decrease in conversion we are willing to tolerate for the new variant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_conversion_rate = 0.2 # based on historical data\n",
    "nC = 7000\n",
    "xC_observed = nC * control_group_conversion_rate\n",
    "nA = 150\n",
    "xA_observed = 33\n",
    "epsilon = 0.03\n",
    "alpha = 0.05\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed  / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_observed:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_observed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the Estimator $\\hat{\\Delta}$ (a.k.a. Standard Error in Frequentist Statistics)\n",
    "\n",
    "In NHST, the first step is to **estimate the standard deviation** of the estimator $\\hat{\\Delta}$ (often called the **standard error**, SE).  \n",
    "We then compare the observed difference in proportions from the experiment to this estimated variability to decide whether the observed effect is “far enough” from what we would expect under the null hypothesis $H_0$.\n",
    "\n",
    "This is a key pain point for NHST:\n",
    "\n",
    "- We **do not know** the true standard deviation — it depends on the unknown underlying conversion probabilities.\n",
    "- Frequentist methods therefore use the **plug-in principle**: estimate the unknown variance by “plugging in” the sample estimates (the data you just observed).\n",
    "\n",
    "But note the circularity:\n",
    "\n",
    "1. We want to know if the data are unusual under $H_0$.\n",
    "2. To measure “unusual,” we need the standard error assuming $H_0$.\n",
    "3. SE depends on the unknown true rates, so we **plug in** $\\hat{p}$ (from the data!).\n",
    "4. We then use this data-derived SE to judge whether the data are unusual.\n",
    "\n",
    "It’s like saying: *“Use my one measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising.”*\n",
    "\n",
    "Frequentists accept this because:\n",
    "\n",
    "- **Long-run frequency view:** if we repeated the procedure many times, it would have correct average properties.\n",
    "- **Pragmatism:** it’s computable from one experiment.\n",
    "- **Simulation evidence:** works “reasonably well” for moderate $n$, though “reasonably well” is debatable (see the *replication crisis*).\n",
    "- **Framework limitation:** in classical stats, parameters are fixed unknowns, so no natural way to treat them as random.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Plug-In Approaches (a.k.a. “Standard Error Hacks”)\n",
    "\n",
    "#### 1. Wald **Pooled** Standard Error (for a “No Effect” Hypothesis)\n",
    "\n",
    "If an hypothesis is **no effect** ($p_A = p_C$), we can pool data from control and variant, because under the hypothesis they are assumed to come from the same distribution.\n",
    "\n",
    "Realizations of sample proportions are:\n",
    "\n",
    "$$\n",
    "\\hat{p}_A = \\frac{x_A}{n_A}, \\qquad\n",
    "\\hat{p}_C = \\frac{x_C}{n_C}.\n",
    "$$\n",
    "\n",
    "If \"no effect\" as true (no difference), a pooled estimator is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{pool}} = \\frac{x_A + x_C}{n_A + n_C}.\n",
    "$$\n",
    "\n",
    "Variance of the difference between two independent proportions is the sum of their variances:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\mathrm{Var}(\\hat{p}_A) + \\mathrm{Var}(\\hat{p}_C).\n",
    "$$\n",
    "\n",
    "Plugging in the pooled estimate:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right).\n",
    "$$\n",
    "\n",
    "So the **pooled standard error** is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{WaldPooled SE} =\n",
    "\\sqrt{\\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right)} }\n",
    "$$\n",
    "\n",
    "This is the classic **standard error of the difference between two independent proportions**.\n",
    "\n",
    "Once SE is computed, we calculate a **z-score** (number of SEs the observed difference is away from the null value 0):\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p}_A - \\hat{p}_C}{\\text{SE}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wald **Unpooled** Standard Error (for Non-Inferiority)\n",
    "\n",
    "If the null is **non-inferiority** (allowing a margin $\\epsilon$), we **cannot** assume $p_A = p_C$, so we don’t pool.\n",
    "\n",
    "Instead we sum the individual variances (using the plug-in trick separately for each group):\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{WaldUnpooled SE}} =\n",
    "\\sqrt{\\frac{\\hat{p}_A(1-\\hat{p}_A)}{n_A} +\n",
    "      \\frac{\\hat{p}_C(1-\\hat{p}_C)}{n_C}}.\n",
    "$$\n",
    "\n",
    "Ideally, the true $p_A$ and $p_C$ should be used here, but we don’t know them — so we substitute $\\hat{p}_A$ and $\\hat{p}_C$.  \n",
    "This works but can be inaccurate if sample sizes are small or rates are extreme.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Newcombe / Score-Based (Wilson)\n",
    "\n",
    "- **Better coverage** than Wald, especially with imbalanced sample sizes or very high/low $p$.\n",
    "- Still closed-form and relatively easy to compute.\n",
    "- Still uses plug-in estimates and suffers from the same “single-sample” limitation.\n",
    "\n",
    "*(Not detailed here — but recommended over Wald when possible.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Miettinen–Nurminen\n",
    "\n",
    "- Widely used in **clinical trials** and regulated industries (e.g., FDA guidance).\n",
    "- Provides improved accuracy for non-inferiority tests.\n",
    "- However: mathematically complex, still plug-in based, and still fundamentally relies on one sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Frequentist methods **must estimate variance from the data itself** — leading to circularity and potential miscalibration, especially for small samples or edge cases.  \n",
    "Later we’ll see how Bayesian methods avoid this by modeling uncertainty about the true conversion rates directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Positive, *p*-value, Significance Level $\\alpha$ (sometimes called “confidence”), and Critical Value\n",
    "\n",
    "Once we have an estimate of the standard error $SE$ (the standard deviation of $\\hat{\\Delta}$), NHST assumes a **sampling distribution** for the estimator under the null hypothesis $H_0$.\n",
    "\n",
    "The idea is:\n",
    "\n",
    "- If we know (or assume) the **mean** and **standard deviation** of $\\hat{\\Delta}$ under $H_0$,  \n",
    "- we can model it with a known probability distribution and calculate how likely any observed result is.\n",
    "\n",
    "Although the true process is discrete (binomial), in practice we often approximate it by a **normal (Gaussian)** distribution. This is mathematically simpler and is a good approximation for moderate or large sample sizes.\n",
    "\n",
    "---\n",
    "\n",
    "##### Using the “Boundary” Case\n",
    "\n",
    "The null hypothesis  for \"non inferiority\" is technically an inequality:\n",
    "\n",
    "$$\n",
    "H_0: E[\\Delta] \\le -\\epsilon.\n",
    "$$\n",
    "\n",
    "But to get a single distribution to work with, we typically use the **boundary value**:\n",
    "\n",
    "$$\n",
    "E[\\Delta] = -\\epsilon.\n",
    "$$\n",
    "\n",
    "Why?  \n",
    "- The boundary is the **least favorable** case for rejecting $H_0$.  \n",
    "- If we can reject $H_0$ at $E[\\Delta] = -\\epsilon$, then we would also reject any stronger null (mean further left).  \n",
    "- This makes the test conservative.\n",
    "\n",
    "So under $H_0$, we model $\\hat{\\Delta}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = -\\epsilon, \\qquad \\sigma = SE.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Computing the *p*-Value\n",
    "\n",
    "The **p-value** is the probability (under $H_0$) of observing a result **as extreme or more extreme** than what we got, in the direction of the alternative $H_1$.\n",
    "\n",
    "In this one-sided non-inferiority test, that means the right tail probability:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P_{H_0}\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\text{obs}}\\big]\n",
    "= \\int_{\\hat{\\Delta}_{\\text{obs}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n",
    "\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,dx.\n",
    "$$\n",
    "\n",
    "This tail integral is the **survival function** of the normal distribution.\n",
    "\n",
    "Using the standard normal CDF $\\Phi$:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = 1 - \\Phi\\!\\left(\\frac{\\hat{\\Delta}_{\\text{obs}}-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Significance Level $\\alpha$ and Critical Value\n",
    "\n",
    "- We choose a **significance level** $\\alpha$ (often $0.05$).  \n",
    "- If $p\\text{-value} \\le \\alpha$, we reject $H_0$ — our result is unlikely under the null.\n",
    "\n",
    "The **critical value** $c$ is the smallest observed difference that would lead to rejection at level $\\alpha$. It is obtained by inverting the right-tail probability:\n",
    "\n",
    "$$\n",
    "c = \\mu + \\sigma \\,\\Phi^{-1}(1 - \\alpha).\n",
    "$$\n",
    "\n",
    "Any observed $\\hat{\\Delta}_{\\text{obs}} \\ge c$ yields $p\\text{-value} \\le \\alpha$ and thus rejects $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "Below we’ll compute the p-value and critical value explicitly and visualize how the right tail behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided): {p_value:.4f}')\n",
    "\n",
    "# inverse survival function to find critical value for given p-value\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for p-value={alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the value we chose an example for the sample the p-value is a bit more than 7%, so that's not unlikely enough if we picked 5% as the alpha, so we would \"fail to rejet\" , menaning we can't say much, we don't know whether we an claim and that new CX does not cause some unwanted degradation. Note that this is because the sample is tiny, we picked it small on purpose as this is a realistic situation when launching a new feature yuo just pujt a tiny big of traffic on the feature to make sure there are no severe bug, but those small sampled are often not sufficient to make a decision based on NHST, this is due to to way the standard error is computed, it needs more data deliver some insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values; recompute to be robust\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H0 - 6 * sigma_HO\n",
    "right = mu_H0 + 6 * sigma_HO\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Right-tail probabilityy\n",
    "p = norm.sf(x0, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Critical value at significance alpha (one-sided)\n",
    "crit_x = norm.ppf(1 - alpha, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade right tail\n",
    "mask = xs >= x0\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Right tail p = {p:.4g}\")\n",
    "\n",
    "# Vertical line at observed x\n",
    "ax.axvline(x0, color=\"C1\", ls=\"--\", lw=1.5, label=f\"observed delta = {x0:.4f}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(crit_x, color=\"C2\", ls=\"-.\", lw=1.5, label=f\"critical value c = {crit_x:.4f}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H0\n",
    "ax.axvline(-epsilon, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H0 = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H0:.4f}, σ={sigma_HO:.4f}) — Right-tail beyond x\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under the null H0\")\n",
    "ax.set_ylabel(\"Probability density under H0\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.show(), display handled by notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Traditional Presentation Using *z*-Scores\n",
    "\n",
    "Another common way to compute the p-value in NHST is to **standardize** the observed statistic rather than work directly with $\\hat{\\Delta}_{\\mathrm{obs}}$.\n",
    "\n",
    "The idea:\n",
    "\n",
    "- Convert our Gaussian with mean $\\mu$ and standard deviation $\\sigma$ into a **standard normal** $N(0,1)$.\n",
    "- This is done by the familiar **$z$-score transformation**:\n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "For our non-inferiority test:\n",
    "\n",
    "$$\n",
    "Z_{\\mathrm{NI}}\n",
    "= \\frac{\\hat{\\Delta} - E[\\Delta]_{H_{\\text{boundary}}}}{SE}\n",
    "= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}\n",
    "= \\frac{\\hat{\\Delta} + \\epsilon}{SE}.\n",
    "$$\n",
    "\n",
    "Thus $Z_{\\mathrm{NI}}$ follows approximately a standard normal $N(0,1)$ under $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Tail Probability in Standard Normal Form\n",
    "\n",
    "We want the probability (under $H_0$) of observing something at least as extreme as our sample:\n",
    "\n",
    "$$\n",
    "P\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\mathrm{obs}}\\big]\n",
    "= P\\!\\left[\\frac{\\hat{\\Delta} + \\epsilon}{SE}\n",
    "       \\ge \\frac{\\hat{\\Delta}_{\\mathrm{obs}}+\\epsilon}{SE}\\right].\n",
    "$$\n",
    "\n",
    "This is simply the **right-tail** probability of a standard normal:\n",
    "\n",
    "$$\n",
    "\\int_{Z_{\\mathrm{NI}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}\\,dz.\n",
    "$$\n",
    "\n",
    "Using the standard normal survival function gives the same p-value as before — this is just the “classical” z-score framing that many NHST tutorials use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive (a.k.a. Type I Error)\n",
    "\n",
    "In this NHST setup, the **alpha**  represents the cutoff for the decision rule (lower means we reject H0, it is the condtional probability of rejecting H0 ( a \"postive\" as H0 is what we don't want to see, concluding “no unacceptable degradation”) while it is actually true so \n",
    "\n",
    "p(Reject H0 | H0 is true)\n",
    "\n",
    "The chance of **rejecting the null hypothesis** when in fact there **is** a degradation.\n",
    "\n",
    "By setting the significance level $\\alpha = 0.05$, we accept a **5% risk** of making this wrong decision. But note that this is a frequentist definition: if we ran the experiement many times,  on avergae we would incorreclty rejet 5% of the time. It does say assign a probaiblity our current decision/experiement (Bayesian method do, see below).\n",
    "\n",
    "It also says nothing about the \"effect size\" that is about how much \"non degratation/improvment\" we may have. For non inferiority it does not matter too much but for superiority we woudl want to know, also something Bayesian appraoch can do more directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### False Negative (Type II Error), Power, and Sample Size\n",
    "\n",
    "The complement of the false positive (Type I) error is the **false negative** —  \n",
    "failing to reject $H_0$ when the alternative $H_1$ is actually true.\n",
    "\n",
    "In the context of a **non-inferiority test**, a false negative means:\n",
    "\n",
    "> We fail the test (do **not** reject $H_0$) even though the new UX is truly **non-inferior** (as good or better than the old one).\n",
    "\n",
    "---\n",
    "\n",
    "##### Choosing an Effect Size Under $H_1$\n",
    "\n",
    "Just as with the Type I error calculation, we need to pick an expected value for the difference $\\Delta$ — but this time **under $H_1$**.\n",
    "\n",
    "- In practice, we must choose a **single reference value** to center the alternative distribution.  \n",
    "- A common (and pragmatic) choice is the **minimum effect size we care to detect** — often set to $E[\\Delta] = 0$ (meaning *no difference* between variant and control).  \n",
    "  - If the variant is truly “no worse” (Δ = 0), the test should reject $H_0$ most of the time.\n",
    "\n",
    "This choice is somewhat **arbitrary** and reflects a **business decision**: “How small of a difference do we consider acceptable to detect?”\n",
    "\n",
    "---\n",
    "\n",
    "##### Modeling Under $H_1$\n",
    "\n",
    "If we assume the variant is truly **no worse** (Δ = 0), we can pool samples to estimate the standard error (since under $H_1$ we’re treating them as coming from the same distribution):\n",
    "\n",
    "$$\n",
    "SE_{H_1}\n",
    "= \\text{WaldPooled SE}\n",
    "= \\sqrt{\\hat{p}_{\\mathrm{pool}}\n",
    "(1-\\hat{p}_{\\mathrm{pool}})\n",
    "\\left(\\tfrac{1}{n_C}+\\tfrac{1}{n_A}\\right)}.\n",
    "$$\n",
    "\n",
    "We then compare this **alternative distribution** (mean = 0, std = $SE_{H_1}$) to the **critical value** $c$ that was already set by the significance level $\\alpha$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Beta and Power\n",
    "\n",
    "- **$\\beta$ (Type II error)** = probability that the observed statistic falls **below the critical value** when the true mean is the one we chose under $H_1$ (e.g., Δ = 0).  \n",
    "- **Power** = $1-\\beta$ = probability of **correctly rejecting** $H_0$ when the variant is truly non-inferior.\n",
    "\n",
    "Graphically:  \n",
    "- The null distribution is centered at $-\\epsilon$ (our boundary).  \n",
    "- The alternative distribution is centered at $0$ (no degradation).  \n",
    "- $\\beta$ is the area of the alternative distribution **to the left of the critical value**.\n",
    "\n",
    "With the numerical example we picked (on purpose, small sample and very unbalanced between control and variant) the power is not good at all (see below), the convential target most test try ot achieve is 80% power, we are only at about 21%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"Observed probability of false negative a.k.a β a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Observed Power (1 - β): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H1 - 6 * sigma_H1\n",
    "right = mu_H1 + 6 * sigma_H1\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H1, scale=sigma_H1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density under H₁\")\n",
    "\n",
    "# Shade left tail (Type II error region)\n",
    "mask = xs <= critical_value\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, \n",
    "                label=f\"Type II error β = {beta:.4g}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(critical_value, color=\"C2\", ls=\"-.\", lw=1.5, \n",
    "           label=f\"critical value c = {critical_value:.4f}\")\n",
    "\n",
    "# Vertical line at observed delta\n",
    "ax.axvline(hatDelta_observed, color=\"C1\", ls=\"--\", lw=1.5, \n",
    "           label=f\"observed delta = {hatDelta_observed:.4f}\")\n",
    "\n",
    "# Vertical line at the mean under H1\n",
    "ax.axvline(mu_H1, color=\"k\", ls=\":\", lw=1.5, \n",
    "           label=f\"mean under H₁ = {mu_H1:.4f}\")\n",
    "\n",
    "# Vertical line at the H0 boundary (for reference)\n",
    "ax.axvline(-epsilon, color=\"gray\", ls=\":\", lw=1.5, alpha=0.7,\n",
    "           label=f\"H₀ boundary = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H1:.4f}, σ={sigma_H1:.4f}) — Type II Error (β) and Power\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under H₁\")\n",
    "ax.set_ylabel(\"Probability density under H₁\")\n",
    "ax.legend(loc=\"best\", fontsize=9)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "\n",
    "# Add text annotation for power\n",
    "power_text = f\"Power = 1 - β = {power:.4f}\"\n",
    "ax.text(0.98, 0.95, power_text, transform=ax.transAxes, \n",
    "        fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Designing for Target Power\n",
    "\n",
    "If we want to achieve a **target power** — commonly 80% (so $\\beta = 0.2$) —  \n",
    "we can **solve for the required sample size** (embedded in $SE$).\n",
    "\n",
    "- Larger $n$ → smaller $SE$ → distributions separate more clearly → higher power.\n",
    "- This is the usual **sample size calculation** step when planning an A/B test.\n",
    "\n",
    "In practice, one:\n",
    "1. Fixes $\\alpha$ (e.g., 0.05).\n",
    "2. Chooses the minimum effect size of interest (e.g., $\\Delta=0$ for non-inferiority).\n",
    "3. Sets desired power (e.g., 80%).\n",
    "4. Solves for $n_C$ and $n_A$ to achieve that power given the pooled variance.\n",
    "\n",
    "The code to solve to get a target Beta can be developed easily, but since we will favor Bayesia approaches whic can work with small sample we we will that TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### NHST\n",
    "\n",
    "NHST has also a notion of Confidence internval but it is not what most people think it mean, it does not say anything about the hypotheis or whether the observe value is close to the truth, of whether rejecing H0 is true or not, it is computed without using any of the hypothesis, it is just using the \"plugged in\" standard deviation SE (derived from the observation, with all the caveats we disussed) and say: if the standard deviation(standare error) was really SE, and our sample would end up in the intevera [a,b] 95% of the time. Not usueful to make any decisions diretly. Some people use it indirectly but looking for overal between the interval some key values, but that's just another way to do the p-value analysis above, or \"pickign the best variant\" below. The p-value computaition is the same computation really and the mainstrema way of doing it for NHST practitioners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST, the **Bayesian approach** is conceptually simpler:\n",
    "\n",
    "- Instead of fixing two specific expected values for $\\Delta$ (one under $H_0$ and one under $H_1$),\n",
    "- We treat the **true conversion difference** $E[\\Delta]$ itself as an **unknown random variable** and reason about its entire probability distribution.\n",
    "\n",
    "This lets us qua ntify directly how likely any value of $\\Delta$ is, given both prior knowledge and the data we observe.\n",
    "\n",
    "The typical workflow is\n",
    "\n",
    "1. Picka prior belief, express it as a Beta distribution\n",
    "2. run the the experiment\n",
    "3. use Bayes theorem to update the prior belieft into a posterior belief\n",
    "4. rinse and repeat as this is one of the strenght of the method, the posterior belief can be use into a new prior before running more experiments that will firm it up, if needed and with perfect mathematical rigor unlike NHST which does not allow peekig or stopping early because of the way sampling works.\n",
    "\n",
    "---\n",
    "\n",
    "#### Using the Beta Distribution for Our Prior Belief\n",
    "\n",
    "For experiments based on **Bernoulli trials** (success/failure, convert/abandon, etc.), the most convenient way to model our **prior belief** about a conversion rate is the **Beta distribution**.\n",
    "\n",
    "> ⚠️ Don’t confuse this **Beta** with the “$\\beta$” from NHST (Type II error).  \n",
    "> Here, *Beta* is the name of a probability distribution.\n",
    "\n",
    "The Beta distribution:\n",
    "\n",
    "- Is defined on the interval $[0,1]$, making it perfect for modeling a **probability**.\n",
    "- Has two shape parameters, $\\alpha$ and $\\beta$, which control how strongly it reflects our prior knowledge.\n",
    "\n",
    "Some examples of possible priors:\n",
    "\n",
    "- **Uninformative prior:** $ \\mathrm{Beta}(1,1) $ — essentially a uniform distribution, expressing “we know nothing.”\n",
    "- **Weakly informative prior:** centered roughly around 17–20% but without being very sure\n",
    "\n",
    "Here are a few graphical example of the Beta distribution for various values of $\\alpha$ and $\\beta$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Four cases:\n",
    "cases = [\n",
    "    (\"Uninformative (flat)\", (\"single\", (1, 1))),\n",
    "    (\"Weakly informative (centered, high entropy)\", (\"single\", (3, 12))),   # mean=0.2\n",
    "    (\"Strong conviction (centered, low entropy)\", (\"single\", (200, 800))),  # mean=0.2\n",
    "    (\"Bi-modal (mixture of Betas)\", (\"mixture\", ((5, 20, 0.5), (20, 5, 0.5))))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (title, (kind, params)) in zip(axes.ravel(), cases):\n",
    "    if kind == \"single\":\n",
    "        a, b = params\n",
    "        y = beta_dist.pdf(x, a, b)\n",
    "        mean = a / (a + b)\n",
    "        ci_low, ci_high = beta_dist.ppf([0.025, 0.975], a, b)\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2)\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        ax.axvline(mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mean={mean:.3f}\")\n",
    "        ax.axvline(ci_low, color=\"C1\", ls=\"--\", lw=1, label=\"95% CI\")\n",
    "        ax.axvline(ci_high, color=\"C1\", ls=\"--\", lw=1)\n",
    "        ax.set_title(f\"{title}\\nBeta(α={a}, β={b})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "    else:\n",
    "        # Mixture of two Betas: (a1, b1, w1), (a2, b2, w2)\n",
    "        (a1, b1, w1), (a2, b2, w2) = params\n",
    "        y = w1 * beta_dist.pdf(x, a1, b1) + w2 * beta_dist.pdf(x, a2, b2)\n",
    "        m1, m2 = a1 / (a1 + b1), a2 / (a2 + b2)\n",
    "        mix_mean = w1 * m1 + w2 * m2\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2, label=\"mixture pdf\")\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        # Component means\n",
    "        ax.axvline(m1, color=\"C2\", ls=\"--\", lw=1, label=f\"mean₁={m1:.2f}\")\n",
    "        ax.axvline(m2, color=\"C3\", ls=\"--\", lw=1, label=f\"mean₂={m2:.2f}\")\n",
    "        # Mixture mean\n",
    "        ax.axvline(mix_mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mixture mean={mix_mean:.2f}\")\n",
    "        ax.set_title(f\"{title}\\n{w1:.1f}·Beta({a1},{b1}) + {w2:.1f}·Beta({a2},{b2})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"p\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"density\")\n",
    "\n",
    "fig.suptitle(\"Four Beta Priors: flat, weakly centered, strongly centered, bi-modal\", fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal definition of the **Beta distribution** looks a bit intimidating both the nominator and denomiator are related to the probability (binomial) when the probability of having m sucess out of n trials (binomial) with where the basic event probability is set to  x. \n",
    "Intuition: $\\alpha − 1$ and $\\beta − 1$ act like prior pseudo-counts of successes m and failures m-n . After observing data, you add the real counts.\n",
    "Special case (uniform prior): Beta(1,1) ⇒ posterior Beta(m + 1, n − m + 1).\n",
    "\n",
    "$$ \n",
    "\n",
    "f(x, \\alpha, \\beta) = \\frac{x^{(\\alpha-1)}(1-x)^{\\beta - 1}}{ B(\\alpha, \\beta)}\n",
    "\n",
    "$$\n",
    "\n",
    "with B being the **Beta function B** defined as normalizing constant:\n",
    "\n",
    "$$\n",
    "\n",
    "B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1} (1-t)^{\\beta-1} dt\n",
    "\n",
    "$$\n",
    "\n",
    "There is not easy way to work with this formula \"by hand\" and that is one of the reason Bayesian approach were historically unpractical as you cannot work with those \"by hand\" like with a Gaussian function, but now that we  have stats package in python it is trivial to use, here are some example of the shapes it can take by picking different values of $\\alpha$ and $\\beta$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative Approach: Assuming We Know Nothing (Non-Informative Prior)\n",
    "\n",
    "For the **variant A**, suppose we start with a **non-informative prior** — meaning we have no knowledge about the conversion rate $p_A$.  \n",
    "We assume $p_A$ could be anywhere between $0$ and $1$ with equal probability.  \n",
    "This is modeled by the Beta distribution:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(1,1)\n",
    "$$\n",
    "\n",
    "— which is just a **uniform prior** (flat line) on $[0,1]$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Posterior After Observing Data\n",
    "\n",
    "After running the experiment with:\n",
    "\n",
    "- $n_A$ = number of trials (users shown variant A),\n",
    "- $x_A$ = number of successes (conversions),\n",
    "\n",
    "the **posterior** distribution for $p_A$ — thanks to the conjugacy of the Beta with the Bernoulli likelihood  is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(x_A+1,\\; n_A - x_A + 1).\n",
    "$$\n",
    "\n",
    "This follows directly from **Bayes’ theorem** and the properties of the Beta distribution.\n",
    "\n",
    "---\n",
    "\n",
    "##### Expected Value of the Posterior\n",
    "\n",
    "For any $\\mathrm{Beta}(\\alpha,\\beta)$ distrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credible Intervals and Visualizing Prior vs. Posterior\n",
    "\n",
    "A **credible interval** answers: *“Given the data and our prior, what range of parameter values has (say) 95% posterior probability?”*   \n",
    "\n",
    "\n",
    "Note that this is completely different than the confidence interval of NHST althoug it sounds kind of the same, it is not, this give us a direct probability of the truth of our hypothesis we can say \"we know that the true conversion rate is between a and b with x% probability. We can pick whatever x we want and we will get an interval\n",
    "\n",
    "So let's say we use **equal-tailed** credible intervals (quantiles at 2.5% and 97.5%).\n",
    "\n",
    "\n",
    "- **Prior** (uninformative): $\\mathrm{Beta}(1,1)$  \n",
    "- **Posterior** after observing $x_A$ conversions out of $n_A$:  \n",
    "  $\\alpha_A = x_A+1,\\; \\beta_A = n_A - x_A + 1$\n",
    "\n",
    "We can compute these intervals and plot how the **posterior** updates our belief compared with the **prior**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "# Ensure we use the beta distribution from scipy.stats\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know with 95% chance of being true that the true converstion rate is between 16.13% and 29.30%. Here is a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-informative prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_noninformative_pdf = beta_dist.pdf(x_range, 1, 1)  # Beta(1,1) - uniform\n",
    "posterior_noninformative_pdf = beta_dist.pdf(x_range, alpha, beta_param)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_noninformative_pdf, 'b--', lw=2, label='Prior: Beta(1, 1) - Non-informative')\n",
    "ax.plot(x_range, posterior_noninformative_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha:.1f}, {beta_param:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L) & (x_range <= p_U)\n",
    "ax.fill_between(x_range[mask], posterior_noninformative_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Non-informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our non inferiority this not quite good enough, what we really wanted is above 17% and there is this tiny sliver past it with the current setup. But assuming that \"we know nothing\" prior to the experiement is not realistic, we know we added a page with an extra click, so unless there is a serious bug the convertion should be \"around\" 17% but we want ot still be open to large deviations. So we can use a wealy informative prior:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Informative Prior Using Historical Data\n",
    "\n",
    "Instead of using a completely non-informative prior $\\mathrm{Beta}(1,1)$, we can incorporate **historical knowledge**.\n",
    "\n",
    "Suppose we know the **control group** conversion rate is about **0.20**, and we want to test for **non-inferiority** with a margin $\\epsilon = 0.03$.\n",
    "\n",
    "For the variant, we choose a prior **centered** at:\n",
    "\n",
    "$$\n",
    "0.20 - 0.03 = 0.17\n",
    "$$\n",
    "\n",
    "(the non-inferiority boundary), but we want this prior to have **high entropy** (wide uncertainty) so it does not dominate the data.  \n",
    "A **Beta** distribution with a modest $\\alpha$ and $\\beta$ can be informative about the center while still uncertain.\n",
    "\n",
    "For a $\\mathrm{Beta}(\\alpha,\\beta)$ distribution:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n",
    "$$\n",
    "\n",
    "- Smaller values of $\\alpha$ and $\\beta$ give a **wider** (more uncertain) prior.\n",
    "- Values around 2–5 keep it **unimodal** but not too tight; $\\alpha,\\beta < 1$ would make it U-shaped.\n",
    "\n",
    "Let’s pick $\\alpha = 3$ and solve for $\\beta$ so the mean is $0.17$:\n",
    "\n",
    "$$\n",
    "0.17 = \\frac{3}{3+\\beta}\n",
    "\\quad\\Longrightarrow\\quad\n",
    "\\beta = \\frac{3}{0.17} - 3 \\approx 14.65.\n",
    "$$\n",
    "\n",
    "So we use a prior $\\mathrm{Beta}(3,14.65)$ — centered at $0.17$ but still fairly wide.\n",
    "\n",
    "Here is a diagram of this prior and the posterior after after observing data $(x_A, n_A)$ for the variant,  and various credible interval, for non inferiority we don't even need the credible interval (see next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative prior parameters\n",
    "target_prior_mean = control_group_conversion_rate - epsilon  # 0.17\n",
    "alpha_prior = 3  # Small value for high entropy\n",
    "beta_prior = (alpha_prior / target_prior_mean) - alpha_prior  # Solve for beta given mean\n",
    "\n",
    "print(f\"Prior: Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "print(f\"Prior mean: {alpha_prior / (alpha_prior + beta_prior):.4f}\")\n",
    "print(f\"Prior variance: {(alpha_prior * beta_prior) / ((alpha_prior + beta_prior)**2 * (alpha_prior + beta_prior + 1)):.6f}\")\n",
    "\n",
    "# Posterior parameters after observing data\n",
    "alpha_posterior = xA_observed + alpha_prior\n",
    "beta_posterior = (nA - xA_observed) + beta_prior\n",
    "\n",
    "print(f\"\\nPosterior: Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Compute 95% credible interval\n",
    "p_L_informative = beta_dist.ppf(0.025, alpha_posterior, beta_posterior)\n",
    "p_U_informative = beta_dist.ppf(0.975, alpha_posterior, beta_posterior)\n",
    "print(f\"95% Credible Interval: [{p_L_informative:.4f}, {p_U_informative:.4f}]\")\n",
    "\n",
    "# Compute 99% credible interval\n",
    "p_L_99_informative = beta_dist.ppf(0.005, alpha_posterior, beta_posterior)\n",
    "p_U_99_informative = beta_dist.ppf(0.995, alpha_posterior, beta_posterior)\n",
    "print(f\"99% Credible Interval: [{p_L_99_informative:.4f}, {p_U_99_informative:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_pdf = beta_dist.pdf(x_range, alpha_prior, beta_prior)\n",
    "posterior_pdf = beta_dist.pdf(x_range, alpha_posterior, beta_posterior)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_pdf, 'b--', lw=2, label=f'Prior: Beta({alpha_prior:.1f}, {beta_prior:.1f})')\n",
    "ax.plot(x_range, posterior_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha_posterior:.1f}, {beta_posterior:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L_informative) & (x_range <= p_U_informative)\n",
    "ax.fill_between(x_range[mask], posterior_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probabilityy that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "prob_non_inferior = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                       alpha_posterior, beta_posterior)\n",
    "print(f\"Probability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probabilityy that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior vs Posterior with non-inferiority tail area (P(p_A > p_C - ε))\n",
    "\n",
    "# Prereqs: control_group_conversion_rate, epsilon, alpha_prior, beta_prior,\n",
    "#          alpha_posterior, beta_posterior, beta_dist, np, plt\n",
    "\n",
    "threshold = control_group_conversion_rate - epsilon\n",
    "\n",
    "# Compute probabilities (posterior and prior) for reference\n",
    "prob_non_inferior_post = beta_dist.sf(threshold, alpha_posterior, beta_posterior)\n",
    "prob_non_inferior_prior = beta_dist.sf(threshold, alpha_prior, beta_prior)\n",
    "\n",
    "# Plot range focused around the relevant region\n",
    "x_min = max(0.0, threshold - 0.12)\n",
    "x_max = min(1.0, threshold + 0.28)\n",
    "x_range = np.linspace(x_min, x_max, 1200)\n",
    "\n",
    "prior_pdf = beta_dist.pdf(x_range, alpha_prior, beta_prior)\n",
    "post_pdf  = beta_dist.pdf(x_range, alpha_posterior, beta_posterior)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Curves\n",
    "ax.plot(x_range, prior_pdf, color=\"#1f77b4\", lw=2, ls=\"--\",\n",
    "        label=f\"Prior Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "ax.plot(x_range, post_pdf, color=\"#d62728\", lw=2.5,\n",
    "        label=f\"Posterior Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "\n",
    "# Non-inferiority boundary\n",
    "ax.axvline(threshold, color=\"k\", ls=\":\", lw=1.8,\n",
    "           label=f\"Non-inferiority boundary = {threshold:.2f}\")\n",
    "\n",
    "# Shade posterior tail (non-inferiority probability)\n",
    "mask_post = x_range >= threshold\n",
    "ax.fill_between(x_range[mask_post], post_pdf[mask_post], color=\"#d62728\", alpha=0.3,\n",
    "                label=f\"Posterior tail area = {prob_non_inferior_post:.3f}\")\n",
    "\n",
    "# Optional: shade prior tail for comparison (lighter)\n",
    "mask_prior = x_range >= threshold\n",
    "ax.fill_between(x_range[mask_prior], prior_pdf[mask_prior], color=\"#1f77b4\", alpha=0.15,\n",
    "                label=f\"Prior tail area = {prob_non_inferior_prior:.3f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_xlabel(\"Conversion rate p_A\", fontsize=12)\n",
    "ax.set_ylabel(\"Density\", fontsize=12)\n",
    "ax.set_title(\"Prior vs Posterior and Non-Inferiority Probability (tail area)\", fontsize=14)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "ax.legend(loc=\"best\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Posterior P(p_A > {threshold:.2f}) = {prob_non_inferior_post:.4f} \"\n",
    "      f\"({prob_non_inferior_post*100:.2f}%)\")\n",
    "print(f\"Prior     P(p_A > {threshold:.2f}) = {prob_non_inferior_prior:.4f} \"\n",
    "      f\"({prob_non_inferior_prior*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Variant\n",
    "\n",
    "This is where the difference between **NHST** and a **Bayesian** approach becomes dramatic.  \n",
    "Let’s compare the main options.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 NHST Approaches\n",
    "\n",
    "\n",
    "#### 1. Winner-Takes-All\n",
    "- Pick the variant with the highest observed conversion rate.\n",
    "\n",
    "**Problems:**  \n",
    "- Ignores uncertainty and sampling noise.  \n",
    "- Easily picks the wrong variant when samples are small.\n",
    "\n",
    "#### 2. Pairwise *t*-Tests with Bonferroni Correction\n",
    "- Run one test for every pair (A vs B, A vs C, B vs C).  \n",
    "- Adjust the significance threshold to control false positives:  \n",
    "  $$\\alpha_\\text{corrected} = \\frac{0.05}{3} \\approx 0.0167.$$\n",
    "\n",
    "**Problems:**  \n",
    "- Multiple comparisons inflate Type I error; Bonferroni is very conservative (higher Type II error).  \n",
    "- Only gives “significant / not significant” — no direct probability of being best.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. ANOVA + Post-Hoc Tests\n",
    "- One-way ANOVA checks if *any* difference exists, then post-hoc tests (Tukey, Dunnett, etc.) try to find which.\n",
    "\n",
    "**Problems:**  \n",
    "- Still needs multiple-comparison corrections.  \n",
    "- ANOVA only says “something differs” — not which is best or by how much.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Confidence Interval Overlap\n",
    "- Compute 95% CIs for each variant and check for overlap.\n",
    "\n",
    "**Problems:**  \n",
    "- Overlapping CIs don’t mean “no difference.”  \n",
    "- Often inconclusive and gives no probability a variant is best.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟 Bayesian Approach — Probability of Being Best\n",
    "\n",
    "The Bayesian framework answers the question we actually care about:  \n",
    "> *Which variant is most likely the best?*\n",
    "\n",
    "**Method:**\n",
    "1. Compute the **posterior Beta distribution** for each variant using its prior and observed data.  \n",
    "2. Draw a large number of samples (e.g., 100k) from each posterior.  \n",
    "3. For each simulated draw, identify which variant has the highest conversion rate.  \n",
    "4. Report the probabilities:  \n",
    "   $$P(A \\text{ is best}),\\; P(B \\text{ is best}),\\; P(C \\text{ is best}), \\ldots$$\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "-  **Direct answer:** “Variant B is best with 88.8% probability.”  \n",
    "-  **Single coherent analysis:** no need for multiple-comparison corrections.  \n",
    "-  **Scales naturally:** works the same way for 3, 5, 10, or 100 variants.  \n",
    "-  **Quantifies uncertainty:** not just yes/no; can report $P(B>A)$, $P(B>A \\;\\&\\; B>C)$, etc.  \n",
    "-  **Flexible:** easily integrates prior knowledge and business context.  \n",
    "-  **Business-friendly:** simple to factor in risk, cost, and implementation difficulty.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Key point:**  \n",
    "Bayesian analysis gives a **probability each variant is best** — a direct, interpretable metric that scales cleanly and supports real-world decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three variants with similar conversion rates\n",
    "# All are non-inferior (above 17% boundary) but we want the best one\n",
    "variants = {\n",
    "    'A': {'n': 800, 'x': 168},  # 21.0% conversion rate\n",
    "    'B': {'n': 800, 'x': 172},  # 21.5% conversion rate\n",
    "    'C': {'n': 800, 'x': 165}   # 20.625% conversion rate\n",
    "}\n",
    "\n",
    "print(\"\\nVariant Data:\")\n",
    "print(\"-\" * 80)\n",
    "for name, data in variants.items():\n",
    "    observed_rate = data['x'] / data['n']\n",
    "    print(f\"Variant {name}: n={data['n']:4d}, x={data['x']:3d}, \"\n",
    "          f\"observed rate = {observed_rate:.4f} ({observed_rate*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nAll variants are above non-inferiority boundary: {control_group_conversion_rate - epsilon:.2f}\")\n",
    "print(\"But which one should we choose?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variants are above non-inferiority boundary: {control_group_conversion_rate - epsilon:.2f}\n",
    "But which one should we choose?\n",
    "In this specific case because all sample have the same size (n), we can just compare the mean but the \"clean\" way to do it is to run a Monte Carlo simulation to see which Variant would win \"in all possible universes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior distributions (using non-informative prior Beta(1,1))\n",
    "posteriors = {}\n",
    "\n",
    "print(\"\\nPosterior Distributions:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, data in variants.items():\n",
    "    # Posterior parameters (with non-informative prior Beta(1,1))\n",
    "    alpha_post = data['x'] + 1\n",
    "    beta_post = data['n'] - data['x'] + 1\n",
    "    \n",
    "    # Posterior statistics\n",
    "    posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "    posterior_var = (alpha_post * beta_post) / \\\n",
    "                    ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n",
    "    posterior_std = np.sqrt(posterior_var)\n",
    "    \n",
    "    # Credible intervals\n",
    "    ci_95_lower = beta_dist.ppf(0.025, alpha_post, beta_post)\n",
    "    ci_95_upper = beta_dist.ppf(0.975, alpha_post, beta_post)\n",
    "    \n",
    "    posteriors[name] = {\n",
    "        'alpha': alpha_post,\n",
    "        'beta': beta_post,\n",
    "        'mean': posterior_mean,\n",
    "        'std': posterior_std,\n",
    "        'ci_95': (ci_95_lower, ci_95_upper)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nVariant {name}:\")\n",
    "    print(f\"  Posterior: Beta(α={alpha_post}, β={beta_post})\")\n",
    "    print(f\"  Posterior mean: {posterior_mean:.4f}\")\n",
    "    print(f\"  Posterior std: {posterior_std:.4f}\")\n",
    "    print(f\"  95% Credible Interval: [{ci_95_lower:.4f}, {ci_95_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create visualization of the three posterior distributions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = {'A': '#1f77b4', 'B': '#ff7f0e', 'C': '#2ca02c'}\n",
    "x_range = np.linspace(0.15, 0.30, 1000)\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    alpha_p = posteriors[name]['alpha']\n",
    "    beta_p = posteriors[name]['beta']\n",
    "    mean_p = posteriors[name]['mean']\n",
    "    ci_lower, ci_upper = posteriors[name]['ci_95']\n",
    "    \n",
    "    # Plot PDF\n",
    "    pdf = beta_dist.pdf(x_range, alpha_p, beta_p)\n",
    "    ax.plot(x_range, pdf, color=colors[name], lw=2.5, \n",
    "            label=f'{name}: mean={mean_p:.4f}')\n",
    "    \n",
    "    # Mark the mean\n",
    "    ax.axvline(mean_p, color=colors[name], ls='--', lw=1, alpha=0.5)\n",
    "    \n",
    "    # Shade 95% credible interval\n",
    "    mask = (x_range >= ci_lower) & (x_range <= ci_upper)\n",
    "    ax.fill_between(x_range[mask], pdf[mask], alpha=0.2, color=colors[name])\n",
    "\n",
    "# Add non-inferiority boundary\n",
    "boundary = control_group_conversion_rate - epsilon\n",
    "ax.axvline(boundary, color='red', ls=':', lw=2, \n",
    "           label=f'Non-inferiority boundary ({boundary:.2f})')\n",
    "\n",
    "# Add control rate\n",
    "ax.axvline(control_group_conversion_rate, color='black', ls=':', lw=2,\n",
    "           label=f'Control rate ({control_group_conversion_rate:.2f})')\n",
    "\n",
    "ax.set_xlabel('Conversion Rate', fontsize=12)\n",
    "ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "ax.set_title('Posterior Distributions for Variants A, B, C', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, ls=':', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All three posterior distributions overlap significantly\")\n",
    "print(\"  This shows there's uncertainty about which is truly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation\n",
    "n_simulations = 100000\n",
    "print(f\"Running {n_simulations:,} simulations...\\n\")\n",
    "\n",
    "# Draw samples from each posterior\n",
    "samples = {}\n",
    "for name in ['A', 'B', 'C']:\n",
    "    alpha_p = posteriors[name]['alpha']\n",
    "    beta_p = posteriors[name]['beta']\n",
    "    samples[name] = beta_dist.rvs(alpha_p, beta_p, size=n_simulations)\n",
    "\n",
    "# For each simulation, determine which variant is best\n",
    "best_counts = {'A': 0, 'B': 0, 'C': 0}\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # Get the sampled values for this simulation\n",
    "    sample_values = {\n",
    "        'A': samples['A'][i],\n",
    "        'B': samples['B'][i],\n",
    "        'C': samples['C'][i]\n",
    "    }\n",
    "    \n",
    "    # Find which variant has the highest value in this simulation\n",
    "    best_variant = max(sample_values, key=lambda k: sample_values[k])\n",
    "    best_counts[best_variant] += 1\n",
    "\n",
    "# Calculate probabilities\n",
    "probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "\n",
    "print(\"RESULTS: Probability Each Variant is Best\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    prob = probabilities[name]\n",
    "    bar = '█' * int(prob * 60)\n",
    "    print(f\"P({name} is best) = {prob:.4f} ({prob*100:5.2f}%) {bar}\")\n",
    "\n",
    "# Determine the winner\n",
    "winner = max(probabilities, key=probabilities.get)\n",
    "winner_prob = probabilities[winner]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAYESIAN CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Variant {winner} is most likely the best\")\n",
    "print(f\"  Probability: {winner_prob:.4f} ({winner_prob*100:.1f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - There's a {winner_prob*100:.1f}% chance that {winner} has the highest true conversion rate\")\n",
    "print(f\"  - This accounts for uncertainty in all three estimates\")\n",
    "print(f\"  - Clear, actionable decision with quantified confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Standalone function to compute non inferiorit and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "def test_non_inferiority(n_control, x_control, variants_data, epsilon, \n",
    "                         alpha_prior=1, beta_prior=1, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Test non-inferiority of multiple variants against a control.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_control : int\n",
    "        Number of samples in control group\n",
    "    x_control : int\n",
    "        Number of successes in control group\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 1000, 'x': 200}, 'B': {'n': 1000, 'x': 215}}\n",
    "    epsilon : float\n",
    "        Non-inferiority margin (e.g., 0.03 for 3%)\n",
    "    alpha_prior : float, optional\n",
    "        Alpha parameter for Beta prior (default: 1 for uniform)\n",
    "    beta_prior : float, optional\n",
    "        Beta parameter for Beta prior (default: 1 for uniform)\n",
    "    threshold : float, optional\n",
    "        Probability threshold for declaring non-inferiority (default: 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with results for each variant containing:\n",
    "        - 'is_non_inferior': bool, whether variant is non-inferior\n",
    "        - 'probability': float, P(variant > control - epsilon)\n",
    "        - 'control_rate': float, posterior mean of control\n",
    "        - 'variant_rate': float, posterior mean of variant\n",
    "        - 'posterior_params': tuple, (alpha, beta) of variant posterior\n",
    "    \"\"\"\n",
    "    # Control posterior\n",
    "    alpha_control = x_control + alpha_prior\n",
    "    beta_control = n_control - x_control + beta_prior\n",
    "    control_rate = alpha_control / (alpha_control + beta_control)\n",
    "    \n",
    "    # Boundary for non-inferiority\n",
    "    boundary = control_rate - epsilon\n",
    "    \n",
    "    results = {}\n",
    "    n_simulations = 100000\n",
    "    \n",
    "    # Sample from control posterior once (reuse for all variants)\n",
    "    control_samples = beta_dist.rvs(alpha_control, beta_control, size=n_simulations)\n",
    "    \n",
    "    for variant_name, data in variants_data.items():\n",
    "        # Variant posterior\n",
    "        alpha_variant = data['x'] + alpha_prior\n",
    "        beta_variant = data['n'] - data['x'] + beta_prior\n",
    "        variant_rate = alpha_variant / (alpha_variant + beta_variant)\n",
    "        \n",
    "        # Sample from variant posterior\n",
    "        variant_samples = beta_dist.rvs(alpha_variant, beta_variant, size=n_simulations)\n",
    "        \n",
    "        # Compute P(variant > control - epsilon)\n",
    "        prob_non_inferior = np.mean(variant_samples > (control_samples - epsilon))\n",
    "        \n",
    "        results[variant_name] = {\n",
    "            'is_non_inferior': prob_non_inferior >= threshold,\n",
    "            'probability': prob_non_inferior,\n",
    "            'control_rate': control_rate,\n",
    "            'variant_rate': variant_rate,\n",
    "            'posterior_params': (alpha_variant, beta_variant)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def select_best_variant(variants_data, alpha_prior=1, beta_prior=1, \n",
    "                       credible_level=0.95, n_simulations=100000):\n",
    "    \"\"\"\n",
    "    Select the best variant among multiple options using Bayesian approach.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 800, 'x': 168}, 'B': {'n': 800, 'x': 172}}\n",
    "    alpha_prior : float, optional\n",
    "        Alpha parameter for Beta prior (default: 1 for uniform)\n",
    "    beta_prior : float, optional\n",
    "        Beta parameter for Beta prior (default: 1 for uniform)\n",
    "    credible_level : float, optional\n",
    "        Credible interval level (default: 0.95)\n",
    "    n_simulations : int, optional\n",
    "        Number of Monte Carlo simulations (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'best_variant': str, name of variant most likely to be best\n",
    "        - 'probabilities': dict, P(each variant is best)\n",
    "        - 'posterior_means': dict, posterior mean for each variant\n",
    "        - 'credible_intervals': dict, (lower, upper) credible interval for each variant\n",
    "        - 'expected_loss': dict, expected loss from choosing each variant\n",
    "    \"\"\"\n",
    "    variant_names = list(variants_data.keys())\n",
    "    posteriors = {}\n",
    "    samples = {}\n",
    "    \n",
    "    # Compute posteriors and draw samples\n",
    "    for name, data in variants_data.items():\n",
    "        alpha_post = data['x'] + alpha_prior\n",
    "        beta_post = data['n'] - data['x'] + beta_prior\n",
    "        \n",
    "        posteriors[name] = {\n",
    "            'alpha': alpha_post,\n",
    "            'beta': beta_post,\n",
    "            'mean': alpha_post / (alpha_post + beta_post)\n",
    "        }\n",
    "        \n",
    "        # Draw samples from posterior\n",
    "        samples[name] = beta_dist.rvs(alpha_post, beta_post, size=n_simulations)\n",
    "        \n",
    "        # Compute credible interval\n",
    "        ci_lower = beta_dist.ppf((1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        ci_upper = beta_dist.ppf(1 - (1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        posteriors[name]['credible_interval'] = (ci_lower, ci_upper)\n",
    "    \n",
    "    # Monte Carlo: count how often each variant is best\n",
    "    best_counts = {name: 0 for name in variant_names}\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Get samples for this iteration\n",
    "        sample_values = {name: samples[name][i] for name in variant_names}\n",
    "        \n",
    "        # Find best variant in this simulation\n",
    "        best_variant = max(sample_values, key=sample_values.get)\n",
    "        best_counts[best_variant] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "    \n",
    "    # Expected loss: E[max(all) - this variant]\n",
    "    expected_loss = {}\n",
    "    for name in variant_names:\n",
    "        max_samples = np.maximum.reduce([samples[v] for v in variant_names])\n",
    "        losses = max_samples - samples[name]\n",
    "        expected_loss[name] = np.mean(losses)\n",
    "    \n",
    "    # Determine best variant\n",
    "    best_variant = max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    return {\n",
    "        'best_variant': best_variant,\n",
    "        'probabilities': probabilities,\n",
    "        'posterior_means': {name: posteriors[name]['mean'] for name in variant_names},\n",
    "        'credible_intervals': {name: posteriors[name]['credible_interval'] \n",
    "                              for name in variant_names},\n",
    "        'expected_loss': expected_loss\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Non-inferiority test\n",
    "    print(\"=\"*80)\n",
    "    print(\"NON-INFERIORITY TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_control = 7000\n",
    "    x_control = 1400\n",
    "    variants = {\n",
    "        'A': {'n': 1000, 'x': 200},\n",
    "        'B': {'n': 1000, 'x': 215},\n",
    "        'C': {'n': 1000, 'x': 195}\n",
    "    }\n",
    "    \n",
    "    non_inf_results = test_non_inferiority(\n",
    "        n_control=n_control,\n",
    "        x_control=x_control,\n",
    "        variants_data=variants,\n",
    "        epsilon=0.03,\n",
    "        threshold=0.95\n",
    "    )\n",
    "    \n",
    "    for variant, result in non_inf_results.items():\n",
    "        print(f\"\\nVariant {variant}:\")\n",
    "        print(f\"  Control rate: {result['control_rate']:.4f}\")\n",
    "        print(f\"  Variant rate: {result['variant_rate']:.4f}\")\n",
    "        print(f\"  P(variant > control - 0.03): {result['probability']:.4f}\")\n",
    "        print(f\"  Non-inferior? {'YES ✓' if result['is_non_inferior'] else 'NO ✗'}\")\n",
    "    \n",
    "    # Select best variant (only among non-inferior ones)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST VARIANT SELECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter to only non-inferior variants\n",
    "    non_inferior_variants = {\n",
    "        name: data for name, data in variants.items()\n",
    "        if non_inf_results[name]['is_non_inferior']\n",
    "    }\n",
    "    \n",
    "    if non_inferior_variants:\n",
    "        best_results = select_best_variant(non_inferior_variants)\n",
    "        \n",
    "        print(f\"\\nBest variant: {best_results['best_variant']}\")\n",
    "        print(f\"\\nProbabilities of being best:\")\n",
    "        for name, prob in best_results['probabilities'].items():\n",
    "            print(f\"  {name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nPosterior means:\")\n",
    "        for name, mean in best_results['posterior_means'].items():\n",
    "            print(f\"  {name}: {mean:.4f}\")\n",
    "        \n",
    "        print(f\"\\n95% Credible Intervals:\")\n",
    "        for name, (lower, upper) in best_results['credible_intervals'].items():\n",
    "            print(f\"  {name}: [{lower:.4f}, {upper:.4f}]\")\n",
    "        \n",
    "        print(f\"\\nExpected loss (if you choose this variant):\")\n",
    "        for name, loss in best_results['expected_loss'].items():\n",
    "            print(f\"  {name}: {loss:.6f} ({loss*100:.4f}%)\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No variants are non-inferior to control!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
