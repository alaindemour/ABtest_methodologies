{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec Summary\n",
    "\n",
    "This notebook goes into a lot of detail and presents all the math to justify using a Bayesian approach, but if you want to skip the math and just get the benefits, skip to the end and use the Python functions test_non_inferiority and select_best_variant which should be sufficient to support our CX analysis with 3 variants and a control.\n",
    "\n",
    "\n",
    "## Problem\n",
    "\n",
    "When launching new web or mobile features, engineering teams face a common dilemma:\n",
    "\n",
    "- **Limited traffic allocation**: New features get only 5-10% of traffic to minimize risk\n",
    "- **Multiple variants**: Design teams often propose 3-5 different implementations\n",
    "- **Small sample sizes**: At launch because of controlled releases, each variant may only see hundreds or low thousands of users\n",
    "- **Need for speed**: We need fast decisions on which variants are best to iterate or scale\n",
    "\n",
    "**Traditional NHST fails here**: With small, unbalanced samples (e.g., 7,000 control vs. 150 per variant), statistical tests either:\n",
    "- Fail to reach significance (underpowered, β > 80%)\n",
    "- Require weeks of data collection\n",
    "- Is unwieldy to compare more than 2 variants at a time\n",
    "\n",
    "## The Bayesian Solution\n",
    "\n",
    "Bayesian methods excel precisely where NHST struggles:\n",
    "\n",
    "### 1. **Works with Small Samples**\n",
    "- Incorporates prior knowledge (historical conversion rates)\n",
    "- Updates beliefs incrementally as data arrives\n",
    "- Provides meaningful conclusions even with n=150 per variant\n",
    "- No arbitrary \"minimum sample size\" requirement\n",
    "\n",
    "### 2. **Handles Unbalanced Allocation Naturally**\n",
    "- 90% control, 10% variants? No problem.\n",
    "- Each variant can have different sample sizes\n",
    "- No need for equal allocation or \"balanced designs\"\n",
    "- Protects existing user experience while testing\n",
    "\n",
    "### 3. **Scales to Many Variants Effortlessly**\n",
    "- Compare 3, 5, 10, or 100 variants simultaneously\n",
    "- Single coherent analysis—no multiple comparison penalties\n",
    "- Direct answer: P(A is best) = 31%, P(B is best) = 47%, P(C is best) = 22%\n",
    "\n",
    "### 4. **Provides Actionable Probabilities**\n",
    "- NHST says: \"Cannot reject H₀\" (not actionable)\n",
    "- Bayesian says: \"47% chance B is best, 22% it's worse than control\" (actionable)\n",
    "- Direct business decision: Deploy B with quantified risk\n",
    "\n",
    "### 5. **Allows Continuous Monitoring**\n",
    "- Can check results anytime without \"p-hacking\" concerns\n",
    "- Update posteriors as new data arrives\n",
    "- Stop early if clear winner emerges\n",
    "- Continue if more certainty needed—mathematically rigorous\n",
    "\n",
    "\n",
    "## Key Benefits Summary\n",
    "\n",
    "| Aspect | Traditional NHST | Bayesian Approach |\n",
    "|--------|-----------------|-------------------|\n",
    "| Small samples | Underpowered, inconclusive | Works well with prior knowledge |\n",
    "| Unbalanced allocation | Loses efficiency | No problem |\n",
    "| Multiple variants | Complex corrections needed | Natural single analysis |\n",
    "| Interpretation | p-value (hard to explain) | Probability (intuitive) |\n",
    "| Decision making | Binary reject/fail | Quantified risk/confidence |\n",
    "| Continuous monitoring | Forbidden (p-hacking) | Allowed and rigorous |\n",
    "| Time to decision | Weeks (need larger n) | Days (works with small n) |\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "For modern product development with:\n",
    "- **Rapid iteration cycles**\n",
    "- **Risk-averse traffic allocation**\n",
    "- **Multiple design options**\n",
    "- **Small initial samples**\n",
    "\n",
    "**Bayesian methods provide:**\n",
    "- Faster decisions (days vs. weeks)\n",
    "- Better use of limited data\n",
    "- Clear, business-friendly outputs\n",
    "- Quantified confidence for risk management\n",
    "\n",
    "This enables product teams to launch confidently, iterate quickly, and scale successful features—all while protecting the existing user experience.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The two standalone Python functions in this notebook provide:\n",
    "1. `test_non_inferiority()` - Verify new features don't degrade experience\n",
    "2. `select_best_variant()` - Choose winning variant with probability\n",
    "\n",
    "Both work with any sample sizes and scale to any number of variants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function\n",
    "from scipy.stats import beta as beta_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth Analysis of the 2 methodologies\n",
    "\n",
    "When evaluating new user experiences (UX) — such as launching **passkeys** and measuring their impact on abandonment rates — our CX team has provided us with 3 variants $A_1$, $A_2$ and $A_3$ for the passkey creation experience. After launch we need to make sure that adding passkey creation does not degrade the success rate of our current CX significantly by comparing each variant to a control group that will remain on our legacy pages, then we have to decide which of the variants performs best.\n",
    "  \n",
    "This notebook compares two major approaches:\n",
    "\n",
    "- **Null Hypothesis Significance Testing (NHST)** — the long-standing statistical framework, widely used but often conceptually tricky and difficult to interpret in practical decision-making.\n",
    "- **Bayesian methods** — increasingly popular because they offer more flexibility, better ability to work on small samples and produce results that are often easier to interpret directly when deciding on actions (e.g., when to shift more traffic from a control to a new variant).\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "For the sake of the discussion let's assume an existing digital identity creation flow with a **completion rate of ~20%** (meaning ~80% of users abandon).  \n",
    "\n",
    "Our test design:\n",
    "\n",
    "- Keep **x%** of traffic on the current experience as the **control group** C.\n",
    "- Send the remaining traffic to one or more **variants** $A_1$, $A_2$, $A_3$.\n",
    "\n",
    "The test will be conducted in 2 steps. First we need to determine that each new experience is **no worse** than the current one, accepting small degradation as unavoidable since we are adding more pages and clicks — then after establishing we haven't degraded the experience, shifting more traffic to the variants and deciding which is the better-performing variant.\n",
    "\n",
    "The type of A/B test — where the first goal is to ensure a new design does **not degrade** the experience — is called a **non-inferiority test** (explained below).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST)\n",
    "\n",
    "At a high level, the **NHST** workflow is:\n",
    "\n",
    "1. **Assume what you *don’t* want to see** — this is the **null hypothesis**.  \n",
    "   - Example in medicine: *“the drug has no effect.”*  \n",
    "   - Example here: *“the new experience increases abandonment.”*\n",
    "2. **Run the experiment** and compute a test statistic.\n",
    "3. **Ask:** *If the null were true, how likely is it that we would observe a result at least this extreme?*  \n",
    "   - If that probability (the **p-value**) is very low — e.g., below a conventional threshold such as 5% — we **reject the null**.\n",
    "\n",
    "Two immediate caveats:\n",
    "- Rejecting the null does **not** prove the opposite is true; it only says the data would be unlikely *if* the null were correct.\n",
    "- “Unlikely enough” is arbitrary — thresholds like 5% are conventions, not laws of nature.\n",
    "\n",
    "A key point: NHST computes **$P(\\text{data} \\mid \\text{hypothesis})$**.  \n",
    "Later we’ll see that the Bayesian approach instead computes **$P(\\text{hypothesis} \\mid \\text{data})$** — a fundamentally different quantity.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Conversion as Random Variables\n",
    "\n",
    "The abandonment or conversion of a UX flow can be modeled with **Bernoulli random variables**:\n",
    "\n",
    "- $X_C$ for the control experience\n",
    "- $X_A$ for a new variant $A$\n",
    "\n",
    "A Bernoulli variable takes only two values: success/failure, convert/abandon, etc.  \n",
    "Each user who sees a page gives one draw from one of these variables.\n",
    "\n",
    "We assume both have the same codomain:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}_C = \\mathcal{X}_A = \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where **1 = convert** (user finishes the intended action, e.g., creating a passkey) and **0 = abandon**.  \n",
    "Technical failures are treated as *success* here because the user attempted the action.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Proportions\n",
    "\n",
    "NHST usually works with **sample proportions**, the average of $n$ Bernoulli draws:\n",
    "\n",
    "$$\n",
    "\\hat{p}_C = \\frac{1}{n}\\sum_{i=1}^n X_{C_i},\n",
    "\\quad\n",
    "\\hat{p}_A = \\frac{1}{n}\\sum_{i=1}^n X_{A_i}.\n",
    "$$\n",
    "\n",
    "Each $\\hat{p}$:\n",
    "\n",
    "- Is a random variable taking values $\\{0,\\tfrac1n,\\tfrac2n,\\ldots,1\\}$.\n",
    "- Is also an **estimator** of the true expected value $p = E[X]$.  \n",
    "  By the law of large numbers, $\\hat{p} \\to p$ as $n$ grows.\n",
    "\n",
    "(Statisticians use a “hat” to denote an estimator.)\n",
    "\n",
    "Formally, an estimator maps $n$ realizations of $X$ ($\\mathcal{X}^n$) to a real number:\n",
    "\n",
    "$$\n",
    "\\hat{p}: \\mathcal{X}^n \\to [0,1].\n",
    "$$\n",
    "\n",
    "Because it is the mean of $n$ Bernoulli variables, $\\hat{p}$ follows a **binomial** distribution that becomes approximately **Gaussian** when $n$ is large.\n",
    "\n",
    "---\n",
    "\n",
    "### Variance and Standard Deviation of a Sample Proportion\n",
    "\n",
    "For a single Bernoulli $X$:  \n",
    "$$\n",
    "\\mathrm{Var}(X) = p(1-p).\n",
    "$$\n",
    "\n",
    "For the sample proportion:\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\tfrac1n \\sum_{i=1}^n X_i\\right)\n",
    "= \\tfrac1{n^2} n p(1-p)\n",
    "= \\tfrac{p(1-p)}{n}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The square root of this variance is the **standard error** — a quantity we’ll use later.\n",
    "\n",
    "$$\n",
    "\\boxed{SE = SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Difference in Proportions\n",
    "\n",
    "We’ll often look at the **difference** between variant and control:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} = \\hat{p}_A - \\hat{p}_C\n",
    "$$\n",
    "\n",
    "This estimates the true difference\n",
    "\n",
    "$$\n",
    "\\Delta = p_A - p_C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis $H_0$** — the “bad” scenario we want to reject:  \n",
    "  the new UX **degrades** conversion by at least some small amount $\\epsilon$ we consider unacceptable (e.g., $3\\%$):\n",
    "\n",
    "  $$\n",
    "  H_0: E[\\Delta] \\le -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis $H_1$** — the new UX is **not worse** than control (possibly better):\n",
    "\n",
    "  $$\n",
    "  H_1: E[\\Delta] > -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Boundary Hypothesis** — used in test construction:  \n",
    "  assume the difference is exactly at the acceptable degradation limit:\n",
    "\n",
    "  $$\n",
    "  E[\\Delta] = -\\epsilon\n",
    "  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Example\n",
    "\n",
    "For a concrete example, we define the following counts and quantities for each experience:\n",
    "\n",
    "- $n_C$ — number of visitors in the **control** group  \n",
    "- $x_C$ — number of **conversions** observed in the control group\n",
    "\n",
    "- $n_A$ — number of visitors in the **variant** group  \n",
    "- $x_A$ — number of **conversions** observed in the variant group\n",
    "\n",
    "- $\\hat{\\Delta}_{\\mathrm{obs}}$ — the **observed difference** in conversion proportions between variant and control\n",
    "\n",
    "- $\\epsilon$ — the **acceptable degradation margin**, i.e., the smallest decrease in conversion we are willing to tolerate for the new variant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_group_conversion_rate = 0.2 # based on historical data\n",
    "nC = 7000\n",
    "xC_observed = nC * control_group_conversion_rate\n",
    "nA = 150\n",
    "xA_observed = 33\n",
    "epsilon = 0.03\n",
    "alpha = 0.05\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed  / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Realization of difference in conversion rate estimator: {hatDelta_observed:.4f}\")\n",
    "print(f\"Control group realization of conversion rate estimator: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group realization of conversion rate estimator: {hatpA_observed:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the Estimator $\\hat{\\Delta}$ (a.k.a. Standard Error in Frequentist Statistics)\n",
    "\n",
    "In NHST, the first step is to **estimate the standard deviation** of the estimator $\\hat{\\Delta}$ (often called the **standard error**, SE).  \n",
    "We then compare the observed difference in proportions from the experiment to this estimated variability to decide whether the observed effect is “far enough” from what we would expect under the null hypothesis $H_0$.\n",
    "\n",
    "This is a key pain point for NHST:\n",
    "\n",
    "- We **do not know** the true standard deviation — it depends on the unknown underlying conversion probabilities.\n",
    "- Frequentist methods therefore use the **plug-in principle**: estimate the unknown variance by “plugging in” the sample estimates (the data you just observed).\n",
    "\n",
    "But note the circularity:\n",
    "\n",
    "1. We want to know if the data are unusual under $H_0$.\n",
    "2. To measure “unusual,” we need the standard error assuming $H_0$.\n",
    "3. SE depends on the unknown true rates, so we **plug in** $\\hat{p}$ (from the data!).\n",
    "4. We then use this data-derived SE to judge whether the data are unusual.\n",
    "\n",
    "It’s like saying: *“Use my one measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising.”*\n",
    "\n",
    "Frequentists accept this because:\n",
    "\n",
    "- **Long-run frequency view:** if we repeated the procedure many times, it would have correct average properties.\n",
    "- **Pragmatism:** it’s computable from one experiment.\n",
    "- **Simulation evidence:** works “reasonably well” for moderate $n$, though “reasonably well” is debatable (see the *replication crisis*).\n",
    "- **Framework limitation:** in classical stats, parameters are fixed unknowns, so no natural way to treat them as random.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Plug-In Approaches (a.k.a. “Standard Error Hacks”)\n",
    "\n",
    "#### 1. Wald **Pooled** Standard Error (for a “No Effect” Hypothesis)\n",
    "\n",
    "If an hypothesis is **no effect** ($p_A = p_C$), we can pool data from control and variant, because under the hypothesis they are assumed to come from the same distribution.\n",
    "\n",
    "Realizations of sample proportions are:\n",
    "\n",
    "$$\n",
    "\\hat{p}_A = \\frac{x_A}{n_A}, \\qquad\n",
    "\\hat{p}_C = \\frac{x_C}{n_C}.\n",
    "$$\n",
    "\n",
    "If \"no effect\" as true (no difference), a pooled estimator is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{pool}} = \\frac{x_A + x_C}{n_A + n_C}.\n",
    "$$\n",
    "\n",
    "Variance of the difference between two independent proportions is the sum of their variances:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\mathrm{Var}(\\hat{p}_A) + \\mathrm{Var}(\\hat{p}_C).\n",
    "$$\n",
    "\n",
    "Plugging in the pooled estimate:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right).\n",
    "$$\n",
    "\n",
    "So the **pooled standard error** is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{WaldPooled SE} =\n",
    "\\sqrt{\\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right)} }\n",
    "$$\n",
    "\n",
    "This is the classic **standard error of the difference between two independent proportions**.\n",
    "\n",
    "Once SE is computed, we calculate a **z-score** (number of SEs the observed difference is away from the null value 0):\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p}_A - \\hat{p}_C}{\\text{SE}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wald **Unpooled** Standard Error (for Non-Inferiority)\n",
    "\n",
    "If the null is **non-inferiority** (allowing a margin $\\epsilon$), we **cannot** assume $p_A = p_C$, so we don’t pool.\n",
    "\n",
    "Instead we sum the individual variances (using the plug-in trick separately for each group):\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{WaldUnpooled SE}} =\n",
    "\\sqrt{\\frac{\\hat{p}_A(1-\\hat{p}_A)}{n_A} +\n",
    "      \\frac{\\hat{p}_C(1-\\hat{p}_C)}{n_C}}.\n",
    "$$\n",
    "\n",
    "Ideally, the true $p_A$ and $p_C$ should be used here, but we don’t know them — so we substitute $\\hat{p}_A$ and $\\hat{p}_C$.  \n",
    "This works but can be inaccurate if sample sizes are small or rates are extreme.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Newcombe / Score-Based (Wilson)\n",
    "\n",
    "- **Better coverage** than Wald, especially with imbalanced sample sizes or very high/low $p$.\n",
    "- Still closed-form and relatively easy to compute.\n",
    "- Still uses plug-in estimates and suffers from the same “single-sample” limitation.\n",
    "\n",
    "*(Not detailed here — but recommended over Wald when possible.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Miettinen–Nurminen\n",
    "\n",
    "- Widely used in **clinical trials** and regulated industries (e.g., FDA guidance).\n",
    "- Provides improved accuracy for non-inferiority tests.\n",
    "- However: mathematically complex, still plug-in based, and still fundamentally relies on one sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Frequentist methods **must estimate variance from the data itself** — leading to circularity and potential miscalibration, especially for small samples or edge cases.  \n",
    "Later we’ll see how Bayesian methods avoid this by modeling uncertainty about the true conversion rates directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Positive, *p*-value, Significance Level $\\alpha$ (sometimes called “confidence”), and Critical Value\n",
    "\n",
    "Once we have an estimate of the standard error $SE$ (the standard deviation of $\\hat{\\Delta}$), NHST assumes a **sampling distribution** for the estimator under the null hypothesis $H_0$.\n",
    "\n",
    "The idea is:\n",
    "\n",
    "- If we know (or assume) the **mean** and **standard deviation** of $\\hat{\\Delta}$ under $H_0$,  \n",
    "- we can model it with a known probability distribution and calculate how likely any observed result is.\n",
    "\n",
    "Although the true process is discrete (binomial), in practice we often approximate it by a **normal (Gaussian)** distribution. This is mathematically simpler and is a good approximation for moderate or large sample sizes.\n",
    "\n",
    "---\n",
    "\n",
    "##### Using the “Boundary” Case\n",
    "\n",
    "The null hypothesis  for \"non inferiority\" is technically an inequality:\n",
    "\n",
    "$$\n",
    "H_0: E[\\Delta] \\le -\\epsilon.\n",
    "$$\n",
    "\n",
    "But to get a single distribution to work with, we typically use the **boundary value**:\n",
    "\n",
    "$$\n",
    "E[\\Delta] = -\\epsilon.\n",
    "$$\n",
    "\n",
    "Why?  \n",
    "- The boundary is the **least favorable** case for rejecting $H_0$.  \n",
    "- If we can reject $H_0$ at $E[\\Delta] = -\\epsilon$, then we would also reject any stronger null (mean further left).  \n",
    "- This makes the test conservative.\n",
    "\n",
    "So under $H_0$, we model $\\hat{\\Delta}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = -\\epsilon, \\qquad \\sigma = SE.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Computing the *p*-Value\n",
    "\n",
    "The **p-value** is the probability (under $H_0$) of observing a result **as extreme or more extreme** than what we got, in the direction of the alternative $H_1$.\n",
    "\n",
    "In this one-sided non-inferiority test, that means the right tail probability:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P_{H_0}\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\text{obs}}\\big]\n",
    "= \\int_{\\hat{\\Delta}_{\\text{obs}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n",
    "\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,dx.\n",
    "$$\n",
    "\n",
    "This tail integral is the **survival function** of the normal distribution.\n",
    "\n",
    "Using the standard normal CDF $\\Phi$:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = 1 - \\Phi\\!\\left(\\frac{\\hat{\\Delta}_{\\text{obs}}-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Significance Level $\\alpha$ and Critical Value\n",
    "\n",
    "- We choose a **significance level** $\\alpha$ (often $0.05$).  \n",
    "- If $p\\text{-value} \\le \\alpha$, we reject $H_0$ — our result is unlikely under the null.\n",
    "\n",
    "The **critical value** $c$ is the smallest observed difference that would lead to rejection at level $\\alpha$. It is obtained by inverting the right-tail probability:\n",
    "\n",
    "$$\n",
    "c = \\mu + \\sigma \\,\\Phi^{-1}(1 - \\alpha).\n",
    "$$\n",
    "\n",
    "Any observed $\\hat{\\Delta}_{\\text{obs}} \\ge c$ yields $p\\text{-value} \\le \\alpha$ and thus rejects $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "Below we’ll compute the p-value and critical value explicitly and visualize how the right tail behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided) for an observed proportion at {hatDelta_observed:.4f}: {p_value:.4f}')\n",
    "\n",
    "# inverse survival function to find critical value for given p-value\n",
    "critical_value = norm.isf(alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for our alpha cutoff value at {alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the value we chose as an example for the sample, the p-value is a bit more than 7%, so that's not unlikely enough if we picked 5% as the alpha cutoff point. So we would \"fail to reject\", meaning we can't say much, we don't know whether we can claim that the new CX does not cause some unwanted degradation.\n",
    "\n",
    "Note that this is because the sample is tiny. We picked it small on purpose as this is a realistic situation when launching a new feature; for various reasons including unresolved bugs you usually just put a tiny bit of traffic on the new feature. But those small samples are often not sufficient to make a decision based on NHST. This is due to the way the standard error is computed; it needs more data to deliver some insights. In this example we would need our observed proportion to get to 0.026 or better to reject the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values; recompute to be robust\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H0 - 6 * sigma_HO\n",
    "right = mu_H0 + 6 * sigma_HO\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Right-tail probability\n",
    "p = norm.sf(x0, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Critical value at significance alpha (one-sided)\n",
    "crit_x = norm.ppf(1 - alpha, loc=mu_H0, scale=sigma_HO)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density\")\n",
    "\n",
    "# Shade right tail\n",
    "mask = xs >= x0\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, label=f\"Right tail p = {p:.4g}\")\n",
    "\n",
    "# Vertical line at observed x\n",
    "ax.axvline(x0, color=\"C1\", ls=\"--\", lw=1.5, label=f\"observed delta = {x0:.4f}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(crit_x, color=\"C2\", ls=\"-.\", lw=1.5, label=f\"critical value c = {crit_x:.4f}\")\n",
    "\n",
    "# Vertical line at the mean for the null hypothesis H0\n",
    "ax.axvline(-epsilon, color=\"k\", ls=\":\", lw=1.5, label=f\"mean under H0 = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H0:.4f}, σ={sigma_HO:.4f}) — Right-tail beyond x\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under the null H0\")\n",
    "ax.set_ylabel(\"Probability density under H0\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "# plt.show(), display handled by notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Traditional Presentation Using *z*-Scores\n",
    "\n",
    "Another common way to compute the p-value in NHST is to **standardize** the observed statistic rather than work directly with $\\hat{\\Delta}_{\\mathrm{obs}}$.\n",
    "\n",
    "The idea:\n",
    "\n",
    "- Convert our Gaussian with mean $\\mu$ and standard deviation $\\sigma$ into a **standard normal** $N(0,1)$.\n",
    "- This is done by the familiar **$z$-score transformation**:\n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "For our non-inferiority test:\n",
    "\n",
    "$$\n",
    "Z_{\\mathrm{NI}}\n",
    "= \\frac{\\hat{\\Delta} - E[\\Delta]_{H_{\\text{boundary}}}}{SE}\n",
    "= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}\n",
    "= \\frac{\\hat{\\Delta} + \\epsilon}{SE}.\n",
    "$$\n",
    "\n",
    "Thus $Z_{\\mathrm{NI}}$ follows approximately a standard normal $N(0,1)$ under $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Tail Probability in Standard Normal Form\n",
    "\n",
    "We want the probability (under $H_0$) of observing something at least as extreme as our sample:\n",
    "\n",
    "$$\n",
    "P\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\mathrm{obs}}\\big]\n",
    "= P\\!\\left[\\frac{\\hat{\\Delta} + \\epsilon}{SE}\n",
    "       \\ge \\frac{\\hat{\\Delta}_{\\mathrm{obs}}+\\epsilon}{SE}\\right].\n",
    "$$\n",
    "\n",
    "This is simply the **right-tail** probability of a standard normal:\n",
    "\n",
    "$$\n",
    "\\int_{Z_{\\mathrm{NI}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}\\,dz.\n",
    "$$\n",
    "\n",
    "Using the standard normal survival function gives the same p-value as before — this is just the “classical” z-score framing that many NHST tutorials use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is hte same pvalue, just a different way to compute the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive (a.k.a. Type I Error)\n",
    "\n",
    "In this NHST setup, the **alpha**  represents the cutoff for the decision rule (lower means we reject H0, it is the condtional probability of rejecting H0 while it is actually true. In this setup the rejection is considered a \"postive\" as H0 is \"what we don't want to see\", so that is concluding “no unacceptable degradation” while there is actually a nasty one.\n",
    "\n",
    "a more formal way of formualting the pvalue is p(Reject H0 | H0 is true)\n",
    "\n",
    "\n",
    "By setting the significance level $\\alpha = 0.05$, we accept a **5% risk** of making this wrong decision when there is a degration. But note that this is a frequentist definition: if we ran the experiment many times,  on average we would incorreclty reject 5% of the time in this case. However tt does assign any actual probablity to our current decision/experiment (Bayesian method do, see below).\n",
    "\n",
    "It also says nothing about the \"effect size\", that is about how much \"non-degradation/improvement\" we may have. For non-inferiority it does not matter too much but for superiority we would want to know, also something Bayesian approach can do more directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### False Negative (Type II Error), Power, and Sample Size\n",
    "\n",
    "The **false negative** — is failing to reject $H_0$ when the alternative $H_1$ is actually true. \n",
    "\n",
    "In the context of a **non-inferiority test**, a false negative means:\n",
    "\n",
    "> We fail the test (do **not** reject $H_0$) even though the new UX is truly **non-inferior** (as good or better than the old one). Usually it means we woud need to keep on gathering data until the test has more power to detect something (see below)\n",
    "\n",
    "---\n",
    "\n",
    "##### Choosing an Effect Size Under $H_1$\n",
    "\n",
    "Just as with the Type I error calculation, we need to pick an expected value for the difference $\\Delta$ — but this time **under $H_1$**.\n",
    "\n",
    "- In practice, we must choose a **single reference value** to center the alternative distribution.  \n",
    "- A common (and pragmatic) choice is the **minimum effect size we care to detect** — often set to $E[\\Delta] = 0$ (meaning *no difference* between variant and control).  \n",
    "  - If the variant is truly “no worse” (Δ = 0), the test should reject $H_0$ most of the time.\n",
    "\n",
    "This choice is somewhat **arbitrary** and reflects a **business decision**: “How small of a difference do we consider acceptable to detect?”\n",
    "\n",
    "---\n",
    "\n",
    "##### Modeling Under $H_1$\n",
    "\n",
    "If we assume the variant is truly **no worse** (Δ = 0), we can pool samples to estimate the standard error (since under $H_1$ we’re treating them as coming from the same distribution):\n",
    "\n",
    "$$\n",
    "SE_{H_1}\n",
    "= \\text{WaldPooled SE}\n",
    "= \\sqrt{\\hat{p}_{\\mathrm{pool}}\n",
    "(1-\\hat{p}_{\\mathrm{pool}})\n",
    "\\left(\\tfrac{1}{n_C}+\\tfrac{1}{n_A}\\right)}.\n",
    "$$\n",
    "\n",
    "We then compare this **alternative distribution** (mean = 0, std = $SE_{H_1}$) to the **critical value** $c$ that was already set by the significance level $\\alpha$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Beta and Power\n",
    "\n",
    "- **$\\beta$ (Type II error)** = probability that the observed statistic falls **below the critical value** when the true mean is the one we chose under $H_1$ (e.g., Δ = 0).  \n",
    "- **Power** = $1-\\beta$ = probability of **correctly rejecting** $H_0$ when the variant is truly non-inferior. Another way to say this is: knowing the property we care about is really there (non-inferiority), what is our probability of detecting it. In machine learning and search queries analysis, Power is also called \"recall\", that is if it is really there can we predict or find it.\n",
    "\n",
    "Graphically:  \n",
    "- The null distribution is centered at $-\\epsilon$ (our boundary).  \n",
    "- The alternative distribution is centered at $0$ (no degradation).  \n",
    "- $\\beta$ is the area of the alternative distribution **to the left of the critical value**.\n",
    "\n",
    "With the numerical example we picked (on purpose, small sample and very unbalanced between control and variant) the power is not good at all (see below). The conventional target most tests try to achieve is 80% power; we are only at about 21%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"Observed probability of false negative a.k.a β a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Observed Power (1 - β): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain for plotting (±6σ around the mean, clipped to reasonable bounds)\n",
    "left = mu_H1 - 6 * sigma_H1\n",
    "right = mu_H1 + 6 * sigma_H1\n",
    "xs = np.linspace(left, right, 1000)\n",
    "pdf = norm.pdf(xs, loc=mu_H1, scale=sigma_H1)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7.5, 4.5))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\n",
    "ax.plot(xs, pdf, color=\"C0\", lw=2, label=\"Density under H₁\")\n",
    "\n",
    "# Shade left tail (Type II error region)\n",
    "mask = xs <= critical_value\n",
    "ax.fill_between(xs[mask], pdf[mask], color=\"C1\", alpha=0.35, \n",
    "                label=f\"Type II error β = {beta:.4g}\")\n",
    "\n",
    "# Vertical line at critical value\n",
    "ax.axvline(critical_value, color=\"C2\", ls=\"-.\", lw=1.5, \n",
    "           label=f\"critical value c = {critical_value:.4f}\")\n",
    "\n",
    "# Vertical line at observed delta\n",
    "ax.axvline(hatDelta_observed, color=\"C1\", ls=\"--\", lw=1.5, \n",
    "           label=f\"observed delta = {hatDelta_observed:.4f}\")\n",
    "\n",
    "# Vertical line at the mean under H1\n",
    "ax.axvline(mu_H1, color=\"k\", ls=\":\", lw=1.5, \n",
    "           label=f\"mean under H₁ = {mu_H1:.4f}\")\n",
    "\n",
    "# Vertical line at the H0 boundary (for reference)\n",
    "ax.axvline(-epsilon, color=\"gray\", ls=\":\", lw=1.5, alpha=0.7,\n",
    "           label=f\"H₀ boundary = {-epsilon:.4f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_title(f\"Normal(μ={mu_H1:.4f}, σ={sigma_H1:.4f}) — Type II Error (β) and Power\")\n",
    "ax.set_xlabel(\"Δ (difference in proportions) under H₁\")\n",
    "ax.set_ylabel(\"Probability density under H₁\")\n",
    "ax.legend(loc=\"best\", fontsize=9)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "\n",
    "# Add text annotation for power\n",
    "power_text = f\"Power = 1 - β = {power:.4f}\"\n",
    "ax.text(0.98, 0.95, power_text, transform=ax.transAxes, \n",
    "        fontsize=10, verticalalignment='top', horizontalalignment='right',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Designing for Target Power\n",
    "\n",
    "If we want to achieve a **target power** — commonly 80% (so $\\beta = 0.2$) —  \n",
    "we can **solve for the required sample size** (embedded in $SE$).\n",
    "\n",
    "- Larger $n$ → smaller $SE$ → distributions separate more clearly → higher power.\n",
    "- This is the usual **sample size calculation** step when planning an A/B test.\n",
    "\n",
    "In practice, one:\n",
    "1. Fixes $\\alpha$ (e.g., 0.05).\n",
    "2. Chooses the minimum effect size of interest (e.g., $\\Delta=0$ for non-inferiority).\n",
    "3. Sets desired power (e.g., 80%).\n",
    "4. Solves for $n_C$ and $n_A$ to achieve that power given the pooled variance.\n",
    "\n",
    "The code to solve to get a target beta can be developed easily, but since we will favor Bayesian approaches which can work with small samples, we will leave that TBD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### NHST Confidence Interval (CI)\n",
    "\n",
    "NHST has also a notion of Confidence Interval but it is not what most people think it means. It does not say anything about the hypothesis or whether the observed value is close to the truth, or whether rejecting H0 is true or not. It is computed without using any of the hypotheses; it is just using the \"plugged in\" standard deviation SE (derived from the observation, with all the caveats we discussed) and says: if the standard deviation (standard error) was really SE, our sample would end up in the interval [a,b] 95% of the time. Not useful to make any decisions directly. Some people use it indirectly by looking for overlaps between the interval and some key values, but that's just another way to do the p-value analysis as above, or \"picking the best variant\" below. The p-value computation is the same computation really and the mainstream way of doing it for NHST practitioners.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST, the **Bayesian approach** is conceptually simpler:\n",
    "\n",
    "- Instead of fixing two specific expected values for $\\Delta$ (one under $H_0$ and one under $H_1$),\n",
    "- We treat the **true conversion difference** $E[\\Delta]$ itself as an **unknown random variable** and reason about its entire probability distribution.\n",
    "\n",
    "This lets us quantify directly how likely any value of $\\Delta$ is, given both prior knowledge and the data we observe.\n",
    "\n",
    "The typical workflow is\n",
    "\n",
    "1. Pick a prior belief, express it as a Beta distribution\n",
    "2. Run the experiment\n",
    "3. Use Bayes theorem to update the prior belief into a posterior belief\n",
    "4. Rinse and repeat as this is one of the strengths of the method; the posterior belief can be used as a new prior before running more experiments that will firm it up, if needed and with perfect mathematical rigor unlike NHST which does not allow peeking or stopping early because of the way sampling works.\n",
    "\n",
    "---\n",
    "\n",
    "#### Using the Beta Distribution for Our Prior Belief\n",
    "\n",
    "For experiments based on **Bernoulli trials** (success/failure, convert/abandon, etc.), the most convenient way to model our **prior belief** about a conversion rate is the **Beta distribution**.\n",
    "\n",
    "> ⚠️ Don’t confuse this **Beta** with the “$\\beta$” from NHST (Type II error).  \n",
    "> Here, *Beta* is the name of a probability distribution.\n",
    "\n",
    "The Beta distribution:\n",
    "\n",
    "- Is defined on the interval $[0,1]$, making it perfect for modeling a **probability**.\n",
    "- Has two shape parameters, $\\alpha$ and $\\beta$, which control how strongly it reflects our prior knowledge.\n",
    "\n",
    "Some examples of possible priors:\n",
    "\n",
    "- **Uninformative prior:** $ \\mathrm{Beta}(1,1) $ — essentially a uniform distribution, expressing “we know nothing.”\n",
    "- **Weakly informative prior:** centered roughly around 17–20% but without being very sure\n",
    "\n",
    "Here are a few graphical example of the Beta distribution for various values of $\\alpha$ and $\\beta$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 1, 1000)\n",
    "\n",
    "# Four cases:\n",
    "cases = [\n",
    "    (\"Uninformative (flat)\", (\"single\", (1, 1))),\n",
    "    (\"Weakly informative (centered, high entropy)\", (\"single\", (3, 12))),   # mean=0.2\n",
    "    (\"Strong conviction (centered, low entropy)\", (\"single\", (200, 800))),  # mean=0.2\n",
    "    (\"Bi-modal (mixture of Betas)\", (\"mixture\", ((5, 20, 0.5), (20, 5, 0.5))))\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True, sharey=True)\n",
    "\n",
    "for ax, (title, (kind, params)) in zip(axes.ravel(), cases):\n",
    "    if kind == \"single\":\n",
    "        a, b = params\n",
    "        y = beta_dist.pdf(x, a, b)\n",
    "        mean = a / (a + b)\n",
    "        ci_low, ci_high = beta_dist.ppf([0.025, 0.975], a, b)\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2)\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        ax.axvline(mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mean={mean:.3f}\")\n",
    "        ax.axvline(ci_low, color=\"C1\", ls=\"--\", lw=1, label=\"95% CI\")\n",
    "        ax.axvline(ci_high, color=\"C1\", ls=\"--\", lw=1)\n",
    "        ax.set_title(f\"{title}\\nBeta(α={a}, β={b})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "    else:\n",
    "        # Mixture of two Betas: (a1, b1, w1), (a2, b2, w2)\n",
    "        (a1, b1, w1), (a2, b2, w2) = params\n",
    "        y = w1 * beta_dist.pdf(x, a1, b1) + w2 * beta_dist.pdf(x, a2, b2)\n",
    "        m1, m2 = a1 / (a1 + b1), a2 / (a2 + b2)\n",
    "        mix_mean = w1 * m1 + w2 * m2\n",
    "\n",
    "        ax.plot(x, y, color=\"C0\", lw=2, label=\"mixture pdf\")\n",
    "        ax.fill_between(x, y, color=\"C0\", alpha=0.12)\n",
    "        # Component means\n",
    "        ax.axvline(m1, color=\"C2\", ls=\"--\", lw=1, label=f\"mean₁={m1:.2f}\")\n",
    "        ax.axvline(m2, color=\"C3\", ls=\"--\", lw=1, label=f\"mean₂={m2:.2f}\")\n",
    "        # Mixture mean\n",
    "        ax.axvline(mix_mean, color=\"k\", ls=\":\", lw=1.2, label=f\"mixture mean={mix_mean:.2f}\")\n",
    "        ax.set_title(f\"{title}\\n{w1:.1f}·Beta({a1},{b1}) + {w2:.1f}·Beta({a2},{b2})\", fontsize=10)\n",
    "        ax.legend(fontsize=8, loc=\"best\")\n",
    "\n",
    "for ax in axes[-1]:\n",
    "    ax.set_xlabel(\"p\")\n",
    "for ax in axes[:, 0]:\n",
    "    ax.set_ylabel(\"density\")\n",
    "\n",
    "fig.suptitle(\"Four Beta Priors: flat, weakly centered, strongly centered, bi-modal\", fontsize=12)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal definition of the **Beta distribution** looks a bit intimidating; both the numerator and denominator are related to the probability (binomial) when the probability of having m successes out of n trials (binomial) where the basic event probability is set to x.\n",
    "Intuition: $\\alpha - 1$ and $\\beta - 1$ act like prior pseudo-counts of successes m and failures n-m. After observing data, you add the real counts.\n",
    "Special case (uniform prior): Beta(1,1) ⇒ posterior Beta(m + 1, n - m + 1).\n",
    "\n",
    "$$ \n",
    "\n",
    "f(x, \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "\n",
    "$$\n",
    "\n",
    "with B being the **Beta function B** defined as normalizing constant:\n",
    "\n",
    "$$\n",
    "\n",
    "B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1} (1-t)^{\\beta-1} dt\n",
    "\n",
    "$$\n",
    "\n",
    "There is no easy way to work with this formula \"by hand\" and that is one of the reasons Bayesian approaches were historically impractical as you cannot work with those \"by hand\" like with a Gaussian function. But now that we have stats packages in Python it is trivial to use. Here are some examples of the shapes it can take by picking different values of $\\alpha$ and $\\beta$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative Approach: Assuming We Know Nothing (Non-Informative Prior)\n",
    "\n",
    "For the **variant A**, suppose we start with a **non-informative prior** — meaning we have no knowledge about the conversion rate $p_A$.  \n",
    "We assume $p_A$ could be anywhere between $0$ and $1$ with equal probability.  \n",
    "This is modeled by the Beta distribution:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(1,1)\n",
    "$$\n",
    "\n",
    "— which is just a **uniform prior** (flat line) on $[0,1]$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Posterior After Observing Data\n",
    "\n",
    "After running the experiment with:\n",
    "\n",
    "- $n_A$ = number of trials (users shown variant A),\n",
    "- $x_A$ = number of successes (conversions),\n",
    "\n",
    "the **posterior** distribution for $p_A$ — thanks to the conjugacy of the Beta with the Bernoulli likelihood  is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(x_A+1,\\; n_A - x_A + 1).\n",
    "$$\n",
    "\n",
    "This follows directly from **Bayes’ theorem** and the properties of the Beta distribution.\n",
    "\n",
    "---\n",
    "\n",
    "##### Expected Value of the Posterior\n",
    "\n",
    "For any $\\mathrm{Beta}(\\alpha,\\beta)$ distrib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credible Intervals and Visualizing Prior vs. Posterior\n",
    "\n",
    "A **credible interval** answers: *“Given the data and our prior, what range of parameter values has (say) 95% posterior probability?”*   \n",
    "\n",
    "\n",
    "Note that this is completely different than the confidence interval of NHST althoug it sounds kind of the same, it is not, this give us a direct probability of the truth of our hypothesis we can say \"we know that the true conversion rate is between a and b with x% probability. We can pick whatever x we want and we will get an interval\n",
    "\n",
    "So let's say we use **equal-tailed** credible intervals (quantiles at 2.5% and 97.5%).\n",
    "\n",
    "\n",
    "- **Prior** (uninformative): $\\mathrm{Beta}(1,1)$  \n",
    "- **Posterior** after observing $x_A$ conversions out of $n_A$:  \n",
    "  $\\alpha_A = x_A+1,\\; \\beta_A = n_A - x_A + 1$\n",
    "\n",
    "We can compute these intervals and plot how the **posterior** updates our belief compared with the **prior**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "# Ensure we use the beta distribution from scipy.stats\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know with 95% chance of being true that the true converstion rate is between 16.13% and 29.30%. Here is a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-informative prior vs posterior\n",
    "x_range = np.linspace(0, 0.5, 1000)\n",
    "prior_noninformative_pdf = beta_dist.pdf(x_range, 1, 1)  # Beta(1,1) - uniform\n",
    "posterior_noninformative_pdf = beta_dist.pdf(x_range, alpha, beta_param)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(x_range, prior_noninformative_pdf, 'b--', lw=2, label='Prior: Beta(1, 1) - Non-informative')\n",
    "ax.plot(x_range, posterior_noninformative_pdf, 'r-', lw=2, label=f'Posterior: Beta({alpha:.1f}, {beta_param:.1f})')\n",
    "\n",
    "# Mark the non-inferiority boundary\n",
    "ax.axvline(control_group_conversion_rate - epsilon, color='k', ls=':', lw=1.5, \n",
    "           label=f'Non-inferiority boundary = {control_group_conversion_rate - epsilon:.2f}')\n",
    "\n",
    "# Mark the control conversion rate\n",
    "ax.axvline(control_group_conversion_rate, color='g', ls=':', lw=1.5, \n",
    "           label=f'Control conversion rate = {control_group_conversion_rate:.2f}')\n",
    "\n",
    "# Shade the 95% credible interval\n",
    "mask = (x_range >= p_L) & (x_range <= p_U)\n",
    "ax.fill_between(x_range[mask], posterior_noninformative_pdf[mask], alpha=0.3, color='red', \n",
    "                label='95% Credible Interval')\n",
    "\n",
    "ax.set_xlabel('Conversion rate p_A', fontsize=12)\n",
    "ax.set_ylabel('Probability density', fontsize=12)\n",
    "ax.set_title('Non-informative Prior vs Posterior Distribution', fontsize=14)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, ls=':', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first version we assumed that \"we know nothing\" prior to the experiment, which is not realistic. We know we added a page with an extra click, so unless there is a serious bug the conversion should be \"around\" the control which has a historical mean of 20%, but we want to be open to the possibility of small deviation around this historical value. We can do this with what is known as a weakly informative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Informative Prior Using Historical Data\n",
    "\n",
    "Instead of using a completely non-informative prior $\\mathrm{Beta}(1,1)$, we can incorporate **historical knowledge**.\n",
    "\n",
    "Suppose we know the **control group** conversion rate is about **0.20**, and we want to test for **non-inferiority** with a margin $\\epsilon = 0.03$.\n",
    "\n",
    "For the variant, we choose a prior **centered** at the historical mean 0.20 but we want this prior to have **high entropy** (wide uncertainty) so it does not dominate the data.  \n",
    "\n",
    "A **Beta** distribution with a modest $\\alpha$ and $\\beta$ can be informative about the center while still uncertain.\n",
    "\n",
    "For a $\\mathrm{Beta}(\\alpha,\\beta)$ distribution:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n",
    "$$\n",
    "\n",
    "- Smaller values of $\\alpha$ and $\\beta$ give a **wider** (more uncertain) prior.\n",
    "\n",
    "Let’s pick $\\alpha = 20$ and solve for $\\beta$ so the mean is $0.2$, Then recompute the posterior distribution but with this prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative prior parameters\n",
    "target_prior_mean = control_group_conversion_rate  # 20%\n",
    "alpha_prior = 20 # Small value for high entropy\n",
    "beta_prior = (alpha_prior / target_prior_mean) - alpha_prior  # Solve for beta given mean\n",
    "\n",
    "print(f\"Prior: Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "print(f\"Prior mean: {alpha_prior / (alpha_prior + beta_prior):.4f}\")\n",
    "print(f\"Prior variance: {(alpha_prior * beta_prior) / ((alpha_prior + beta_prior)**2 * (alpha_prior + beta_prior + 1)):.6f}\")\n",
    "\n",
    "# Posterior parameters after observing data\n",
    "alpha_posterior = xA_observed + alpha_prior\n",
    "beta_posterior = (nA - xA_observed) + beta_prior\n",
    "\n",
    "print(f\"\\nPosterior: Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Compute probability that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "non_inferiority_threshold = control_group_conversion_rate - epsilon\n",
    "prob_non_inferior = 1 - beta_dist.cdf(non_inferiority_threshold, alpha_posterior, beta_posterior)\n",
    "\n",
    "print(f\"\\nProbability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probability that the variant conversion rate is above {non_inferiority_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here unlike the NHST, thanks to adding a reasonable prior that uses our understanding of how the CX for passkey is built, we end up having something actionable. We just have an above 95% probability of being over the cutoff so we are non-inferior, good to go.\n",
    "\n",
    "Here is a diagram of this prior and the posterior after observing data $(x_A, n_A)$ for the variant, and various credible intervals. For non-inferiority we don't even need the credible interval (see next).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probability that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "prob_non_inferior = 1 - beta_dist.cdf(control_group_conversion_rate - epsilon, \n",
    "                                       alpha_posterior, beta_posterior)\n",
    "print(f\"Probability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probability that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior vs Posterior with non-inferiority tail area (P(p_A > p_C - ε))\n",
    "\n",
    "# Prereqs: control_group_conversion_rate, epsilon, alpha_prior, beta_prior,\n",
    "#          alpha_posterior, beta_posterior, beta_dist, np, plt\n",
    "\n",
    "threshold = control_group_conversion_rate - epsilon\n",
    "\n",
    "# Compute probabilities (posterior and prior) for reference\n",
    "prob_non_inferior_post = beta_dist.sf(threshold, alpha_posterior, beta_posterior)\n",
    "prob_non_inferior_prior = beta_dist.sf(threshold, alpha_prior, beta_prior)\n",
    "\n",
    "# Plot range focused around the relevant region\n",
    "x_min = max(0.0, threshold - 0.12)\n",
    "x_max = min(1.0, threshold + 0.28)\n",
    "x_range = np.linspace(x_min, x_max, 1200)\n",
    "\n",
    "prior_pdf = beta_dist.pdf(x_range, alpha_prior, beta_prior)\n",
    "post_pdf  = beta_dist.pdf(x_range, alpha_posterior, beta_posterior)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Curves\n",
    "ax.plot(x_range, prior_pdf, color=\"#1f77b4\", lw=2, ls=\"--\",\n",
    "        label=f\"Prior Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "ax.plot(x_range, post_pdf, color=\"#d62728\", lw=2.5,\n",
    "        label=f\"Posterior Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "\n",
    "# Non-inferiority boundary\n",
    "ax.axvline(threshold, color=\"k\", ls=\":\", lw=1.8,\n",
    "           label=f\"Non-inferiority boundary = {threshold:.2f}\")\n",
    "\n",
    "# Shade posterior tail (non-inferiority probability)\n",
    "mask_post = x_range >= threshold\n",
    "ax.fill_between(x_range[mask_post], post_pdf[mask_post], color=\"#d62728\", alpha=0.3,\n",
    "                label=f\"Posterior tail area = {prob_non_inferior_post:.3f}\")\n",
    "\n",
    "# Optional: shade prior tail for comparison (lighter)\n",
    "mask_prior = x_range >= threshold\n",
    "ax.fill_between(x_range[mask_prior], prior_pdf[mask_prior], color=\"#1f77b4\", alpha=0.15,\n",
    "                label=f\"Prior tail area = {prob_non_inferior_prior:.3f}\")\n",
    "\n",
    "# Decorations\n",
    "ax.set_xlim(x_min, x_max)\n",
    "ax.set_xlabel(\"Conversion rate p_A\", fontsize=12)\n",
    "ax.set_ylabel(\"Density\", fontsize=12)\n",
    "ax.set_title(\"Prior vs Posterior and Non-Inferiority Probability (tail area)\", fontsize=14)\n",
    "ax.grid(True, ls=\":\", alpha=0.5)\n",
    "ax.legend(loc=\"best\", fontsize=10)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Posterior P(p_A > {threshold:.2f}) = {prob_non_inferior_post:.4f} \"\n",
    "      f\"({prob_non_inferior_post*100:.2f}%)\")\n",
    "print(f\"Prior     P(p_A > {threshold:.2f}) = {prob_non_inferior_prior:.4f} \"\n",
    "      f\"({prob_non_inferior_prior*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Variant\n",
    "\n",
    "This is where the difference between **NHST** and a **Bayesian** approach becomes dramatic.  \n",
    "Let’s compare the main options.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧪 NHST Approaches\n",
    "\n",
    "\n",
    "#### 1. Winner-Takes-All\n",
    "- Pick the variant with the highest observed conversion rate.\n",
    "\n",
    "**Problems:**  \n",
    "- Ignores uncertainty and sampling noise.  \n",
    "- Easily picks the wrong variant when samples are small.\n",
    "\n",
    "#### 2. Pairwise *t*-Tests with Bonferroni Correction\n",
    "- Run one test for every pair (A vs B, A vs C, B vs C).  \n",
    "- Adjust the significance threshold to control false positives:  \n",
    "  $$\\alpha_\\text{corrected} = \\frac{0.05}{3} \\approx 0.0167.$$\n",
    "\n",
    "**Problems:**  \n",
    "- Multiple comparisons inflate Type I error; Bonferroni is very conservative (higher Type II error).  \n",
    "- Only gives “significant / not significant” — no direct probability of being best.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. ANOVA + Post-Hoc Tests\n",
    "- One-way ANOVA checks if *any* difference exists, then post-hoc tests (Tukey, Dunnett, etc.) try to find which.\n",
    "\n",
    "**Problems:**  \n",
    "- Still needs multiple-comparison corrections.  \n",
    "- ANOVA only says “something differs” — not which is best or by how much.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Confidence Interval Overlap\n",
    "- Compute 95% CIs for each variant and check for overlap.\n",
    "\n",
    "**Problems:**  \n",
    "- Overlapping CIs don’t mean “no difference.”  \n",
    "- Often inconclusive and gives no probability a variant is best.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌟 Bayesian Approach — Probability of Being Best\n",
    "\n",
    "The Bayesian framework answers the question we actually care about:  \n",
    "> *Which variant is most likely the best?*\n",
    "\n",
    "**Method:**\n",
    "1. Compute the **posterior Beta distribution** for each variant using its prior and observed data.  \n",
    "2. Draw a large number of samples (e.g., 100k) from each posterior.  \n",
    "3. For each simulated draw, identify which variant has the highest conversion rate.  \n",
    "4. Report the probabilities:  \n",
    "   $$P(A \\text{ is best}),\\; P(B \\text{ is best}),\\; P(C \\text{ is best}), \\ldots$$\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "-  **Direct answer:** “Variant B is best with 88.8% probability.”  \n",
    "-  **Single coherent analysis:** no need for multiple-comparison corrections.  \n",
    "-  **Scales naturally:** works the same way for 3, 5, 10, or 100 variants.  \n",
    "-  **Quantifies uncertainty:** not just yes/no; can report $P(B>A)$, $P(B>A \\;\\&\\; B>C)$, etc.  \n",
    "-  **Flexible:** easily integrates prior knowledge and business context.  \n",
    "-  **Business-friendly:** simple to factor in risk, cost, and implementation difficulty.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Key point:**  \n",
    "Bayesian analysis gives a **probability each variant is best** — a direct, interpretable metric that scales cleanly and supports real-world decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three variants with similar conversion rates\n",
    "# All are non-inferior (above 17% boundary) but we want the best one\n",
    "variants = {\n",
    "    'A': {'n': 800, 'x': 168},  # 21.0% conversion rate\n",
    "    'B': {'n': 800, 'x': 172},  # 21.5% conversion rate\n",
    "    'C': {'n': 800, 'x': 165}   # 20.625% conversion rate\n",
    "}\n",
    "\n",
    "print(\"\\nVariant Data:\")\n",
    "print(\"-\" * 80)\n",
    "for name, data in variants.items():\n",
    "    observed_rate = data['x'] / data['n']\n",
    "    print(f\"Variant {name}: n={data['n']:4d}, x={data['x']:3d}, \"\n",
    "          f\"observed rate = {observed_rate:.4f} ({observed_rate*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nAll variants are above non-inferiority boundary: {control_group_conversion_rate - epsilon:.2f}\")\n",
    "print(\"But which one should we choose?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variants are above non-inferiority boundary: {control_group_conversion_rate - epsilon:.2f}\n",
    "But which one should we choose?\n",
    "In this specific case because all sample have the same size (n), we can just compare the mean but the \"clean\" way to do it is to run a Monte Carlo simulation to see which Variant would win \"in all possible universes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior distributions (using non-informative prior Beta(1,1))\n",
    "posteriors = {}\n",
    "\n",
    "print(\"\\nPosterior Distributions:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, data in variants.items():\n",
    "    # Posterior parameters (with non-informative prior Beta(1,1))\n",
    "    alpha_post = data['x'] + 1\n",
    "    beta_post = data['n'] - data['x'] + 1\n",
    "    \n",
    "    # Posterior statistics\n",
    "    posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "    posterior_var = (alpha_post * beta_post) / \\\n",
    "                    ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n",
    "    posterior_std = np.sqrt(posterior_var)\n",
    "    \n",
    "    # Credible intervals\n",
    "    ci_95_lower = beta_dist.ppf(0.025, alpha_post, beta_post)\n",
    "    ci_95_upper = beta_dist.ppf(0.975, alpha_post, beta_post)\n",
    "    \n",
    "    posteriors[name] = {\n",
    "        'alpha': alpha_post,\n",
    "        'beta': beta_post,\n",
    "        'mean': posterior_mean,\n",
    "        'std': posterior_std,\n",
    "        'ci_95': (ci_95_lower, ci_95_upper)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nVariant {name}:\")\n",
    "    print(f\"  Posterior: Beta(α={alpha_post}, β={beta_post})\")\n",
    "    print(f\"  Posterior mean: {posterior_mean:.4f}\")\n",
    "    print(f\"  Posterior std: {posterior_std:.4f}\")\n",
    "    print(f\"  95% Credible Interval: [{ci_95_lower:.4f}, {ci_95_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create visualization of the three posterior distributions\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = {'A': '#1f77b4', 'B': '#ff7f0e', 'C': '#2ca02c'}\n",
    "x_range = np.linspace(0.15, 0.30, 1000)\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    alpha_p = posteriors[name]['alpha']\n",
    "    beta_p = posteriors[name]['beta']\n",
    "    mean_p = posteriors[name]['mean']\n",
    "    ci_lower, ci_upper = posteriors[name]['ci_95']\n",
    "    \n",
    "    # Plot PDF\n",
    "    pdf = beta_dist.pdf(x_range, alpha_p, beta_p)\n",
    "    ax.plot(x_range, pdf, color=colors[name], lw=2.5, \n",
    "            label=f'{name}: mean={mean_p:.4f}')\n",
    "    \n",
    "    # Mark the mean\n",
    "    ax.axvline(mean_p, color=colors[name], ls='--', lw=1, alpha=0.5)\n",
    "    \n",
    "    # Shade 95% credible interval\n",
    "    mask = (x_range >= ci_lower) & (x_range <= ci_upper)\n",
    "    ax.fill_between(x_range[mask], pdf[mask], alpha=0.2, color=colors[name])\n",
    "\n",
    "# Add non-inferiority boundary\n",
    "boundary = control_group_conversion_rate - epsilon\n",
    "ax.axvline(boundary, color='red', ls=':', lw=2, \n",
    "           label=f'Non-inferiority boundary ({boundary:.2f})')\n",
    "\n",
    "# Add control rate\n",
    "ax.axvline(control_group_conversion_rate, color='black', ls=':', lw=2,\n",
    "           label=f'Control rate ({control_group_conversion_rate:.2f})')\n",
    "\n",
    "ax.set_xlabel('Conversion Rate', fontsize=12)\n",
    "ax.set_ylabel('Posterior Density', fontsize=12)\n",
    "ax.set_title('Posterior Distributions for Variants A, B, C', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(True, ls=':', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ All three posterior distributions overlap significantly\")\n",
    "print(\"  This shows there's uncertainty about which is truly best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation\n",
    "n_simulations = 100000\n",
    "print(f\"Running {n_simulations:,} simulations...\\n\")\n",
    "\n",
    "# Draw samples from each posterior\n",
    "samples = {}\n",
    "for name in ['A', 'B', 'C']:\n",
    "    alpha_p = posteriors[name]['alpha']\n",
    "    beta_p = posteriors[name]['beta']\n",
    "    samples[name] = beta_dist.rvs(alpha_p, beta_p, size=n_simulations)\n",
    "\n",
    "# For each simulation, determine which variant is best\n",
    "best_counts = {'A': 0, 'B': 0, 'C': 0}\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # Get the sampled values for this simulation\n",
    "    sample_values = {\n",
    "        'A': samples['A'][i],\n",
    "        'B': samples['B'][i],\n",
    "        'C': samples['C'][i]\n",
    "    }\n",
    "    \n",
    "    # Find which variant has the highest value in this simulation\n",
    "    best_variant = max(sample_values, key=lambda k: sample_values[k])\n",
    "    best_counts[best_variant] += 1\n",
    "\n",
    "# Calculate probabilities\n",
    "probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "\n",
    "print(\"RESULTS: Probability Each Variant is Best\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    prob = probabilities[name]\n",
    "    bar = '█' * int(prob * 60)\n",
    "    print(f\"P({name} is best) = {prob:.4f} ({prob*100:5.2f}%) {bar}\")\n",
    "\n",
    "# Determine the winner\n",
    "winner = max(probabilities, key=probabilities.get)\n",
    "winner_prob = probabilities[winner]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAYESIAN CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Variant {winner} is most likely the best\")\n",
    "print(f\"  Probability: {winner_prob:.4f} ({winner_prob*100:.1f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - There's a {winner_prob*100:.1f}% chance that {winner} has the highest true conversion rate\")\n",
    "print(f\"  - This accounts for uncertainty in all three estimates\")\n",
    "print(f\"  - Clear, actionable decision with quantified confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Standalone function to compute non inferiorit and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "def test_non_inferiority(n_control, x_control, variants_data, epsilon, \n",
    "                         alpha_prior=1, beta_prior=1, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Test non-inferiority of multiple variants against a control.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_control : int\n",
    "        Number of samples in control group\n",
    "    x_control : int\n",
    "        Number of successes in control group\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 1000, 'x': 200}, 'B': {'n': 1000, 'x': 215}}\n",
    "    epsilon : float\n",
    "        Non-inferiority margin (e.g., 0.03 for 3%)\n",
    "    alpha_prior : float, optional\n",
    "        Alpha parameter for Beta prior (default: 1 for uniform)\n",
    "    beta_prior : float, optional\n",
    "        Beta parameter for Beta prior (default: 1 for uniform)\n",
    "    threshold : float, optional\n",
    "        Probability threshold for declaring non-inferiority (default: 0.95)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with results for each variant containing:\n",
    "        - 'is_non_inferior': bool, whether variant is non-inferior\n",
    "        - 'probability': float, P(variant > control - epsilon)\n",
    "        - 'control_rate': float, posterior mean of control\n",
    "        - 'variant_rate': float, posterior mean of variant\n",
    "        - 'posterior_params': tuple, (alpha, beta) of variant posterior\n",
    "    \"\"\"\n",
    "    # Control posterior\n",
    "    alpha_control = x_control + alpha_prior\n",
    "    beta_control = n_control - x_control + beta_prior\n",
    "    control_rate = alpha_control / (alpha_control + beta_control)\n",
    "    \n",
    "    # Boundary for non-inferiority\n",
    "    boundary = control_rate - epsilon\n",
    "    \n",
    "    results = {}\n",
    "    n_simulations = 100000\n",
    "    \n",
    "    # Sample from control posterior once (reuse for all variants)\n",
    "    control_samples = beta_dist.rvs(alpha_control, beta_control, size=n_simulations)\n",
    "    \n",
    "    for variant_name, data in variants_data.items():\n",
    "        # Variant posterior\n",
    "        alpha_variant = data['x'] + alpha_prior\n",
    "        beta_variant = data['n'] - data['x'] + beta_prior\n",
    "        variant_rate = alpha_variant / (alpha_variant + beta_variant)\n",
    "        \n",
    "        # Sample from variant posterior\n",
    "        variant_samples = beta_dist.rvs(alpha_variant, beta_variant, size=n_simulations)\n",
    "        \n",
    "        # Compute P(variant > control - epsilon)\n",
    "        prob_non_inferior = np.mean(variant_samples > (control_samples - epsilon))\n",
    "        \n",
    "        results[variant_name] = {\n",
    "            'is_non_inferior': prob_non_inferior >= threshold,\n",
    "            'probability': prob_non_inferior,\n",
    "            'control_rate': control_rate,\n",
    "            'variant_rate': variant_rate,\n",
    "            'posterior_params': (alpha_variant, beta_variant)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def select_best_variant(variants_data, alpha_prior=1, beta_prior=1, \n",
    "                       credible_level=0.95, n_simulations=100000):\n",
    "    \"\"\"\n",
    "    Select the best variant among multiple options using Bayesian approach.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 800, 'x': 168}, 'B': {'n': 800, 'x': 172}}\n",
    "    alpha_prior : float, optional\n",
    "        Alpha parameter for Beta prior (default: 1 for uniform)\n",
    "    beta_prior : float, optional\n",
    "        Beta parameter for Beta prior (default: 1 for uniform)\n",
    "    credible_level : float, optional\n",
    "        Credible interval level (default: 0.95)\n",
    "    n_simulations : int, optional\n",
    "        Number of Monte Carlo simulations (default: 100000)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'best_variant': str, name of variant most likely to be best\n",
    "        - 'probabilities': dict, P(each variant is best)\n",
    "        - 'posterior_means': dict, posterior mean for each variant\n",
    "        - 'credible_intervals': dict, (lower, upper) credible interval for each variant\n",
    "        - 'expected_loss': dict, expected loss from choosing each variant\n",
    "    \"\"\"\n",
    "    variant_names = list(variants_data.keys())\n",
    "    posteriors = {}\n",
    "    samples = {}\n",
    "    \n",
    "    # Compute posteriors and draw samples\n",
    "    for name, data in variants_data.items():\n",
    "        alpha_post = data['x'] + alpha_prior\n",
    "        beta_post = data['n'] - data['x'] + beta_prior\n",
    "        \n",
    "        posteriors[name] = {\n",
    "            'alpha': alpha_post,\n",
    "            'beta': beta_post,\n",
    "            'mean': alpha_post / (alpha_post + beta_post)\n",
    "        }\n",
    "        \n",
    "        # Draw samples from posterior\n",
    "        samples[name] = beta_dist.rvs(alpha_post, beta_post, size=n_simulations)\n",
    "        \n",
    "        # Compute credible interval\n",
    "        ci_lower = beta_dist.ppf((1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        ci_upper = beta_dist.ppf(1 - (1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        posteriors[name]['credible_interval'] = (ci_lower, ci_upper)\n",
    "    \n",
    "    # Monte Carlo: count how often each variant is best\n",
    "    best_counts = {name: 0 for name in variant_names}\n",
    "    \n",
    "    for i in range(n_simulations):\n",
    "        # Get samples for this iteration\n",
    "        sample_values = {name: samples[name][i] for name in variant_names}\n",
    "        \n",
    "        # Find best variant in this simulation\n",
    "        best_variant = max(sample_values, key=sample_values.get)\n",
    "        best_counts[best_variant] += 1\n",
    "    \n",
    "    # Calculate probabilities\n",
    "    probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "    \n",
    "    # Expected loss: E[max(all) - this variant]\n",
    "    expected_loss = {}\n",
    "    for name in variant_names:\n",
    "        max_samples = np.maximum.reduce([samples[v] for v in variant_names])\n",
    "        losses = max_samples - samples[name]\n",
    "        expected_loss[name] = np.mean(losses)\n",
    "    \n",
    "    # Determine best variant\n",
    "    best_variant = max(probabilities, key=probabilities.get)\n",
    "    \n",
    "    return {\n",
    "        'best_variant': best_variant,\n",
    "        'probabilities': probabilities,\n",
    "        'posterior_means': {name: posteriors[name]['mean'] for name in variant_names},\n",
    "        'credible_intervals': {name: posteriors[name]['credible_interval'] \n",
    "                              for name in variant_names},\n",
    "        'expected_loss': expected_loss\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Non-inferiority test\n",
    "    print(\"=\"*80)\n",
    "    print(\"NON-INFERIORITY TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    n_control = 7000\n",
    "    x_control = 1400\n",
    "    variants = {\n",
    "        'A': {'n': 1000, 'x': 200},\n",
    "        'B': {'n': 1000, 'x': 215},\n",
    "        'C': {'n': 1000, 'x': 195}\n",
    "    }\n",
    "    \n",
    "    non_inf_results = test_non_inferiority(\n",
    "        n_control=n_control,\n",
    "        x_control=x_control,\n",
    "        variants_data=variants,\n",
    "        epsilon=0.03,\n",
    "        threshold=0.95\n",
    "    )\n",
    "    \n",
    "    for variant, result in non_inf_results.items():\n",
    "        print(f\"\\nVariant {variant}:\")\n",
    "        print(f\"  Control rate: {result['control_rate']:.4f}\")\n",
    "        print(f\"  Variant rate: {result['variant_rate']:.4f}\")\n",
    "        print(f\"  P(variant > control - 0.03): {result['probability']:.4f}\")\n",
    "        print(f\"  Non-inferior? {'YES ✓' if result['is_non_inferior'] else 'NO ✗'}\")\n",
    "    \n",
    "    # Select best variant (only among non-inferior ones)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"BEST VARIANT SELECTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter to only non-inferior variants\n",
    "    non_inferior_variants = {\n",
    "        name: data for name, data in variants.items()\n",
    "        if non_inf_results[name]['is_non_inferior']\n",
    "    }\n",
    "    \n",
    "    if non_inferior_variants:\n",
    "        best_results = select_best_variant(non_inferior_variants)\n",
    "        \n",
    "        print(f\"\\nBest variant: {best_results['best_variant']}\")\n",
    "        print(f\"\\nProbabilities of being best:\")\n",
    "        for name, prob in best_results['probabilities'].items():\n",
    "            print(f\"  {name}: {prob:.4f} ({prob*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nPosterior means:\")\n",
    "        for name, mean in best_results['posterior_means'].items():\n",
    "            print(f\"  {name}: {mean:.4f}\")\n",
    "        \n",
    "        print(f\"\\n95% Credible Intervals:\")\n",
    "        for name, (lower, upper) in best_results['credible_intervals'].items():\n",
    "            print(f\"  {name}: [{lower:.4f}, {upper:.4f}]\")\n",
    "        \n",
    "        print(f\"\\nExpected loss (if you choose this variant):\")\n",
    "        for name, loss in best_results['expected_loss'].items():\n",
    "            print(f\"  {name}: {loss:.6f} ({loss*100:.4f}%)\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  No variants are non-inferior to control!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nhst_non_inferiority_test(n_control, x_control, n_variant, x_variant, \n",
    "                                epsilon, alpha=0.05, h1_effect_size=0.0):\n",
    "    \"\"\"\n",
    "    Perform NHST non-inferiority test for a single variant against control.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_control : int\n",
    "        Number of samples in control group\n",
    "    x_control : int\n",
    "        Number of successes in control group\n",
    "    n_variant : int\n",
    "        Number of samples in variant group\n",
    "    x_variant : int\n",
    "        Number of successes in variant group\n",
    "    epsilon : float\n",
    "        Non-inferiority margin (e.g., 0.03 for 3% acceptable degradation)\n",
    "    alpha : float, optional\n",
    "        Significance level for Type I error (default: 0.05)\n",
    "    h1_effect_size : float, optional\n",
    "        Expected effect size under H1 for power calculation (default: 0.0, meaning no difference)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing:\n",
    "        - 'decision': str, 'REJECT H0' or 'FAIL TO REJECT H0'\n",
    "        - 'p_value': float, probability of observed result under H0\n",
    "        - 'power': float, probability of rejecting H0 when H1 is true\n",
    "        - 'observed_difference': float, observed difference in proportions (variant - control)\n",
    "        - 'critical_value': float, threshold for rejection at significance level alpha\n",
    "        - 'se_h0': float, standard error under H0 (boundary case)\n",
    "        - 'se_h1': float, standard error under H1\n",
    "        - 'control_rate': float, observed control conversion rate\n",
    "        - 'variant_rate': float, observed variant conversion rate\n",
    "        - 'z_statistic': float, standardized test statistic\n",
    "    \"\"\"\n",
    "    from scipy.stats import norm\n",
    "    \n",
    "    # Observed proportions\n",
    "    p_control = x_control / n_control\n",
    "    p_variant = x_variant / n_variant\n",
    "    observed_diff = p_variant - p_control\n",
    "    \n",
    "    # Standard error under H0 (unpooled, since we're not assuming p_A = p_C)\n",
    "    se_h0 = np.sqrt((p_control * (1 - p_control) / n_control) + \n",
    "                     (p_variant * (1 - p_variant) / n_variant))\n",
    "    \n",
    "    # Hypothesis test under boundary H0: E[Delta] = -epsilon\n",
    "    mu_h0 = -epsilon\n",
    "    \n",
    "    # Compute p-value (right-tail test: H1 is Delta > -epsilon)\n",
    "    p_value = norm.sf(observed_diff, loc=mu_h0, scale=se_h0)\n",
    "    \n",
    "    # Critical value for rejection at significance level alpha\n",
    "    critical_value = norm.isf(alpha, loc=mu_h0, scale=se_h0)\n",
    "    \n",
    "    # Decision\n",
    "    decision = 'REJECT H0' if p_value <= alpha else 'FAIL TO REJECT H0'\n",
    "    \n",
    "    # Z-statistic (standardized)\n",
    "    z_statistic = (observed_diff - mu_h0) / se_h0\n",
    "    \n",
    "    # Power calculation under H1\n",
    "    # Under H1, assume true effect is h1_effect_size (e.g., 0 for \"no worse\")\n",
    "    # Use pooled SE if h1_effect_size = 0 (assuming p_variant = p_control under H1)\n",
    "    if h1_effect_size == 0.0:\n",
    "        # Pooled proportion for H1\n",
    "        p_pooled = (x_control + x_variant) / (n_control + n_variant)\n",
    "        se_h1 = np.sqrt(p_pooled * (1 - p_pooled) * (1/n_control + 1/n_variant))\n",
    "    else:\n",
    "        # Use unpooled SE with assumed rates under H1\n",
    "        # Assume variant has control_rate + h1_effect_size\n",
    "        p_variant_h1 = p_control + h1_effect_size\n",
    "        se_h1 = np.sqrt((p_control * (1 - p_control) / n_control) + \n",
    "                        (p_variant_h1 * (1 - p_variant_h1) / n_variant))\n",
    "    \n",
    "    # Under H1, the distribution is centered at h1_effect_size\n",
    "    mu_h1 = h1_effect_size\n",
    "    \n",
    "    # Power = P(reject H0 | H1 is true) = P(observed_diff > critical_value | H1)\n",
    "    # This is the right-tail probability of the H1 distribution beyond critical_value\n",
    "    power = norm.sf(critical_value, loc=mu_h1, scale=se_h1)\n",
    "    \n",
    "    return {\n",
    "        'decision': decision,\n",
    "        'p_value': p_value,\n",
    "        'power': power,\n",
    "        'observed_difference': observed_diff,\n",
    "        'critical_value': critical_value,\n",
    "        'se_h0': se_h0,\n",
    "        'se_h1': se_h1,\n",
    "        'control_rate': p_control,\n",
    "        'variant_rate': p_variant,\n",
    "        'z_statistic': z_statistic\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage with the notebook's data\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\"*80)\n",
    "    print(\"NHST NON-INFERIORITY TEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use the same data from the notebook\n",
    "    result = nhst_non_inferiority_test(\n",
    "        n_control=7000,\n",
    "        x_control=1400,  # 20% rate\n",
    "        n_variant=150,\n",
    "        x_variant=33,\n",
    "        epsilon=0.03,\n",
    "        alpha=0.05,\n",
    "        h1_effect_size=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nObserved Data:\")\n",
    "    print(f\"  Control: n={7000}, x={1400}, rate={result['control_rate']:.4f}\")\n",
    "    print(f\"  Variant: n={150}, x={33}, rate={result['variant_rate']:.4f}\")\n",
    "    print(f\"  Observed difference (variant - control): {result['observed_difference']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTest Configuration:\")\n",
    "    print(f\"  Non-inferiority margin (ε): {0.03:.3f}\")\n",
    "    print(f\"  Significance level (α): {0.05:.3f}\")\n",
    "    print(f\"  H1 effect size: {0.0:.3f}\")\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"  Decision: {result['decision']}\")\n",
    "    print(f\"  p-value: {result['p_value']:.4f}\")\n",
    "    print(f\"  z-statistic: {result['z_statistic']:.4f}\")\n",
    "    print(f\"  Critical value: {result['critical_value']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nStandard Errors:\")\n",
    "    print(f\"  SE under H0 (boundary): {result['se_h0']:.4f}\")\n",
    "    print(f\"  SE under H1 (pooled): {result['se_h1']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nPower Analysis:\")\n",
    "    print(f\"  Power (1 - β): {result['power']:.4f} ({result['power']*100:.1f}%)\")\n",
    "    print(f\"  Type II error (β): {1 - result['power']:.4f} ({(1-result['power'])*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if result['decision'] == 'REJECT H0':\n",
    "        print(f\"  ✓ The variant is statistically non-inferior to control at α={0.05}\")\n",
    "        print(f\"    (p-value {result['p_value']:.4f} < {0.05})\")\n",
    "    else:\n",
    "        print(f\"  ✗ Cannot conclude non-inferiority at α={0.05}\")\n",
    "        print(f\"    (p-value {result['p_value']:.4f} > {0.05})\")\n",
    "    \n",
    "    print(f\"\\n  Power is {result['power']*100:.1f}% - this test has a {(1-result['power'])*100:.1f}% chance\")\n",
    "    print(f\"  of missing non-inferiority even if variant truly has no degradation.\")\n",
    "    \n",
    "    if result['power'] < 0.8:\n",
    "        print(f\"\\n  ⚠️  WARNING: Power is below conventional threshold of 80%!\")\n",
    "        print(f\"     Consider increasing sample size for more reliable results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
