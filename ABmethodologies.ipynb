{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exec Summary\n",
    "\n",
    "This notebook presents both Null Hypothesis Significance Testing (NHST) and Bayesian Methodologies, goes into a lot of details about the math and the methodological considerations which make a Bayesian approach a better fit for web/mobile AB tests. But if you want to skip the math and just get the benefits, go to the end of the notebook and use the Python functions:\n",
    "\n",
    "1. `test_non_inferiority()` - Verify new features don't degrade experience\n",
    "2. `select_best_variant()` - Choose winning variant with probability\n",
    "\n",
    "which should be sufficient to support a CX analysis with 3 variants and a control. \n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "When launching new web or mobile features, engineering teams face a common dilemma:\n",
    "\n",
    "- **Limited traffic allocation**: At launch new features get only 2-5% of traffic to minimize risk\n",
    "- **Multiple variants**: Design teams often propose 3-5 different implementations\n",
    "- **Small sample sizes**: At launch because of controlled releases, each variant may only see hundreds or low thousands of users\n",
    "- **Need for speed**: We need fast decisions on which variants are best to iterate or scale\n",
    "- **Less than perfect launch logistics** because of bugs or misconfiguration the allocation between variants and control may not be what was planned i.e. some samples may be too large or too small or even missing.\n",
    "\n",
    "**Traditional NHST fails here**: With small, unbalanced samples (e.g., 7,000 control vs. 150 per variant), statistical tests either:\n",
    "- Fail to reach significance (underpowered, Œ≤ > 0.8, meaning power < 20%)\n",
    "- Require weeks of data collection\n",
    "- Is unwieldy to compare more than 2 variants at a time\n",
    "\n",
    "## The Bayesian Solution\n",
    "\n",
    "Bayesian methods excel precisely where NHST struggles:\n",
    "\n",
    "### 1. **Works with Small Samples**\n",
    "- Incorporates prior knowledge (historical conversion rates)\n",
    "- Updates beliefs incrementally as data arrives\n",
    "- Provides meaningful conclusions even with n=150 per variant\n",
    "- No arbitrary \"minimum sample size\" requirement to reach statistical significance.\n",
    "\n",
    "### 2. **Handles Unbalanced Allocation Naturally**\n",
    "- 90% control, 10% variants? No problem.\n",
    "- Each variant can have different sample sizes\n",
    "- No need for equal allocation or \"balanced designs\"\n",
    "- Protects existing user experience while testing\n",
    "\n",
    "### 3. **Scales to Many Variants Effortlessly**\n",
    "- Compare 3, 5, 10, or 100 variants simultaneously\n",
    "- Single coherent analysis‚Äîno multiple comparison penalties\n",
    "- Direct answer: P(A is best) = 31%, P(B is best) = 47%, P(C is best) = 22%\n",
    "\n",
    "### 4. **Provides Actionable Probabilities**\n",
    "- NHST says: \"Cannot reject H‚ÇÄ\" , not directly actionable, say if one has idea of the cost of a bad decision there is no real way to compute an expected value for what can be seen as a bet on being right or wrong.\n",
    "- Bayesian says: \"47% chance B is best, 22% it's worse than control\" (actionable, expected value can be computed, can directly feed into a decision algorithm)\n",
    "- Direct business decision: Deploy B with quantified risk\n",
    "\n",
    "### 5. **Allows Continuous Monitoring**\n",
    "- Can check results anytime without \"p-hacking\" concerns\n",
    "- Smoothly and incrementally update posterior probability as new data arrives\n",
    "- Stop early if clear winner emerges, or rebalance incrementally (See multi armed bandits strategies)\n",
    "- Continue if more certainty needed‚Äîmathematically rigorous\n",
    "\n",
    "\n",
    "## Key Benefits Summary\n",
    "\n",
    "| Aspect | Traditional NHST | Bayesian Approach |\n",
    "|--------|-----------------|-------------------|\n",
    "| Small samples | Underpowered, inconclusive | Works well with prior knowledge |\n",
    "| Unbalanced allocation | Loses efficiency | No problem |\n",
    "| Multiple variants | Complex corrections needed | Natural single analysis |\n",
    "| Interpretation | p-value (hard to explain) | Probability (intuitive) |\n",
    "| Decision making | Binary reject/fail | Quantified risk/confidence |\n",
    "| Continuous monitoring | Forbidden (p-hacking) | Allowed and rigorous |\n",
    "| Time to decision | Weeks (need larger n) | Days (works with small n) |\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "For modern product development with:\n",
    "- **Rapid iteration cycles**\n",
    "- **bugs/misconfiguration in the traffic splitters**\n",
    "- **Risk-averse traffic allocation**\n",
    "- **Multiple design options**\n",
    "- **Small initial samples**\n",
    "\n",
    "**Bayesian methods provide:**\n",
    "- Faster decisions (days vs. weeks)\n",
    "- Better use of limited data\n",
    "- Clear, business-friendly outputs\n",
    "- Quantified confidence for risk management\n",
    "\n",
    "This enables product teams to launch confidently, iterate quickly, and scale successful features‚Äîall while protecting the existing user experience.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The two standalone Python functions in this notebook provide:\n",
    "1. `test_non_inferiority()` - Verify new features don't degrade experience\n",
    "2. `select_best_variant()` - Choose winning variant with probability\n",
    "\n",
    "Both work with any sample sizes and scale to any number of variants.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from scipy.stats import norm\n",
    "from statsmodels.stats.proportion import confint_proportions_2indep\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "# import the beta function from scipy.special\n",
    "from scipy.special import beta as beta_function\n",
    "from scipy.stats import beta as beta_dist\n",
    "from plotting_utils import plot_gaussian_hypothesis_test\n",
    "from plotting_utils import plot_type_ii_error_analysis, plot_beta_prior_comparison, plot_prior_vs_posterior\n",
    "from plotting_utils import plot_informative_prior_posterior_comparison, plot_weakly_informative_prior_with_variants\n",
    "from plotting_utils import plot_multiple_posteriors_comparison\n",
    "from nhst import compute_sample_size_non_inferiority\n",
    "from bayesian import test_non_inferiority, select_best_variant, test_non_inferiority_weakly_informative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Depth Analysis of the 2 methodologies\n",
    "\n",
    "When evaluating new user experiences (UX) ‚Äî such as launching **passkeys** and measuring their impact on abandonment rates ‚Äî our CX team has provided us with 3 variants $A_1$, $A_2$ and $A_3$ for the passkey creation experience. After launch we need to make sure that adding passkey creation does not degrade the success rate of our current CX significantly by comparing each variant to a control group that will remain on our legacy pages, then we have to decide which of the variants performs best.\n",
    "  \n",
    "This notebook compares two major approaches:\n",
    "\n",
    "- **Null Hypothesis Significance Testing (NHST)** ‚Äî the long-standing statistical framework, widely used but often conceptually tricky and difficult to interpret in practical business decision-making.\n",
    "- **Bayesian methods** ‚Äî increasingly popular even because they offer more flexibility, better ability to work on small samples and produce results that are often easier to interpret directly when deciding on actions (e.g., when to shift more traffic from a control to a new variant).\n",
    "\n",
    "---\n",
    "\n",
    "## Test Setup: Control Group vs. Variants\n",
    "\n",
    "For the sake of the discussion let's assume an existing digital identity+credentials creation flow with a **completion rate of ~20%** (meaning ~80% of users abandon).  \n",
    "\n",
    "Our test design:\n",
    "\n",
    "- Keep **x%** of traffic on the current experience as the **control group** C.\n",
    "- Send the remaining traffic to one or more **variants** $A_1$, $A_2$, $A_3$.\n",
    "\n",
    "The test will be conducted in 2 steps. First we need to determine that each new experience is **no worse** than the current one, accepting small degradation as unavoidable since we are adding more pages and clicks ‚Äî then after establishing we haven't degraded the experience in an unacceptable fashion, shifting more traffic to the variants and deciding which is the better-performing variant.\n",
    "\n",
    "The type of A/B test ‚Äî where the first goal is to ensure a new design does **not degrade** the experience ‚Äî is called a **non-inferiority test** (explained below).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Hypothesis Significance Testing (NHST)\n",
    "\n",
    "At a high level, the **NHST** workflow is:\n",
    "\n",
    "1. **Assume what you *don‚Äôt* want to see** ‚Äî this is the **null hypothesis**.  \n",
    "   - Example in medicine: *‚Äúthe drug has no effect.‚Äù*  \n",
    "   - Example here: *‚Äúthe new experience significantly increases abandonment.‚Äù*\n",
    "2. **Run the experiment** and compute a test statistic which in this case would be a proportion number of successes over total attempts\n",
    "3. **Ask:** *If the null hypothesis were true, how likely is it that we would observe a result at least this extreme?*  \n",
    "   - If that probability (the **p-value**) is very low ‚Äî e.g., below a conventional threshold such as 5% ‚Äî we **reject the null**.\n",
    "\n",
    "Two immediate caveats:\n",
    "- Rejecting the null does **not** prove the opposite is true; it only says the data would be unlikely *if* the null were correct The p-value itself is quantified as P(data | H‚ÇÄ), but the resulting decision (reject or fail to reject) comes with no probability of being correct. Without P(H‚ÇÄ | data), we cannot compute expected values for decision-making‚Äîif deploying a bad variant costs $100k and a good one gains $50k, NHST provides no framework to quantify the expected value of the decision\n",
    "- ‚ÄúUnlikely enough‚Äù is also completely arbitrary ‚Äî thresholds like 5% are conventions, not laws of nature.\n",
    "\n",
    "A key point: NHST computes **$P(\\text{data} \\mid \\text{hypothesis})$**.  \n",
    "Later we‚Äôll see that the Bayesian approach instead computes **$P(\\text{hypothesis} \\mid \\text{data})$** ‚Äî a fundamentally different quantity.\n",
    "\n",
    "---\n",
    "\n",
    "### Modeling Conversion as Random Variables\n",
    "\n",
    "The abandonment or conversion of a UX flow can be modeled with **Bernoulli random variables**:\n",
    "\n",
    "- $X_C$ for the control experience\n",
    "- $X_A$ for a new variant $A$\n",
    "\n",
    "A Bernoulli variable takes only two values: success/failure, convert/abandon, etc.  \n",
    "Each user who sees a page gives one draw from one of these variables.\n",
    "\n",
    "We assume both have the same codomain:\n",
    "\n",
    "$$\n",
    "\\mathcal{X}_C = \\mathcal{X}_A = \\{0,1\\}\n",
    "$$\n",
    "\n",
    "where **1 = convert** (user finishes the intended action, e.g., creating a passkey) and **0 = abandon**.  \n",
    "Technical failures are treated as *success* here because the user attempted the action.\n",
    "\n",
    "---\n",
    "\n",
    "### Sample Proportions\n",
    "\n",
    "NHST usually works with **sample proportions**, the average of $n$ Bernoulli draws:\n",
    "\n",
    "$$\n",
    "\\hat{p}_C = \\frac{1}{n}\\sum_{i=1}^n X_{C_i},\n",
    "\\quad\n",
    "\\hat{p}_A = \\frac{1}{n}\\sum_{i=1}^n X_{A_i}.\n",
    "$$\n",
    "\n",
    "Each $\\hat{p}$:\n",
    "\n",
    "- Is a random variable taking values $\\{0,\\tfrac1n,\\tfrac2n,\\ldots,1\\}$.\n",
    "- Is also an **estimator** of the true expected value $p = E[X]$.  \n",
    "  By the law of large numbers, $\\hat{p} \\to p$ as $n$ grows.\n",
    "\n",
    "(Statisticians use a ‚Äúhat‚Äù to denote an estimator.)\n",
    "\n",
    "Formally, an estimator maps $n$ realizations of $X$ ($\\mathcal{X}^n$) to a real number:\n",
    "\n",
    "$$\n",
    "\\hat{p}: \\mathcal{X}^n \\to [0,1].\n",
    "$$\n",
    "\n",
    "Because it is the mean of $n$ Bernoulli variables, $\\hat{p}$ follows a **binomial** distribution that becomes approximately **Gaussian** when $n$ is large.\n",
    "\n",
    "---\n",
    "\n",
    "### Variance and Standard Deviation of a Sample Proportion\n",
    "\n",
    "For a single Bernoulli $X$:  \n",
    "$$\n",
    "\\mathrm{Var}(X) = p(1-p).\n",
    "$$\n",
    "\n",
    "For the sample proportion:\n",
    "$$\n",
    "\\mathrm{Var}\\!\\left(\\tfrac1n \\sum_{i=1}^n X_i\\right)\n",
    "= \\tfrac1{n^2} n p(1-p)\n",
    "= \\tfrac{p(1-p)}{n}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathrm{Var}(\\hat{p}) = \\frac{p(1-p)}{n}}\n",
    "$$\n",
    "\n",
    "The square root of this variance is the **standard error** ‚Äî a quantity we‚Äôll use later.\n",
    "\n",
    "$$\n",
    "\\boxed{SE = SD(\\hat{p}) = \\sqrt{\\frac{p(1-p)}{n}}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Difference in Proportions\n",
    "\n",
    "For or current problem of deciding \"non ineferiority\" or \"superiority\" we will use a metric which is the **difference** between variant and control proportions:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} = \\hat{p}_A - \\hat{p}_C\n",
    "$$\n",
    "\n",
    "This estimates the true difference\n",
    "\n",
    "$$\n",
    "\\Delta = p_A - p_C.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "- **Null Hypothesis $H_0$** ‚Äî the ‚Äúbad‚Äù scenario we want to reject:  \n",
    "  the new UX **degrades** conversion by at least some small amount $\\epsilon$ we consider unacceptable (e.g., $3\\%$):\n",
    "\n",
    "  $$\n",
    "  H_0: E[\\Delta] \\le -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Alternative Hypothesis $H_1$** ‚Äî the new UX is **not worse** than control (possibly better):\n",
    "\n",
    "  $$\n",
    "  H_1: E[\\Delta] > -\\epsilon\n",
    "  $$\n",
    "\n",
    "- **Boundary Hypothesis** ‚Äî used in test construction:  \n",
    "  assume the difference is exactly at the acceptable degradation limit:\n",
    "\n",
    "  $$\n",
    "  E[\\Delta] = -\\epsilon\n",
    "  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Example\n",
    "\n",
    "For a concrete example, we define the following counts and quantities for each experience:\n",
    "\n",
    "- $n_C$ : number of visitors in the **control** group  \n",
    "- $x_C$ : number of **conversions** observed in the control group\n",
    "\n",
    "- $n_A$ : number of visitors in the **variant** A group  \n",
    "- $x_A$ : number of **conversions** observed in the variant A group\n",
    "\n",
    "- $\\hat{\\Delta}_{\\mathrm{obs}}$ : the **observed difference** in conversion proportions between variant and control, it becomes negative when it goes in the \"wrong\" direction of what we don't want to see happening (degradation)\n",
    "\n",
    "- $-\\epsilon$ : the **acceptable degradation margin**, i.e., the smallest decrease in conversion we are willing to tolerate for the new variant\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual experiment data\n",
    "nC = 4411\n",
    "xC_observed = 3138\n",
    "control_group_conversion_rate = xC_observed / nC \n",
    "\n",
    "# Three variants with actual experiment data\n",
    "variants = {\n",
    "    'A': {'n': 561, 'x': 381},  \n",
    "    'B': {'n': 285, 'x': 192},  \n",
    "    'C': {'n': 294, 'x': 201}   \n",
    "}\n",
    "\n",
    "\n",
    "# Variant A data (from variants dictionary)\n",
    "nA = variants['A']['n']\n",
    "xA_observed = variants['A']['x']\n",
    "\n",
    "# Test parameters\n",
    "epsilon = 0.03  # 5% non-inferiority margin\n",
    "nhst_alpha = 0.05    # 5% significance level\n",
    "\n",
    "# Derived values\n",
    "hatpC_observed = xC_observed / nC\n",
    "hatpA_observed = xA_observed / nA\n",
    "hatDelta_observed = hatpA_observed - hatpC_observed\n",
    "\n",
    "print(f\"Control group conversion rate: {hatpC_observed:.4f}\")\n",
    "print(f\"Treatment group A conversion rate: {hatpA_observed:.4f}\")\n",
    "print(f\"Observed difference in conversion rate: {hatDelta_observed:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation of the Estimator $\\hat{\\Delta}$ (a.k.a. Standard Error in Frequentist Statistics)\n",
    "\n",
    "In NHST, the first step is to **estimate the standard deviation** of the estimator $\\hat{\\Delta}$ (often called the **standard error**, SE).  Note that this is already a relatively convoluted concept: we need to decide in an hypothetical world where we could repeat the experience many times and also assuming the null hypothesis what kind of variance we would observe in the results.\n",
    "\n",
    "We then compare the observed difference in proportions from the experiment to this estimated variability to decide whether the observed effect is ‚Äúfar enough‚Äù from what we would expect still all under the null hypothesis $H_0$.\n",
    "\n",
    "This is a key pain point for NHST:\n",
    "\n",
    "- We **do not know** the true standard deviation ‚Äî it depends on the unknown and unknowable underlying conversion probabilities.\n",
    "- Frequentist methods therefore use the **plug-in principle**: estimate the unknown variance by ‚Äúplugging in‚Äù the sample estimates (the data you just observed).\n",
    "\n",
    "But note the circularity:\n",
    "\n",
    "1. We want to know if the data are unusual under $H_0$.\n",
    "2. To measure ‚Äúunusual,‚Äù we need the standard error assuming $H_0$.\n",
    "3. SE depends on the unknown true rates, so we **plug in** $\\hat{p}$ (from the data!).\n",
    "4. We then use this data-derived SE to judge whether the data are unusual.\n",
    "\n",
    "It‚Äôs like saying: *‚ÄúUse my one measurement to tell me how variable my measurements are, then use that to decide if my measurement is surprising.‚Äù*\n",
    "\n",
    "Frequentists accept this because:\n",
    "\n",
    "- **Long-run frequency view:** if we repeated the procedure many  times in a hypothetical world and everything remained constant, it would have correct average properties over the long run.\n",
    "- **Pragmatism:** it‚Äôs computable from one experiment.\n",
    "- **Simulation evidence:** works ‚Äúreasonably well‚Äù for not too small $n$, though ‚Äúreasonably well‚Äù is debatable (google many articles and books about the *replication crisis* in medicine and \"soft\" sciences).\n",
    "- **Framework limitation:** in classical stats, parameters are fixed unknowns, so no natural way to treat them as random.\n",
    "\n",
    "---\n",
    "\n",
    "### Common Plug-In Approaches (a.k.a. ‚ÄúStandard Error Hacks‚Äù)\n",
    "\n",
    "#### 1. Wald **Pooled** Standard Error (for a ‚ÄúNo Effect‚Äù Hypothesis)\n",
    "\n",
    "If an hypothesis is **no effect** ($p_A = p_C$), we can pool data from control and variant, because under the hypothesis they are assumed to come from the same distribution.\n",
    "\n",
    "Realizations of sample proportions are:\n",
    "\n",
    "$$\n",
    "\\hat{p}_A = \\frac{x_A}{n_A}, \\qquad\n",
    "\\hat{p}_C = \\frac{x_C}{n_C}.\n",
    "$$\n",
    "\n",
    "If \"no effect\" as true (no difference), a pooled estimator is:\n",
    "\n",
    "$$\n",
    "\\hat{p}_{\\text{pool}} = \\frac{x_A + x_C}{n_A + n_C}.\n",
    "$$\n",
    "\n",
    "Variance of the difference between two independent proportions is the sum of their variances:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\mathrm{Var}(\\hat{p}_A) + \\mathrm{Var}(\\hat{p}_C).\n",
    "$$\n",
    "\n",
    "Plugging in the pooled estimate:\n",
    "\n",
    "$$\n",
    "\\mathrm{Var}(\\hat{p}_A - \\hat{p}_C)\n",
    "= \\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right).\n",
    "$$\n",
    "\n",
    "So the **pooled standard error** is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{WaldPooled SE} =\n",
    "\\sqrt{\\hat{p}_{\\text{pool}}(1-\\hat{p}_{\\text{pool}})\n",
    "\\left(\\frac{1}{n_A}+\\frac{1}{n_C}\\right)} }\n",
    "$$\n",
    "\n",
    "This is the classic **standard error of the difference between two independent proportions**.\n",
    "\n",
    "Once SE is computed, we calculate a **z-score** (number of SEs the observed difference is away from the null value 0):\n",
    "\n",
    "$$\n",
    "z = \\frac{\\hat{p}_A - \\hat{p}_C}{\\text{SE}}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Wald **Unpooled** Standard Error (for Non-Inferiority)\n",
    "\n",
    "If the null is **non-inferiority** (allowing a margin $-\\epsilon$), we **cannot** assume $p_A = p_C$, so we don‚Äôt pool.\n",
    "\n",
    "Instead we sum the individual variances (using the plug-in trick separately for each group):\n",
    "\n",
    "$$\n",
    "\\widehat{\\text{WaldUnpooled SE}} =\n",
    "\\sqrt{\\frac{\\hat{p}_A(1-\\hat{p}_A)}{n_A} +\n",
    "      \\frac{\\hat{p}_C(1-\\hat{p}_C)}{n_C}}.\n",
    "$$\n",
    "\n",
    "Ideally, the true $p_A$ and $p_C$ should be used here, but we don‚Äôt know them ‚Äî so we substitute $\\hat{p}_A$ and $\\hat{p}_C$.  \n",
    "This works but can be inaccurate if sample sizes are too small or the underlying rates are toward extremes.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Newcombe / Score-Based (Wilson)\n",
    "\n",
    "- **Better coverage** than Wald, especially with imbalanced sample sizes or very high/low $p$.\n",
    "- Still closed-form and relatively easy to compute.\n",
    "- Still uses plug-in estimates and suffers from the same ‚Äúsingle-sample‚Äù limitation.\n",
    "\n",
    "*(Not detailed here ‚Äî but recommended over Wald when possible.)*\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Miettinen‚ÄìNurminen\n",
    "\n",
    "- Widely used in **clinical trials** and regulated industries (e.g., FDA guidance).\n",
    "- Provides improved accuracy for non-inferiority tests.\n",
    "- However: mathematically complex, still plug-in based, and still fundamentally relies on one sample.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "\n",
    "Frequentist methods **must estimate variance from the data itself** ‚Äî leading to circularity and potential miscalibration, especially for small samples or edge cases.  \n",
    "Later we‚Äôll see how Bayesian methods avoid this by modeling uncertainty about the true conversion rates directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_proportion = (xC_observed + xA_observed) / (nC + nA)\n",
    "wald_pooled_SE = (pooled_proportion * (1 - pooled_proportion) * (1/nC + 1/nA))**0.5\n",
    "print(f\"Wald Pooled Standard Error: {wald_pooled_SE:.4f}\")\n",
    "wald_unpooled_SE = ((hatpC_observed * (1 - hatpC_observed) / nC) + (hatpA_observed * (1 - hatpA_observed) / nA))**0.5\n",
    "print(f\"Wald Unpooled Standard Error: {wald_unpooled_SE:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probability of False Positive, *p*-value, Significance Level $\\alpha$ (sometimes called ‚Äúconfidence‚Äù), and Critical Value\n",
    "\n",
    "Once we have an estimate of the standard error $SE$ the standard deviation of $\\hat{\\Delta}$, NHST assumes a **sampling distribution** for the estimator under the null hypothesis $H_0$.\n",
    "\n",
    "The idea is:\n",
    "\n",
    "- If we know (or assume) the **mean** and **standard deviation** of $\\hat{\\Delta}$ under $H_0$,  \n",
    "- we can model it with a known probability distribution and calculate how likely any observed result is.\n",
    "\n",
    "Although the true process is discrete (binomial), in practice we often approximate it by a **normal (Gaussian)** distribution. This is mathematically simpler and is a good approximation for moderate or large sample sizes.\n",
    "\n",
    "---\n",
    "\n",
    "##### Using  ‚ÄúBoundary‚Äù as the mean of the distribution to integrate to compute the probability of what we are observing.\n",
    "\n",
    "To decide whether the result we are seeing is \"unlikely\", we are going to integrate the right tail of the probably distribution with for lower bound $\\hat{\\Delta}_{\\mathrm{obs}}$ and upper bound $+\\infty$, which means \"probability of the conversion to be at $\\hat{\\Delta}_{\\mathrm{obs}}$ or better\"\n",
    "\n",
    "The null hypothesis  for \"non inferiority\" is technically an inequality\n",
    "\n",
    "$$\n",
    "H_0: E[\\Delta] \\le -\\epsilon.\n",
    "$$\n",
    "\n",
    "So we could imagine many probability distributions centered at any values below $-\\epsilon$ and they would all be be covered (true) by $H0$ but we don't want to do the computuation for all of them, so to get a single distribution to work with, we should use the **boundary value** as the mean of the distribition we will work with:\n",
    "\n",
    "$$\n",
    "\\mu = E[\\Delta] = -\\epsilon.\n",
    "$$\n",
    "\n",
    "\n",
    "Why?  \n",
    "\n",
    "- Whatever decision we make with $\\mu= E[\\Delta] = -\\epsilon$ as the mean of the distribution AND $\\hat{\\Delta}_{\\mathrm{obs}}$ as the lower, we would make the exact same decision if we were integrating a distribution centered somewhere lower on the x axis, than $-\\epsilon$,  as the lower bound of the integral $\\hat{\\Delta}_{\\mathrm{obs}}$ does not move. This would make the right tail even smaller/thiner (less surface, less probable) as you move the mean to the left. But the opposite is not true that is if we picked distribution centered higher than $-\\epsilon$ but still integrated from $\\hat{\\Delta}_{\\mathrm{obs}}$, we could fail to reject the null becuase the probabilithyg get higher (say > 5%) but still reject it if moved the  mean back to $-\\epsilon$ . So this is the most conservative and only correct test we can make.\n",
    "\n",
    "So under $H_0$, we model $\\hat{\\Delta}$ as:\n",
    "\n",
    "$$\n",
    "\\hat{\\Delta} \\sim N(\\mu, \\sigma)\n",
    "$$\n",
    "\n",
    "with\n",
    "\n",
    "$$\n",
    "\\mu = -\\epsilon, \\qquad \\sigma = SE.\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Gaussian N(mu, sigma) and shade the right-tail area beyond x\n",
    "\n",
    "# Use previously defined values\n",
    "SE_H0 = wald_unpooled_SE\n",
    "mu_H0 = -epsilon\n",
    "sigma_HO = SE_H0\n",
    "x0 = hatDelta_observed\n",
    "\n",
    "# Create the plot using the helper function\n",
    "fig, ax = plot_gaussian_hypothesis_test(\n",
    "    mu_H0=mu_H0,\n",
    "    sigma_H0=sigma_HO,\n",
    "    observed_value=x0,\n",
    "    alpha=nhst_alpha,\n",
    "    epsilon=epsilon\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Computing the *p*-Value\n",
    "\n",
    "The **p-value** is the probability (under $H_0$) of observing a result **as extreme or more extreme** than what we got, in the direction of the alternative $H_1$.\n",
    "\n",
    "In this one-sided non-inferiority test, that means the right tail probability:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = P_{H_0}\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\text{obs}}\\big]\n",
    "= \\int_{\\hat{\\Delta}_{\\text{obs}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\n",
    "\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\\,dx.\n",
    "$$\n",
    "\n",
    "This tail integral is the **survival function** of the normal distribution.\n",
    "\n",
    "Using the standard normal CDF $\\Phi$:\n",
    "\n",
    "$$\n",
    "p\\text{-value} = 1 - \\Phi\\!\\left(\\frac{\\hat{\\Delta}_{\\text{obs}}-\\mu}{\\sigma}\\right).\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "##### Significance Level $\\alpha$ and Critical Value\n",
    "\n",
    "- We choose a **significance level** $\\alpha$ (often $0.05$).  \n",
    "- If $p\\text{-value} \\le \\alpha$, we reject $H_0$ ‚Äî our result is unlikely under the null.\n",
    "\n",
    "The **critical value** $c$ is the smallest observed difference that would lead to rejection at level $\\alpha$. It is obtained by inverting the right-tail probability:\n",
    "\n",
    "$$\n",
    "c = \\mu + \\sigma \\,\\Phi^{-1}(1 - \\alpha).\n",
    "$$\n",
    "\n",
    "Any observed $\\hat{\\Delta}_{\\text{obs}} \\ge c$ yields $p\\text{-value} \\le \\alpha$ and thus rejects $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "Below we‚Äôll compute the p-value and critical value explicitly and visualize how the right tail behaves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H0 = wald_unpooled_SE\n",
    "from scipy.stats import norm\n",
    "\n",
    "mu_H0 = -epsilon    # mean\n",
    "sigma_HO = SE_H0  # standard deviation\n",
    "x = hatDelta_observed  # value to evaluate\n",
    "\n",
    "# Survival function P(X > x)\n",
    "p_value = norm.sf(x, loc=mu_H0, scale=sigma_HO)\n",
    "print(f'p-value (one-sided) for an observed difference in proportions at {hatDelta_observed:.4f}: {p_value:.4f}')\n",
    "\n",
    "# inverse survival function to find critical value for given p-value\n",
    "critical_value = norm.isf(nhst_alpha, loc=mu_H0, scale=sigma_HO)\n",
    "print(f\"Critical value for our alpha cutoff value at {nhst_alpha:.4f}: {critical_value:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with the value we chose as an example for the sample, the p-value is a bit more than 7%, so that's not unlikely enough if we picked 5% as the alpha cutoff point. So we would \"fail to reject\", meaning we can't say much, we don't know whether we can claim that the new CX does not cause some unwanted degradation.\n",
    "\n",
    "Note that this is because the sample is tiny. We picked it small on purpose as this is a realistic situation when launching a new feature; for various reasons including unresolved bugs you usually just put a tiny bit of traffic on the new feature. But those small samples are often not sufficient to make a decision based on NHST. This is due to the way the standard error is computed; it needs more data to deliver some insights. In this example we would need our observed proportion to get to 0.026 or better to reject the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Traditional Presentation Using *z*-Scores\n",
    "\n",
    "Another common way to compute the p-value in NHST is to **standardize** the observed statistic rather than work directly with $\\hat{\\Delta}_{\\mathrm{obs}}$.\n",
    "\n",
    "The idea:\n",
    "\n",
    "- Convert our Gaussian with mean $\\mu$ and standard deviation $\\sigma$ into a **standard normal** $N(0,1)$.\n",
    "- This is done by the familiar **$z$-score transformation**:\n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu}{\\sigma}.\n",
    "$$\n",
    "\n",
    "For our non-inferiority test:\n",
    "\n",
    "$$\n",
    "Z_{\\mathrm{NI}}\n",
    "= \\frac{\\hat{\\Delta} - E[\\Delta]_{H_{\\text{boundary}}}}{SE}\n",
    "= \\frac{\\hat{\\Delta} - (-\\epsilon)}{SE}\n",
    "= \\frac{\\hat{\\Delta} + \\epsilon}{SE}.\n",
    "$$\n",
    "\n",
    "Thus $Z_{\\mathrm{NI}}$ follows approximately a standard normal $N(0,1)$ under $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Tail Probability in Standard Normal Form\n",
    "\n",
    "We want the probability (under $H_0$) of observing something at least as extreme as our sample:\n",
    "\n",
    "$$\n",
    "P\\big[\\hat{\\Delta} \\ge \\hat{\\Delta}_{\\mathrm{obs}}\\big]\n",
    "= P\\!\\left[\\frac{\\hat{\\Delta} + \\epsilon}{SE}\n",
    "       \\ge \\frac{\\hat{\\Delta}_{\\mathrm{obs}}+\\epsilon}{SE}\\right].\n",
    "$$\n",
    "\n",
    "This is simply the **right-tail** probability of a standard normal:\n",
    "\n",
    "$$\n",
    "\\int_{Z_{\\mathrm{NI}}}^{+\\infty} \n",
    "\\frac{1}{\\sqrt{2\\pi}}\\,e^{-z^2/2}\\,dz.\n",
    "$$\n",
    "\n",
    "Using the standard normal survival function gives the same p-value as before ‚Äî this is just the ‚Äúclassical‚Äù z-score framing that many NHST tutorials use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zni = (hatDelta_observed + epsilon) / SE_H0\n",
    "p_zni = norm.sf(zni)\n",
    "print(f\"z_NI: {zni:.4f}, p-value: {p_zni:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same pvalue, just a different way to compute the integral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False Positive (a.k.a. Type I Error)\n",
    "\n",
    "In this NHST setup, the **alpha**  represents the cutoff for the decision rule (lower means we reject H0, it is the conditional probability of rejecting H0 while it is actually true. In this \"non inferiority\" setup the rejection is considered a \"postive\" as H0 is \"what we don't want to see\", so that is concluding ‚Äúno unacceptable degradation‚Äù while there is actually a nasty one.\n",
    "\n",
    "This conditional probability P(Reject H‚ÇÄ | H‚ÇÄ is true) defines the p-value that is P(data as extreme or more extreme | H‚ÇÄ is true); we reject H‚ÇÄ when p-value ‚â§ alpha\n",
    "\n",
    "\n",
    "By setting the significance level $\\alpha = 0.05$, we accept a **5% risk** of making this wrong decision when there is a degration. But note that this is a frequentist definition: if we ran the experiment many times,  on average we would incorrectly reject 5% of the time in this case. However it does assign any actual probablity to our current decision/experiment (Bayesian method do, see below).\n",
    "\n",
    "It also says nothing about the \"effect size\", that is about how much \"non-degradation/improvement\" we may have. For non-inferiority it does not matter too much but for superiority we would want to know, also something Bayesian approach can do more directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### False Negative (Type II Error), Power, and Sample Size\n",
    "\n",
    "The **false negative** ‚Äî is failing to reject $H_0$ when the alternative $H_1$ is actually true. \n",
    "\n",
    "In the context of a **non-inferiority test**, a false negative means:\n",
    "\n",
    "We fail the test (do **not** reject $H_0$) even though the new UX is truly **non-inferior** (as good or better than the old one). Usually it means we woud need to keep on gathering data until the test has more power to detect something (see below)\n",
    "\n",
    "---\n",
    "\n",
    "##### Choosing an Effect Size Under $H_1$\n",
    "\n",
    "Just as with the Type I error calculation, we need to pick an expected value for the difference $\\Delta$ ‚Äî but this time **under $H_1$**.\n",
    "\n",
    "- In practice, we must choose a **single reference value** to center the alternative distribution.  \n",
    "- A common (and pragmatic) choice is the **minimum effect size we care to detect** ‚Äî often set to $E[\\Delta] = 0$ (meaning *no difference* between variant and control).  \n",
    "  - If the variant is truly ‚Äúno worse‚Äù (Œî = 0), the test should reject $H_0$ most of the time.\n",
    "\n",
    "This choice is somewhat **arbitrary** and reflects a **business decision**: ‚ÄúHow small of a difference do we consider acceptable to detect?‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "##### Modeling Under $H_1$\n",
    "\n",
    "If we assume the variant is truly **no worse** (Œî = 0), we can pool samples to estimate the standard error (since under $H_1$ we‚Äôre treating them as coming from the same distribution):\n",
    "\n",
    "$$\n",
    "SE_{H_1}\n",
    "= \\text{WaldPooled SE}\n",
    "= \\sqrt{\\hat{p}_{\\mathrm{pool}}\n",
    "(1-\\hat{p}_{\\mathrm{pool}})\n",
    "\\left(\\tfrac{1}{n_C}+\\tfrac{1}{n_A}\\right)}.\n",
    "$$\n",
    "\n",
    "We then compare this **alternative distribution** (mean = 0, std = $SE_{H_1}$) to the **critical value** $c$ that was already set by the significance level $\\alpha$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Beta and Power\n",
    "\n",
    "- **$\\beta$ (Type II error)** = false negative rate , = probability that the observed statistic falls **below the critical value** that was established under H0, but with a different distribution with a mean is assumed the one we chose under $H_1$ (e.g., Œî = 0) and also the standard error establisehd under H1. Negaive here mean \"fail to reject\" while we truy had no inferiority.\n",
    "    \n",
    "- **Power** = $1-\\beta$ = probability of **correctly rejecting** $H_0$ when the variant is truly non-inferior. Another way to say this is: knowing the property we care about is really there (non-inferiority), what is our probability of detecting it. In machine learning and search queries analysis, Power is also called \"recall\", that is if it is the propoerty we care about is really there how often can we predict or find it.\n",
    "\n",
    "Graphically:  \n",
    "- The null distribution is centered at $-\\epsilon$ (our boundary).  \n",
    "- The alternative distribution is centered at $0$ (no degradation).  \n",
    "- $\\beta$ is the area of the alternative distribution **to the left of the critical value**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SE_H1 = wald_pooled_SE\n",
    "mu_H1 = 0\n",
    "sigma_H1 = SE_H1\n",
    "x = critical_value\n",
    "beta = norm.cdf(x, loc=mu_H1, scale=sigma_H1)\n",
    "print(f\"Observed probability of false negative a.k.a Œ≤ a.k.a type 2 errors,  at critical value : {beta:.4f}\")\n",
    "power = 1 - beta\n",
    "print(f\"Observed Power (1 - Œ≤): {power:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Type II error analysis\n",
    "\n",
    "# Create the plot using the helper function\n",
    "fig, ax = plot_type_ii_error_analysis(\n",
    "    mu_H1=mu_H1,\n",
    "    sigma_H1=sigma_H1,\n",
    "    critical_value=critical_value,\n",
    "    hatDelta_observed=hatDelta_observed,\n",
    "    epsilon=epsilon,\n",
    "    beta=beta,\n",
    "    power=power\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "##### Designing for Target Power\n",
    "\n",
    "If we want to achieve a **target power** ‚Äî commonly 80% (so $\\beta = 0.2$) ‚Äî  \n",
    "we can **solve for the required sample size** (embedded in $SE$).\n",
    "\n",
    "- Larger $n$ ‚Üí smaller $SE$ ‚Üí distributions separate more clearly ‚Üí higher power.\n",
    "- This is the usual **sample size calculation** step when planning an A/B test.\n",
    "\n",
    "In practice, one:\n",
    "1. Fixes $\\alpha$ (e.g., 0.05).\n",
    "2. Chooses the minimum effect size of interest (e.g., $\\Delta=0$ for non-inferiority).\n",
    "3. Sets desired power (e.g., 80%).\n",
    "4. Solves for $n_C$ and $n_A$ to achieve that power given the pooled variance.\n",
    "\n",
    "The code to solve to get a target beta can be developed easily, but since we will favor Bayesian approaches which can work with small samples, we will leave that TBD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compute sample size for our passkey example using the utility function\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE SIZE CALCULATION FOR NON-INFERIORITY TEST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Parameters from our example\n",
    "p_control = control_group_conversion_rate  \n",
    "epsilon_val = epsilon  # 0.03\n",
    "alpha_val = nhst_alpha  # 0.05\n",
    "target_power = 0.80\n",
    "\n",
    "print(f\"\\nParameters:\")\n",
    "print(f\"  Control conversion rate: {p_control:.2%}\")\n",
    "print(f\"  Non-inferiority margin (Œµ): {epsilon_val:.2%}\")\n",
    "print(f\"  Significance level (Œ±): {alpha_val:.2%}\")\n",
    "print(f\"  Target power: {target_power:.2%}\")\n",
    "print(f\"  Assumed true difference under H1: 0 (no difference)\")\n",
    "\n",
    "# Equal allocation (1:1)\n",
    "result_equal = compute_sample_size_non_inferiority(\n",
    "    p_control=p_control,\n",
    "    epsilon=epsilon_val,\n",
    "    alpha=alpha_val,\n",
    "    target_power=target_power,\n",
    "    h1_effect_size=0.0,\n",
    "    allocation_ratio=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EQUAL ALLOCATION (1:1 - Control:Variant)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Required sample size per group: {result_equal['n_variant']:,}\")\n",
    "print(f\"  Control: {result_equal['n_control']:,}\")\n",
    "print(f\"  Variant: {result_equal['n_variant']:,}\")\n",
    "print(f\"  Total: {result_equal['n_total']:,}\")\n",
    "print(f\"\\nAchieved power: {result_equal['power_achieved']:.4f} ({result_equal['power_achieved']*100:.1f}%)\")\n",
    "\n",
    "# Unequal allocation (10:1 - more traffic to control)\n",
    "result_unequal = compute_sample_size_non_inferiority(\n",
    "    p_control=p_control,\n",
    "    epsilon=epsilon_val,\n",
    "    alpha=alpha_val,\n",
    "    target_power=target_power,\n",
    "    h1_effect_size=0.0,\n",
    "    allocation_ratio=0.1  # Variant gets 10% of control's sample size\n",
    ")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"UNEQUAL ALLOCATION (10:1 - Control gets 10x more traffic)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Control: {result_unequal['n_control']:,}\")\n",
    "print(f\"  Variant: {result_unequal['n_variant']:,}\")\n",
    "print(f\"  Total: {result_unequal['n_total']:,}\")\n",
    "print(f\"\\nAchieved power: {result_unequal['power_achieved']:.4f} ({result_unequal['power_achieved']*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPARISON WITH CURRENT EXAMPLE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Current sample sizes:\")\n",
    "print(f\"  Control: {nC:,}\")\n",
    "print(f\"  Variant: {nA:,}\")\n",
    "print(f\"  Observed power: {power:.4f} ({power*100:.1f}%)\")\n",
    "print(f\"\\nTo achieve 80% power, you would need:\")\n",
    "print(f\"  Equal allocation: {result_equal['n_variant']:,} per group\")\n",
    "print(f\"  Increase factor: {result_equal['n_variant'] / nA:.1f}x more samples per group\")\n",
    "\n",
    "print(f\"\\nüí° KEY INSIGHT:\")\n",
    "print(f\"   With current n={nA}, power is only {power*100:.1f}%\")\n",
    "print(f\"   Need n‚âà{result_equal['n_variant']:,} per group for 80% power\")\n",
    "print(f\"   This is why NHST struggles with small samples!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##### NHST Confidence Interval (CI)\n",
    "\n",
    "NHST also has a notion of Confidence Interval but it is not what most people think it means. It does not say anything about the hypothesis or whether the observed value is close to the truth, or whether rejecting H0 is true or not. It is computed without using any of the hypotheses; it is just using the \"plugged in\" standard deviation SE (derived from the observation, with all the caveats we discussed) and says: if we repeated the experiment many times and constructed an interval each time using this procedure, 95% of those intervals would contain the true parameter value. The parameter is fixed; the interval is random across repeated experiments. Not useful to make any decisions directly. Some people use it indirectly by looking for overlaps between the interval and some key values, but that's just another way to do the p-value analysis as above, or \"picking the best variant\" below. The p-value computation is the same computation really and the mainstream way of doing it for NHST practitioners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Approach\n",
    "\n",
    "In contrast to NHST, the **Bayesian approach** is conceptually simpler:\n",
    "\n",
    "- Instead of fixing two specific expected values for $\\Delta$ (one under $H_0$ and one under $H_1$),\n",
    "- We treat the **true conversion difference** $E[\\Delta]$ itself as an **unknown random variable** and reason about its entire probability distribution.\n",
    "\n",
    "This lets us quantify directly how likely any value of $\\Delta$ is, given both prior knowledge and the data we observe.\n",
    "\n",
    "The typical workflow is\n",
    "\n",
    "1. Pick a prior belief, express it as a Beta distribution\n",
    "2. Run the experiment\n",
    "3. Use Bayes theorem to update the prior belief into a posterior belief\n",
    "4. Rinse and repeat as this is one of the strengths of the method; the posterior belief can be used as a new prior before running more experiments that will firm it up, if needed and with perfect mathematical rigor unlike NHST which does not allow peeking or stopping early because of the way sampling works.\n",
    "\n",
    "---\n",
    "\n",
    "#### Using the Beta Distribution for Our Prior Belief\n",
    "\n",
    "For experiments based on **Bernoulli trials** (success/failure, convert/abandon, etc.), the most convenient way to model our **prior belief** about a conversion rate is the **Beta distribution**.\n",
    "\n",
    "> ‚ö†Ô∏è Don‚Äôt confuse this **Beta** with the ‚Äú$\\beta$‚Äù from NHST (Type II error).  \n",
    "> Here, *Beta* is the name of a probability distribution.\n",
    "\n",
    "The Beta distribution:\n",
    "\n",
    "- Is defined on the interval $[0,1]$, making it perfect for modeling a **probability**.\n",
    "- Has two shape parameters, $\\alpha$ and $\\beta$, which control how strongly it reflects our prior knowledge.\n",
    "\n",
    "Some examples of possible priors:\n",
    "\n",
    "- **Uninformative prior:** $ \\mathrm{Beta}(1,1) $ ‚Äî essentially a uniform distribution, expressing ‚Äúwe know nothing.‚Äù\n",
    "- **Weakly informative prior:** centered roughly around 17‚Äì20% but without being very sure\n",
    "\n",
    "Here are a few graphical example of the Beta distribution for various values of $\\alpha$ and $\\beta$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison of different Beta prior distributions\n",
    "fig, axes = plot_beta_prior_comparison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formal definition of the **Beta distribution** looks a bit intimidating; both the numerator and denominator are related to the probability (binomial) when the probability of having m successes out of n trials (binomial) where the basic event probability is set to x.\n",
    "Intuition: $\\alpha - 1$ and $\\beta - 1$ act like prior pseudo-counts of successes m and failures n-m. After observing data, you add the real counts.\n",
    "Special case (uniform prior): Beta(1,1) ‚áí posterior Beta(m + 1, n - m + 1).\n",
    "\n",
    "$$ \n",
    "\n",
    "f(x, \\alpha, \\beta) = \\frac{x^{\\alpha-1}(1-x)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "\n",
    "$$\n",
    "\n",
    "with B being the **Beta function B** defined as normalizing constant:\n",
    "\n",
    "$$\n",
    "\n",
    "B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1} (1-t)^{\\beta-1} dt\n",
    "\n",
    "$$\n",
    "\n",
    "There is no easy way to work with this formula \"by hand\" and that is one of the reasons Bayesian approaches were historically impractical as you cannot work with those \"by hand\" like with a Gaussian function. But now that we have stats packages in Python it is trivial to use. Here are some examples of the shapes it can take by picking different values of $\\alpha$ and $\\beta$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conservative Approach: Assuming We Know Nothing (Non-Informative Prior)\n",
    "\n",
    "For the **variant A**, suppose we start with a **non-informative prior** ‚Äî meaning we have no knowledge about the conversion rate $p_A$.  \n",
    "We assume $p_A$ could be anywhere between $0$ and $1$ with equal probability.  \n",
    "This is modeled by the Beta distribution:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(1,1)\n",
    "$$\n",
    "\n",
    "‚Äî which is just a **uniform prior** (flat line) on $[0,1]$.\n",
    "\n",
    "---\n",
    "\n",
    "##### Posterior After Observing Data\n",
    "\n",
    "After running the experiment with:\n",
    "\n",
    "- $n_A$ = number of trials (users shown variant A),\n",
    "- $x_A$ = number of successes (conversions),\n",
    "\n",
    "the **posterior** distribution for $p_A$ ‚Äî thanks to the conjugacy of the Beta with the Bernoulli likelihood  is:\n",
    "\n",
    "$$\n",
    "\\mathrm{Beta}(x_A+1,\\; n_A - x_A + 1).\n",
    "$$\n",
    "\n",
    "This follows directly from **Bayes‚Äô theorem** and the properties of the Beta distribution.\n",
    "\n",
    "---\n",
    "\n",
    "##### Expected Value of the Posterior\n",
    "\n",
    "For any $\\mathrm{Beta}(\\alpha,\\beta)$ distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_value_posterior = (xA_observed + 1) / (nA + 2)\n",
    "print(f\"Expected value of posterior distribution for p_A: {expected_value_posterior:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credible Intervals and Visualizing Prior vs. Posterior\n",
    "\n",
    "A **credible interval** answers: *‚ÄúGiven the data and our prior, what range of parameter values has (say) 95% posterior probability?‚Äù*   \n",
    "\n",
    "\n",
    "Note that this is completely different than the confidence interval of NHST although it sounds kind of the same, it is not, this gives us a direct probability of the truth of our hypothesis we can say \"we know that the true conversion rate is between a and b with x% probability. We can pick whatever x we want and we will get an interval\n",
    "\n",
    "So let's say we use **equal-tailed** credible intervals (quantiles at 2.5% and 97.5%).\n",
    "\n",
    "\n",
    "- **Prior** (uninformative): $\\mathrm{Beta}(1,1)$  \n",
    "- **Posterior** after observing $x_A$ conversions out of $n_A$:  \n",
    "  $\\alpha_A = x_A+1,\\; \\beta_A = n_A - x_A + 1$\n",
    "\n",
    "We can compute these intervals and plot how the **posterior** updates our belief compared with the **prior**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior parameters\n",
    "nhst_alpha = xA_observed + 1\n",
    "beta_param = nA - xA_observed + 1\n",
    "\n",
    "\n",
    "\n",
    "# Compute 95% credible interval (2.5th and 97.5th percentiles)\n",
    "p_L = beta_dist.ppf(0.025, nhst_alpha, beta_param)\n",
    "p_U = beta_dist.ppf(0.975, nhst_alpha, beta_param)\n",
    "\n",
    "# Output the result\n",
    "print(f\"95% Credible Interval for p: [{p_L:.4f}, {p_U:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know with 95% chance of being true that the true conversion rate is between 16.13% and 29.30%. Here is a visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize non-informative prior vs posterior\n",
    "fig, ax = plot_prior_vs_posterior(\n",
    "    alpha=nhst_alpha,\n",
    "    beta_param=beta_param,\n",
    "    control_group_conversion_rate=control_group_conversion_rate,\n",
    "    epsilon=epsilon,\n",
    "    p_L=p_L,\n",
    "    p_U=p_U\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first version we assumed that \"we know nothing\" prior to the experiment, which is not realistic. We know we added a page with an extra click, so unless there is a serious bug the conversion should be \"around\" the control which has a historical mean of 20%, but we want to be open to the possibility of small deviation around this historical value. We can do this with what is known as a weakly informative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weakly Informative Prior Using the Control Group as the base for the prior\n",
    "\n",
    "Instead of using a completely non-informative prior $\\mathrm{Beta}(1,1)$, we can use the control group as we can see that the variants operate in the same range. That's a sound metholdology because in an web/mobile envrionment with frequent releases, seasonal shift in usage patterns, marketing campaigns, etc.. an historical prior is usually not a good defijtion of the \"legacy\" we want to compare the variant with. Only the control group give a us a clean comparable.\n",
    "\n",
    "Suppose we know the control group conversion rate is in the variable **control_group_conversion_rate** , and we want to test for **non-inferiority** with a margin **$\\epsilon$**.\n",
    "\n",
    "For the variant, we choose a prior **centered** on what we expect the new mean to be  **control_group_conversion_rate - $\\epsilon$** i.e.  but we want this prior to have **high entropy** (wide uncertainty) so it does not dominate the data.  \n",
    "\n",
    "A **Beta** distribution with a modest $\\alpha$ and $\\beta$ can be informative about the center while still uncertain.\n",
    "\n",
    "For a $\\mathrm{Beta}(\\alpha,\\beta)$ distribution:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{\\alpha}{\\alpha+\\beta}\n",
    "$$\n",
    "\n",
    "- Smaller values of $\\alpha$ and $\\beta$ give a **wider** (more uncertain) prior.\n",
    "\n",
    "Let‚Äôs pick $\\alpha = 20$ and solve for $\\beta$ so the mean is **control_group_conversion_rate - $\\epsilon$** Then recompute the posterior distribution but with this prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informative prior parameters\n",
    "expected_degradation = 0.01 # we expect conversion to be a bit lower, but it has to be above the non-inferiority margin epsilon\n",
    "target_prior_mean = control_group_conversion_rate  - expected_degradation\n",
    "alpha_prior = 20 # Small value for high entropy\n",
    "beta_prior = (alpha_prior / target_prior_mean) - alpha_prior  # Solve for beta given mean\n",
    "\n",
    "print(f\"Prior: Beta({alpha_prior:.2f}, {beta_prior:.2f})\")\n",
    "print(f\"Prior mean: {alpha_prior / (alpha_prior + beta_prior):.4f}\")\n",
    "print(f\"Prior variance: {(alpha_prior * beta_prior) / ((alpha_prior + beta_prior)**2 * (alpha_prior + beta_prior + 1)):.6f}\")\n",
    "\n",
    "# Posterior parameters after observing data\n",
    "alpha_posterior = xA_observed + alpha_prior\n",
    "beta_posterior = (nA - xA_observed) + beta_prior\n",
    "\n",
    "print(f\"\\nPosterior: Beta({alpha_posterior:.2f}, {beta_posterior:.2f})\")\n",
    "posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "print(f\"Posterior mean: {posterior_mean:.4f}\")\n",
    "\n",
    "# Compute probability that variant is non-inferior (p_A > p_C - epsilon)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "non_inferiority_threshold = control_group_conversion_rate - epsilon\n",
    "prob_non_inferior = 1 - beta_dist.cdf(non_inferiority_threshold, alpha_posterior, beta_posterior)\n",
    "\n",
    "print(f\"\\nProbability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probability that the variant conversion rate is above {non_inferiority_threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here unlike the NHST, thanks to adding a reasonable prior that uses our understanding of how the CX for passkey is built, we end up having something actionable. We just have an above 95% probability of being over the cutoff so we are non-inferior, good to go.\n",
    "\n",
    "Here is a diagram of this prior and the posterior after observing data $(x_A, n_A)$ for the variant, and various credible intervals. For non-inferiority we don't even need the credible interval (see next).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute probability that variant is non-inferior (p_A > p_C - expected_degradation)\n",
    "# This is P(p_A > 0.17) under the posterior\n",
    "prob_non_inferior = 1 - beta_dist.cdf(control_group_conversion_rate - expected_degradation, \n",
    "                                       alpha_posterior, beta_posterior)\n",
    "print(f\"Probability that variant is non-inferior: {prob_non_inferior:.4f}\")\n",
    "print(f\"This means there's a {prob_non_inferior*100:.2f}% probability that the variant conversion rate is above {control_group_conversion_rate - epsilon:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_data = test_non_inferiority_weakly_informative(\n",
    "    n_control=nC,\n",
    "    x_control=xC_observed,\n",
    "    variants_data=variants,\n",
    "    epsilon=epsilon,\n",
    "    alpha_prior_strength=20,\n",
    ")\n",
    "print(f'probability that variant A is non-inferior: {posterior_data[\"A\"][\"probability\"]:.4f} ')\n",
    "posterior_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior vs Posterior with non-inferiority tail area (P(p_A > p_C - Œµ))\n",
    "\n",
    "threshold = control_group_conversion_rate - epsilon\n",
    "\n",
    "# Create the plot using the helper function\n",
    "fig, ax, prob_non_inferior_post, prob_non_inferior_prior = plot_informative_prior_posterior_comparison(\n",
    "    alpha_prior=alpha_prior,\n",
    "    beta_prior=beta_prior,\n",
    "    alpha_posterior=alpha_posterior,\n",
    "    beta_posterior=beta_posterior,\n",
    "    threshold=threshold\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Posterior P(p_A > {threshold:.2f}) = {prob_non_inferior_post:.4f} \"\n",
    "      f\"({prob_non_inferior_post*100:.2f}%)\")\n",
    "print(f\"Prior     P(p_A > {threshold:.2f}) = {prob_non_inferior_prior:.4f} \"\n",
    "      f\"({prob_non_inferior_prior*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plot_weakly_informative_prior_with_variants(variants_results=posterior_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking the Best Variant\n",
    "\n",
    "This is where the difference between **NHST** and a **Bayesian** approach becomes dramatic.  \n",
    "Let‚Äôs compare the main options.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ NHST Approaches\n",
    "\n",
    "\n",
    "#### 1. Winner-Takes-All\n",
    "- Pick the variant with the highest observed conversion rate.\n",
    "\n",
    "**Problems:**  \n",
    "- Ignores uncertainty and sampling noise.  \n",
    "- Easily picks the wrong variant when samples are small.\n",
    "\n",
    "#### 2. Pairwise *t*-Tests with Bonferroni Correction\n",
    "- Run one test for every pair (A vs B, A vs C, B vs C).  \n",
    "- Adjust the significance threshold to control false positives:  \n",
    "  $$\\alpha_\\text{corrected} = \\frac{0.05}{3} \\approx 0.0167.$$\n",
    "\n",
    "**Problems:**  \n",
    "- Multiple comparisons inflate Type I error; Bonferroni is very conservative (higher Type II error).  \n",
    "- Only gives ‚Äúsignificant / not significant‚Äù ‚Äî no direct probability of being best.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. ANOVA + Post-Hoc Tests\n",
    "- One-way ANOVA checks if *any* difference exists, then post-hoc tests (Tukey, Dunnett, etc.) try to find which.\n",
    "\n",
    "**Problems:**  \n",
    "- Still needs multiple-comparison corrections.  \n",
    "- ANOVA only says ‚Äúsomething differs‚Äù ‚Äî not which is best or by how much.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Confidence Interval Overlap\n",
    "- Compute 95% CIs for each variant and check for overlap.\n",
    "\n",
    "**Problems:**  \n",
    "- Overlapping CIs don‚Äôt mean ‚Äúno difference.‚Äù  \n",
    "- Often inconclusive and gives no probability a variant is best.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Key Takeaway\n",
    "\n",
    "**NHST was designed for asymmetric questions:**\n",
    "- \"Is this drug better than placebo?\" (directional)\n",
    "- \"Does this treatment have an effect?\" (vs. no effect)\n",
    "\n",
    "**NHST struggles with symmetric questions:**\n",
    "- \"Which variant is better, A or B?\" (symmetric)\n",
    "- Forces arbitrary directionality or multiple-testing corrections\n",
    "- Cannot directly compute P(A > B | data)\n",
    "\n",
    "**Bayesian methods naturally handle symmetric comparisons:**\n",
    "- Compute posterior for each variant\n",
    "- Directly calculate P(A > B), P(B > A), P(C is best), etc.\n",
    "- No need for multiple-testing corrections\n",
    "- Scales to any number of variants\n",
    "- Provides actionable probabilities for decision-making\n",
    "\n",
    "This is why **Bayesian methods are superior for A/B testing** where the goal is to pick the best variant, not just detect if one exists.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary: Why NHST Struggles with Symmetric A vs B Comparisons\n",
    "\n",
    "All NHST approaches share fundamental limitations:\n",
    "\n",
    "| Limitation | Impact |\n",
    "|------------|--------|\n",
    "| **Computes P(data \\| hypothesis)** | Not what we want: P(hypothesis \\| data) |\n",
    "| **Binary decisions** | Reject/fail-to-reject; no probability of being better |\n",
    "| **Asymmetric framework** | Must pick a direction or waste Œ± budget |\n",
    "| **No direct answer** | Cannot directly answer \"Which is better?\" |\n",
    "| **No expected value** | Cannot compute expected value for decision-making |\n",
    "\n",
    "**What we actually want:**\n",
    "- P(A > B | data) ‚Äî direct probability A is better\n",
    "- P(B > A | data) ‚Äî direct probability B is better  \n",
    "- Symmetric treatment of both variants\n",
    "- Actionable metric for decision-making\n",
    "\n",
    "**The Bayesian approach provides exactly this.** Let's demonstrate:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåü Bayesian Approach ‚Äî Probability of Being Best\n",
    "\n",
    "The Bayesian framework answers the question we actually care about:  \n",
    "> *Which variant is most likely the best?*\n",
    "\n",
    "**Method:**\n",
    "1. Compute the **posterior Beta distribution** for each variant using its prior and observed data.  \n",
    "2. Draw a large number of samples (e.g., 100k) from each posterior.  \n",
    "3. For each simulated draw, identify which variant has the highest conversion rate.  \n",
    "4. Report the probabilities:  \n",
    "   $$P(A \\text{ is best}),\\; P(B \\text{ is best}),\\; P(C \\text{ is best}), \\ldots$$\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages:**\n",
    "-  **Direct answer:** ‚ÄúVariant B is best with 88.8% probability.‚Äù  \n",
    "-  **Single coherent analysis:** no need for multiple-comparison corrections.  \n",
    "-  **Scales naturally:** works the same way for 3, 5, 10, or 100 variants.  \n",
    "-  **Quantifies uncertainty:** not just yes/no; can report $P(B>A)$, $P(B>A \\;\\&\\; B>C)$, etc.  \n",
    "-  **Flexible:** easily integrates prior knowledge and business context.  \n",
    "-  **Business-friendly:** simple to factor in risk, cost, and implementation difficulty.  \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Key point:**  \n",
    "Bayesian analysis gives a **probability each variant is best** ‚Äî a direct, interpretable metric that scales cleanly and supports real-world decision making.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"\\nVariant conversion rates:\")\n",
    "for name, data in variants.items():\n",
    "    rate = data['x'] / data['n']\n",
    "    print(f\"  {name}: {rate:.4f} ({data['x']}/{data['n']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variants are above non-inferiority boundary: {control_group_conversion_rate - epsilon:.2f}\n",
    "But which one should we choose?\n",
    "In this specific case because all samples have the same size (n), we can just compare the mean but the \"clean\" way to do it is to run a Monte Carlo simulation to see which Variant would win \"in all possible universes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior distributions (using non-informative prior Beta(1,1))\n",
    "posteriors = {}\n",
    "\n",
    "print(\"\\nPosterior Distributions:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, data in variants.items():\n",
    "    # Posterior parameters (with non-informative prior Beta(1,1))\n",
    "    alpha_post = data['x'] + 1\n",
    "    beta_post = data['n'] - data['x'] + 1\n",
    "    \n",
    "    # Posterior statistics\n",
    "    posterior_mean = alpha_post / (alpha_post + beta_post)\n",
    "    posterior_var = (alpha_post * beta_post) / \\\n",
    "                    ((alpha_post + beta_post)**2 * (alpha_post + beta_post + 1))\n",
    "    posterior_std = np.sqrt(posterior_var)\n",
    "    \n",
    "    # Credible intervals\n",
    "    ci_95_lower = beta_dist.ppf(0.025, alpha_post, beta_post)\n",
    "    ci_95_upper = beta_dist.ppf(0.975, alpha_post, beta_post)\n",
    "    \n",
    "    posteriors[name] = {\n",
    "        'alpha': alpha_post,\n",
    "        'beta': beta_post,\n",
    "        'mean': posterior_mean,\n",
    "        'std': posterior_std,\n",
    "        'ci_95': (ci_95_lower, ci_95_upper)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nVariant {name}:\")\n",
    "    print(f\"  Posterior: Beta(Œ±={alpha_post}, Œ≤={beta_post})\")\n",
    "    print(f\"  Posterior mean: {posterior_mean:.4f}\")\n",
    "    print(f\"  Posterior std: {posterior_std:.4f}\")\n",
    "    print(f\"  95% Credible Interval: [{ci_95_lower:.4f}, {ci_95_upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of the three posterior distributions\n",
    "fig, ax = plot_multiple_posteriors_comparison(\n",
    "    posteriors=posteriors,\n",
    "    control_group_conversion_rate=control_group_conversion_rate,\n",
    "    epsilon=epsilon\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì All three posterior distributions overlap significantly\")\n",
    "print(\"  This shows there's uncertainty about which is truly best\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulation\n",
    "n_simulations = 100000\n",
    "print(f\"Running {n_simulations:,} simulations...\\n\")\n",
    "\n",
    "# Draw samples from each posterior\n",
    "samples = {}\n",
    "for name in ['A', 'B', 'C']:\n",
    "    alpha_p = posteriors[name]['alpha']\n",
    "    beta_p = posteriors[name]['beta']\n",
    "    samples[name] = beta_dist.rvs(alpha_p, beta_p, size=n_simulations)\n",
    "\n",
    "# For each simulation, determine which variant is best\n",
    "best_counts = {'A': 0, 'B': 0, 'C': 0}\n",
    "\n",
    "for i in range(n_simulations):\n",
    "    # Get the sampled values for this simulation\n",
    "    sample_values = {\n",
    "        'A': samples['A'][i],\n",
    "        'B': samples['B'][i],\n",
    "        'C': samples['C'][i]\n",
    "    }\n",
    "    \n",
    "    # Find which variant has the highest value in this simulation\n",
    "    best_variant = max(sample_values, key=lambda k: sample_values[k])\n",
    "    best_counts[best_variant] += 1\n",
    "\n",
    "# Calculate probabilities\n",
    "probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "\n",
    "print(\"RESULTS: Probability Each Variant is Best\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name in ['A', 'B', 'C']:\n",
    "    prob = probabilities[name]\n",
    "    bar = '‚ñà' * int(prob * 60)\n",
    "    print(f\"P({name} is best) = {prob:.4f} ({prob*100:5.2f}%) {bar}\")\n",
    "\n",
    "# Determine the winner\n",
    "winner = max(probabilities, key=probabilities.get)\n",
    "winner_prob = probabilities[winner]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BAYESIAN CONCLUSION:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Variant {winner} is most likely the best\")\n",
    "print(f\"  Probability: {winner_prob:.4f} ({winner_prob*100:.1f}%)\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - There's a {winner_prob*100:.1f}% chance that {winner} has the highest true conversion rate\")\n",
    "print(f\"  - This accounts for uncertainty in all three estimates\")\n",
    "print(f\"  - Clear, actionable decision with quantified confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Standalone function to compute non inferiorit and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"Bayesian utility functions imported:\")\n",
    "print(\"  - test_non_inferiority()\")\n",
    "print(\"  - select_best_variant()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NHST utility functions\n",
    "from nhst import nhst_non_inferiority_test, compute_sample_size_non_inferiority\n",
    "\n",
    "print(\"NHST utility functions imported:\")\n",
    "print(\"  - nhst_non_inferiority_test()\")\n",
    "print(\"  - compute_sample_size_non_inferiority()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
