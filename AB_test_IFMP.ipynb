{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian A/B Test Workflow\n",
    "\n",
    "This notebook demonstrates a complete Bayesian A/B testing workflow using utility functions.\n",
    "\n",
    "## Workflow Overview\n",
    "\n",
    "1. **Setup**: Define experiment data (control + variants)\n",
    "2. **Non-Inferiority Test**: Verify variants don't degrade performance\n",
    "3. **Visualize Results**: Plot prior and posteriors for all variants\n",
    "4. **Select Best Variant**: Choose the winning variant with probability\n",
    "5. **Visualize Selection**: Compare all variant posteriors\n",
    "\n",
    "## Key Advantages of Bayesian Approach\n",
    "\n",
    "- ✅ Works with small, unbalanced samples\n",
    "- ✅ Provides actionable probabilities (not just p-values)\n",
    "- ✅ Scales to many variants effortlessly\n",
    "- ✅ Allows continuous monitoring without p-hacking concerns\n",
    "- ✅ Directly answers: \"Which variant is best?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Define Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Bayesian utility functions\n",
    "from bayesian import (\n",
    "    test_non_inferiority_weakly_informative,\n",
    "    select_best_variant\n",
    ")\n",
    "\n",
    "# Import plotting utilities\n",
    "from plotting_utils import (\n",
    "    plot_weakly_informative_prior_with_variants,\n",
    "    plot_multiple_posteriors_comparison\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Experiment Data\n",
    "\n",
    "Example: Testing 3 passkey creation UX variants against a control.\n",
    "\n",
    "- **Control**: Current experience (~71% completion rate)\n",
    "- **Variants A, B, C**: New passkey creation flows\n",
    "- **Goal**: Ensure variants don't degrade completion, then pick the best one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control group data\n",
    "n_control = 10000  # Number of users\n",
    "x_control = 3500  # Number who completed\n",
    "control_rate = x_control / n_control\n",
    "\n",
    "print(f\"Control Group:\")\n",
    "print(f\"  Sample size: {n_control:,}\")\n",
    "print(f\"  Conversions: {x_control:,}\")\n",
    "print(f\"  Conversion rate: {control_rate:.2%}\")\n",
    "\n",
    "# Variant data\n",
    "variants_data = {\n",
    "    'A': {'n': 17581, 'x': 6272},\n",
    "    'B': {'n': 13841, 'x': 1411},\n",
    "    'C': {'n': 14317, 'x': 3511}\n",
    "}\n",
    "\n",
    "print(f\"\\nVariants:\")\n",
    "for name, data in variants_data.items():\n",
    "    rate = data['x'] / data['n']\n",
    "    print(f\"  {name}: n={data['n']:3d}, x={data['x']:3d}, rate={rate:.2%}\")\n",
    "\n",
    "# Test parameters\n",
    "epsilon = 0.02  # 2% non-inferiority margin (acceptable degradation, business decision)\n",
    "print(f\"\\nNon-inferiority margin (ε): {epsilon:.1%}\")\n",
    "print(f\"Non-inferiority threshold: {control_rate - epsilon:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Non-Inferiority Test\n",
    "\n",
    "Test whether each variant is **non-inferior** (not significantly worse) than control.\n",
    "\n",
    "### Key Insight: Domain Knowledge vs Business Tolerance\n",
    "\n",
    "The test separates two important concepts:\n",
    "1. **Expected degradation** (domain knowledge): \"Adding 2 extra clicks will degrade by ~2%\"\n",
    "2. **Business tolerance** (epsilon): \"We can accept up to 5% degradation\"\n",
    "\n",
    "The prior is centered at your **expected** performance (domain knowledge), while the test checks against the **maximum acceptable** threshold (business requirement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain knowledge: Guestimate that adding passkey creation (2 extra clicks) will degrade by ~2%, \n",
    "# we use it to center our prior so this is not a threshold at all, just a prior belief which will change after seeing data. \n",
    "# we can play with the value but it has to ve be less than epsilon otherwise it woudl mean we would\n",
    "# know even before testing that this is inferiro and there woud be no point testing.\n",
    "expected_degradation = 0.01\n",
    "\n",
    "# Run non-inferiority test\n",
    "results = test_non_inferiority_weakly_informative(\n",
    "    n_control=n_control,\n",
    "    x_control=x_control,\n",
    "    variants_data=variants_data,\n",
    "    epsilon=epsilon,  # Business: can tolerate 5% degradation\n",
    "    expected_degradation=expected_degradation,  # Domain: expect 2% degradation\n",
    "    alpha_prior_strength=20,  # Weak prior (high entropy)\n",
    "    threshold=0.95  # 95% probability required to be sure, can adjust based on risk tolerance\n",
    ")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PRIOR AND THRESHOLD SETUP\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Control rate: {control_rate:.2%}\")\n",
    "print(f\"Expected degradation (domain knowledge): {expected_degradation:.1%}\")\n",
    "print(f\"  → Prior centered at: {control_rate - expected_degradation:.2%}\")\n",
    "print(f\"Maximum acceptable degradation (business): {epsilon:.1%}\")\n",
    "print(f\"  → Test threshold at: {control_rate - epsilon:.2%}\")\n",
    "print(f\"\\nThis means:\")\n",
    "print(f\"  • Prior says: 'I expect variant around {control_rate - expected_degradation:.1%}'\")\n",
    "print(f\"  • Test says: 'Must be above {control_rate - epsilon:.1%} to pass'\")\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NON-INFERIORITY TEST RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for variant_name, result in results.items():\n",
    "    status = \"✓ NON-INFERIOR\" if result['is_non_inferior'] else \"✗ NOT NON-INFERIOR\"\n",
    "    print(f\"\\nVariant {variant_name}: {status}\")\n",
    "    print(f\"  P(variant > threshold): {result['probability']:.2%}\")\n",
    "    print(f\"  Posterior mean: {result['variant_rate']:.2%}\")\n",
    "    print(f\"  Prior mean: {result['prior_mean']:.2%}\")\n",
    "    print(f\"  Observed rate: {variants_data[variant_name]['x']/variants_data[variant_name]['n']:.2%}\")\n",
    "\n",
    "# Summary\n",
    "non_inferior_count = sum(1 for r in results.values() if r['is_non_inferior'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Summary: {non_inferior_count}/{len(variants_data)} variants are non-inferior\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Non-Inferiority Test\n",
    "\n",
    "Plot shows:\n",
    "- **Gray dashed line**: Common weakly informative prior\n",
    "- **Colored lines**: Posterior distribution for each variant\n",
    "- **Red dotted line**: Non-inferiority threshold\n",
    "- **Black dash-dot line**: Control conversion rate\n",
    "- **Text box**: Probability each variant exceeds threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization - simplified usage!\n",
    "fig, ax = plot_weakly_informative_prior_with_variants(results)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Best Variant\n",
    "\n",
    "Among non-inferior variants, which one is most likely the best?\n",
    "\n",
    "Uses Monte Carlo simulation to compute **P(variant is best)** for each variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best variant among all (or filter to non-inferior only)\n",
    "# For this example, we'll analyze all variants\n",
    "selection_results = select_best_variant(\n",
    "    variants_data=variants_data,\n",
    "    alpha_prior=1,  # Non-informative prior for selection\n",
    "    beta_prior=1,\n",
    "    credible_level=0.95,\n",
    "    n_simulations=100000\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*80)\n",
    "print(\"BEST VARIANT SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nProbability each variant is best:\")\n",
    "for name, prob in selection_results['probabilities'].items():\n",
    "    bar = '█' * int(prob * 60)\n",
    "    print(f\"  {name}: {prob:.2%} {bar}\")\n",
    "\n",
    "winner = selection_results['best_variant']\n",
    "winner_prob = selection_results['probabilities'][winner]\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"WINNER: Variant {winner}\")\n",
    "print(f\"  Probability of being best: {winner_prob:.2%}\")\n",
    "print(f\"  Posterior mean: {selection_results['posterior_means'][winner]:.2%}\")\n",
    "print(f\"  95% Credible interval: [{selection_results['credible_intervals'][winner][0]:.2%}, {selection_results['credible_intervals'][winner][1]:.2%}]\")\n",
    "print(f\"  Expected loss: {selection_results['expected_loss'][winner]:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Variant Comparison\n",
    "\n",
    "Compare posterior distributions of all variants to see overlap and separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare posteriors for plotting\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "posteriors = {}\n",
    "for name, data in variants_data.items():\n",
    "    # Using non-informative prior Beta(1,1) for fair comparison\n",
    "    alpha_post = data['x'] + 1\n",
    "    beta_post = data['n'] - data['x'] + 1\n",
    "    \n",
    "    posteriors[name] = {\n",
    "        'alpha': alpha_post,\n",
    "        'beta': beta_post,\n",
    "        'mean': alpha_post / (alpha_post + beta_post),\n",
    "        'ci_95': (\n",
    "            beta_dist.ppf(0.025, alpha_post, beta_post),\n",
    "            beta_dist.ppf(0.975, alpha_post, beta_post)\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Create comparison plot\n",
    "fig, ax = plot_multiple_posteriors_comparison(\n",
    "    posteriors=posteriors,\n",
    "    control_group_conversion_rate=control_rate,\n",
    "    epsilon=epsilon\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This workflow demonstrates:\n",
    "\n",
    "1. ✅ **Non-Inferiority Testing**: Verify variants don't significantly degrade performance\n",
    "   - Uses weakly informative prior based on control data\n",
    "   - Provides direct probability: P(variant > threshold)\n",
    "   - Works with small, unbalanced samples\n",
    "\n",
    "2. ✅ **Variant Selection**: Choose the best performing variant\n",
    "   - Direct answer: P(variant A is best), P(variant B is best), etc.\n",
    "   - No multiple comparison corrections needed\n",
    "   - Scales to any number of variants\n",
    "\n",
    "3. ✅ **Actionable Results**: Business-friendly outputs\n",
    "   - \"Variant B is best with 47% probability\"\n",
    "   - \"Expected loss if we choose C instead: 0.0023\"\n",
    "   - Clear decision-making support\n",
    "\n",
    "## Key Takeaway\n",
    "\n",
    "Bayesian methods provide:\n",
    "- **Faster decisions** (works with small samples)\n",
    "- **Better interpretability** (probabilities, not p-values)\n",
    "- **Greater flexibility** (any sample sizes, multiple variants)\n",
    "- **Continuous monitoring** (no p-hacking issues)\n",
    "\n",
    "For modern product development with rapid iteration and risk-averse traffic allocation, Bayesian methods are superior to traditional NHST approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions Used in the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian utilities for A/B testing.\n",
    "\n",
    "This module contains Bayesian methods for non-inferiority testing,\n",
    "variant selection, and conversion rate analysis using Beta-Bernoulli\n",
    "conjugate models with Monte Carlo simulation.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import beta as beta_dist\n",
    "\n",
    "\n",
    "def test_non_inferiority(\n",
    "    n_control,\n",
    "    x_control,\n",
    "    variants_data,\n",
    "    epsilon,\n",
    "    alpha_prior,\n",
    "    beta_prior,\n",
    "    threshold=0.95,\n",
    "):\n",
    "    \"\"\"\n",
    "    Test non-inferiority of multiple variants against a control.\n",
    "\n",
    "    Uses Bayesian Beta-Bernoulli conjugate model with Monte Carlo simulation\n",
    "    to compute the probability that each variant's conversion rate is within\n",
    "    an acceptable degradation margin of the control.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_control : int\n",
    "        Number of samples in control group\n",
    "    x_control : int\n",
    "        Number of successes in control group\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 1000, 'x': 200}, 'B': {'n': 1000, 'x': 215}}\n",
    "    epsilon : float\n",
    "        Non-inferiority margin (e.g., 0.03 for 3%)\n",
    "    alpha_prior : float\n",
    "        Alpha parameter for Beta prior\n",
    "    beta_prior : float\n",
    "        Beta parameter for Beta prior\n",
    "    threshold : float, optional\n",
    "        Probability threshold for declaring non-inferiority (default: 0.95)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict : Dictionary with results for each variant containing:\n",
    "        - 'is_non_inferior': bool, whether variant is non-inferior\n",
    "        - 'probability': float, P(variant > control - epsilon)\n",
    "        - 'control_rate': float, posterior mean of control\n",
    "        - 'variant_rate': float, posterior mean of variant\n",
    "        - 'posterior_params': tuple, (alpha, beta) of variant posterior\n",
    "\n",
    "    \"\"\"\n",
    "    # Control posterior\n",
    "    alpha_control = x_control + alpha_prior\n",
    "    beta_control = n_control - x_control + beta_prior\n",
    "    control_rate = alpha_control / (alpha_control + beta_control)\n",
    "\n",
    "    # Boundary for non-inferiority\n",
    "    boundary = control_rate - epsilon\n",
    "\n",
    "    results = {}\n",
    "    n_simulations = 100000\n",
    "\n",
    "    # Sample from control posterior once (reuse for all variants)\n",
    "    control_samples = beta_dist.rvs(alpha_control, beta_control, size=n_simulations)\n",
    "\n",
    "    for variant_name, data in variants_data.items():\n",
    "        # Variant posterior\n",
    "        alpha_variant = data[\"x\"] + alpha_prior\n",
    "        beta_variant = data[\"n\"] - data[\"x\"] + beta_prior\n",
    "        variant_rate = alpha_variant / (alpha_variant + beta_variant)\n",
    "\n",
    "        # Sample from variant posterior\n",
    "        variant_samples = beta_dist.rvs(alpha_variant, beta_variant, size=n_simulations)\n",
    "\n",
    "        # Compute P(variant > control - epsilon)\n",
    "        prob_non_inferior = np.mean(variant_samples > (control_samples - epsilon))\n",
    "\n",
    "        results[variant_name] = {\n",
    "            \"is_non_inferior\": prob_non_inferior >= threshold,\n",
    "            \"probability\": prob_non_inferior,\n",
    "            \"control_rate\": control_rate,\n",
    "            \"variant_rate\": variant_rate,\n",
    "            \"posterior_params\": (alpha_variant, beta_variant),\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_non_inferiority_weakly_informative(\n",
    "    n_control,\n",
    "    x_control,\n",
    "    variants_data,\n",
    "    epsilon,\n",
    "    expected_degradation=None,\n",
    "    alpha_prior_strength=20,\n",
    "    threshold=0.95,\n",
    "):\n",
    "    \"\"\"\n",
    "    Test non-inferiority using weakly informative prior based on domain knowledge.\n",
    "\n",
    "    This function constructs a weakly informative prior centered at your expected\n",
    "    variant performance (based on domain knowledge), then tests against a separate\n",
    "    non-inferiority threshold (based on business requirements).\n",
    "\n",
    "    Key insight: Prior belief (where you expect the variant to be) is SEPARATE from\n",
    "    the decision threshold (worst acceptable performance).\n",
    "\n",
    "    The prior is constructed as:\n",
    "    - α_prior = alpha_prior_strength (default: 20, for high entropy/wide uncertainty)\n",
    "    - β_prior = (α_prior / target_mean) - α_prior\n",
    "    - target_mean = control_rate - expected_degradation\n",
    "\n",
    "    The test threshold is:\n",
    "    - non_inferiority_threshold = control_rate - epsilon\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_control : int\n",
    "        Number of samples in control group\n",
    "    x_control : int\n",
    "        Number of successes in control group\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 561, 'x': 381}, 'B': {'n': 285, 'x': 192}}\n",
    "    epsilon : float\n",
    "        Non-inferiority margin - maximum acceptable degradation (business requirement)\n",
    "        Example: 0.05 means \"can tolerate up to 5% degradation\"\n",
    "    expected_degradation : float, optional\n",
    "        Expected degradation based on domain knowledge (e.g., \"adding 2 clicks will\n",
    "        degrade by ~2%\"). If None, defaults to epsilon (conservative).\n",
    "        Should typically be LESS than epsilon.\n",
    "        Example: 0.02 means \"expect 2% degradation\"\n",
    "    alpha_prior_strength : float, optional\n",
    "        Strength parameter for the prior (default: 20). Smaller values give\n",
    "        wider (more uncertain) priors. Typical values: 10-30.\n",
    "    threshold : float, optional\n",
    "        Probability threshold for declaring non-inferiority (default: 0.95)\n",
    "        Variant is non-inferior if P(variant > control - epsilon) >= threshold\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict : Dictionary with results for each variant containing:\n",
    "        - 'is_non_inferior': bool, whether variant is non-inferior\n",
    "        - 'probability': float, P(variant > control - epsilon)\n",
    "        - 'control_rate': float, observed control conversion rate\n",
    "        - 'variant_rate': float, posterior mean of variant\n",
    "        - 'posterior_params': tuple, (alpha, beta) of variant posterior\n",
    "        - 'prior_params': tuple, (alpha_prior, beta_prior) used\n",
    "        - 'prior_mean': float, mean of the prior distribution\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute control conversion rate\n",
    "    control_rate = x_control / n_control\n",
    "\n",
    "    # Determine expected degradation (defaults to epsilon if not specified)\n",
    "    if expected_degradation is None:\n",
    "        expected_degradation = epsilon\n",
    "\n",
    "    # Construct weakly informative prior centered at expected performance\n",
    "    # This reflects domain knowledge, separate from the business threshold\n",
    "    target_prior_mean = control_rate - expected_degradation\n",
    "    alpha_prior = alpha_prior_strength\n",
    "    beta_prior = (alpha_prior / target_prior_mean) - alpha_prior\n",
    "\n",
    "    # Verify prior is valid (must have positive parameters)\n",
    "    if beta_prior <= 0:\n",
    "        raise ValueError(\n",
    "            f\"Invalid prior parameters: beta_prior={beta_prior:.4f} <= 0. \"\n",
    "            f\"This can happen when epsilon is too large relative to control_rate. \"\n",
    "            f\"Try reducing epsilon or increasing alpha_prior_strength.\"\n",
    "        )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for variant_name, data in variants_data.items():\n",
    "        # Variant posterior: Beta(x + α_prior, n - x + β_prior)\n",
    "        n = data[\"n\"]\n",
    "        x = data[\"x\"]\n",
    "        alpha_posterior = x + alpha_prior\n",
    "        beta_posterior = (n - x) + beta_prior\n",
    "        variant_posterior_mean = alpha_posterior / (alpha_posterior + beta_posterior)\n",
    "\n",
    "        # Non-inferiority threshold\n",
    "        non_inferiority_threshold = control_rate - epsilon\n",
    "\n",
    "        # Direct calculation: P(variant > threshold) using Beta CDF\n",
    "        # P(X > threshold) = 1 - P(X <= threshold) = 1 - CDF(threshold)\n",
    "        prob_non_inferior = 1 - beta_dist.cdf(\n",
    "            non_inferiority_threshold, alpha_posterior, beta_posterior\n",
    "        )\n",
    "\n",
    "        results[variant_name] = {\n",
    "            \"is_non_inferior\": prob_non_inferior >= threshold,\n",
    "            \"probability\": prob_non_inferior,\n",
    "            \"control_rate\": control_rate,\n",
    "            \"variant_rate\": variant_posterior_mean,\n",
    "            \"posterior_params\": (alpha_posterior, beta_posterior),\n",
    "            \"prior_params\": (alpha_prior, beta_prior),\n",
    "            \"prior_mean\": target_prior_mean,\n",
    "            \"threshold\": non_inferiority_threshold,  # Store the actual test threshold\n",
    "            \"epsilon\": epsilon,  # Store epsilon for reference\n",
    "            \"n\": n,  # Store sample size for plotting\n",
    "            \"x\": x,  # Store successes for plotting\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def select_best_variant(\n",
    "    variants_data,\n",
    "    alpha_prior=1,\n",
    "    beta_prior=1,\n",
    "    credible_level=0.95,\n",
    "    n_simulations=100000,\n",
    "):\n",
    "    \"\"\"\n",
    "    Select the best variant among multiple options using Bayesian approach.\n",
    "\n",
    "    Uses Monte Carlo simulation to determine which variant has the highest\n",
    "    probability of being the best, along with expected loss calculations\n",
    "    for decision-making under uncertainty.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    variants_data : dict\n",
    "        Dictionary with variant names as keys and {'n': samples, 'x': successes} as values\n",
    "        Example: {'A': {'n': 800, 'x': 168}, 'B': {'n': 800, 'x': 172}}\n",
    "    alpha_prior : float, optional\n",
    "        Alpha parameter for Beta prior (default: 1 for uniform)\n",
    "    beta_prior : float, optional\n",
    "        Beta parameter for Beta prior (default: 1 for uniform)\n",
    "    credible_level : float, optional\n",
    "        Credible interval level (default: 0.95)\n",
    "    n_simulations : int, optional\n",
    "        Number of Monte Carlo simulations (default: 100000)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict : Dictionary containing:\n",
    "        - 'best_variant': str, name of variant most likely to be best\n",
    "        - 'probabilities': dict, P(each variant is best)\n",
    "        - 'posterior_means': dict, posterior mean for each variant\n",
    "        - 'credible_intervals': dict, (lower, upper) credible interval for each variant\n",
    "        - 'expected_loss': dict, expected loss from choosing each variant\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> variants = {\n",
    "    ...     'A': {'n': 800, 'x': 168},\n",
    "    ...     'B': {'n': 800, 'x': 172},\n",
    "    ...     'C': {'n': 800, 'x': 165}\n",
    "    ... }\n",
    "    >>> result = select_best_variant(variants)\n",
    "    >>> print(result['best_variant'])\n",
    "    'B'\n",
    "    >>> print(result['probabilities'])\n",
    "    {'A': 0.31, 'B': 0.47, 'C': 0.22}\n",
    "    \"\"\"\n",
    "    variant_names = list(variants_data.keys())\n",
    "    posteriors = {}\n",
    "    samples = {}\n",
    "\n",
    "    # Compute posteriors and draw samples\n",
    "    for name, data in variants_data.items():\n",
    "        alpha_post = data[\"x\"] + alpha_prior\n",
    "        beta_post = data[\"n\"] - data[\"x\"] + beta_prior\n",
    "\n",
    "        posteriors[name] = {\n",
    "            \"alpha\": alpha_post,\n",
    "            \"beta\": beta_post,\n",
    "            \"mean\": alpha_post / (alpha_post + beta_post),\n",
    "        }\n",
    "\n",
    "        # Draw samples from posterior\n",
    "        samples[name] = beta_dist.rvs(alpha_post, beta_post, size=n_simulations)\n",
    "\n",
    "        # Compute credible interval\n",
    "        ci_lower = beta_dist.ppf((1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        ci_upper = beta_dist.ppf(1 - (1 - credible_level) / 2, alpha_post, beta_post)\n",
    "        posteriors[name][\"credible_interval\"] = (ci_lower, ci_upper)\n",
    "\n",
    "    # Monte Carlo: count how often each variant is best\n",
    "    best_counts = {name: 0 for name in variant_names}\n",
    "\n",
    "    for i in range(n_simulations):\n",
    "        # Get samples for this iteration\n",
    "        sample_values = {name: samples[name][i] for name in variant_names}\n",
    "\n",
    "        # Find best variant in this simulation\n",
    "        best_variant = max(sample_values, key=sample_values.get)\n",
    "        best_counts[best_variant] += 1\n",
    "\n",
    "    # Calculate probabilities\n",
    "    probabilities = {name: count / n_simulations for name, count in best_counts.items()}\n",
    "\n",
    "    # Expected loss: E[max(all) - this variant]\n",
    "    expected_loss = {}\n",
    "    for name in variant_names:\n",
    "        max_samples = np.maximum.reduce([samples[v] for v in variant_names])\n",
    "        losses = max_samples - samples[name]\n",
    "        expected_loss[name] = np.mean(losses)\n",
    "\n",
    "    # Determine best variant\n",
    "    best_variant = max(probabilities, key=probabilities.get)\n",
    "\n",
    "    return {\n",
    "        \"best_variant\": best_variant,\n",
    "        \"probabilities\": probabilities,\n",
    "        \"posterior_means\": {name: posteriors[name][\"mean\"] for name in variant_names},\n",
    "        \"credible_intervals\": {\n",
    "            name: posteriors[name][\"credible_interval\"] for name in variant_names\n",
    "        },\n",
    "        \"expected_loss\": expected_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: Mathematical Foundations\n",
    "\n",
    "This appendix provides the rigorous mathematical foundations underlying Bayesian A/B testing with Beta-Binomial conjugacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 The Beta Distribution\n",
    "\n",
    "### Definition\n",
    "\n",
    "The **Beta distribution** is a continuous probability distribution defined on the interval $[0,1]$, making it ideal for modeling probabilities and proportions.\n",
    "\n",
    "A random variable $p$ follows a Beta distribution with shape parameters $\\alpha > 0$ and $\\beta > 0$:\n",
    "\n",
    "$$\n",
    "p \\sim \\mathrm{Beta}(\\alpha, \\beta)\n",
    "$$\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "\n",
    "$$\n",
    "f(p \\mid \\alpha, \\beta) = \\frac{p^{\\alpha-1}(1-p)^{\\beta-1}}{B(\\alpha, \\beta)}\n",
    "\\quad \\text{for } p \\in [0,1]\n",
    "$$\n",
    "\n",
    "where $B(\\alpha, \\beta)$ is the **Beta function**, serving as a normalizing constant:\n",
    "\n",
    "$$\n",
    "B(\\alpha, \\beta) = \\int_0^1 t^{\\alpha-1} (1-t)^{\\beta-1} \\, dt\n",
    "$$\n",
    "\n",
    "The Beta function can also be expressed in terms of the **Gamma function**:\n",
    "\n",
    "$$\n",
    "B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\n",
    "$$\n",
    "\n",
    "where $\\Gamma(n) = (n-1)!$ for positive integers.\n",
    "\n",
    "---\n",
    "\n",
    "### Properties\n",
    "\n",
    "**Mean (Expected Value)**:\n",
    "$$\n",
    "E[p] = \\frac{\\alpha}{\\alpha + \\beta}\n",
    "$$\n",
    "\n",
    "**Variance**:\n",
    "$$\n",
    "\\mathrm{Var}(p) = \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n",
    "$$\n",
    "\n",
    "**Mode** (for $\\alpha, \\beta > 1$):\n",
    "$$\n",
    "\\text{Mode}(p) = \\frac{\\alpha - 1}{\\alpha + \\beta - 2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition: Pseudo-Counts\n",
    "\n",
    "The parameters $\\alpha$ and $\\beta$ can be interpreted as **prior pseudo-observations**:\n",
    "\n",
    "- $\\alpha - 1$ = \"prior successes\"\n",
    "- $\\beta - 1$ = \"prior failures\"\n",
    "- $\\alpha + \\beta$ = \"prior sample size\"\n",
    "\n",
    "**Special cases**:\n",
    "\n",
    "- **Beta(1, 1)**: Uniform distribution — complete ignorance, all values equally likely\n",
    "- **Beta(2, 2)**: Slightly peaked at 0.5 — weak belief in fairness\n",
    "- **Beta(10, 10)**: Strongly peaked at 0.5 — strong prior belief\n",
    "- **Beta(10, 90)**: Peaked at 0.1 — strong belief in low probability\n",
    "\n",
    "**Concentration parameter**: $\\alpha + \\beta$ controls \"certainty\"\n",
    "- Small values → wide, flat distribution (high uncertainty)\n",
    "- Large values → narrow, peaked distribution (high certainty)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 The Bernoulli Likelihood\n",
    "\n",
    "### Single Trial\n",
    "\n",
    "A **Bernoulli trial** is a random experiment with exactly two outcomes: success (1) or failure (0).\n",
    "\n",
    "For a single observation $x \\in \\{0, 1\\}$ with success probability $p$:\n",
    "\n",
    "$$\n",
    "P(x \\mid p) = p^x (1-p)^{1-x}\n",
    "$$\n",
    "\n",
    "This equals:\n",
    "- $p$ if $x = 1$ (success)\n",
    "- $1-p$ if $x = 0$ (failure)\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple Independent Trials\n",
    "\n",
    "For $n$ independent Bernoulli trials with $k$ successes and $(n-k)$ failures:\n",
    "\n",
    "$$\n",
    "P(\\text{data} \\mid p) = P(k \\text{ successes in } n \\text{ trials} \\mid p)\n",
    "$$\n",
    "\n",
    "The **likelihood function** is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p \\mid k, n) = \\binom{n}{k} p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "where $\\binom{n}{k} = \\frac{n!}{k!(n-k)!}$ is the binomial coefficient.\n",
    "\n",
    "**Key insight**: For Bayesian inference, the binomial coefficient is a **constant** (doesn't depend on $p$), so we can write:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(p \\mid k, n) \\propto p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "This form — $p^k (1-p)^{n-k}$ — is **crucial** for conjugacy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Bayesian Inference: From Prior to Posterior\n",
    "\n",
    "### Bayes' Theorem\n",
    "\n",
    "Bayes' theorem relates prior beliefs to posterior beliefs after observing data:\n",
    "\n",
    "$$\n",
    "\\boxed{P(\\theta \\mid \\text{data}) = \\frac{P(\\text{data} \\mid \\theta) \\cdot P(\\theta)}{P(\\text{data})}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $P(\\theta \\mid \\text{data})$ = **Posterior** (what we want)\n",
    "- $P(\\text{data} \\mid \\theta)$ = **Likelihood** (how likely data is given parameter)\n",
    "- $P(\\theta)$ = **Prior** (our belief before seeing data)\n",
    "- $P(\\text{data})$ = **Evidence** (normalizing constant)\n",
    "\n",
    "---\n",
    "\n",
    "### The Evidence (Marginal Likelihood)\n",
    "\n",
    "The denominator $P(\\text{data})$ is computed by **integrating over all possible parameter values**:\n",
    "\n",
    "$$\n",
    "P(\\text{data}) = \\int_{\\text{all } \\theta} P(\\text{data} \\mid \\theta) \\cdot P(\\theta) \\, d\\theta\n",
    "$$\n",
    "\n",
    "For our conversion rate problem with $p \\in [0,1]$:\n",
    "\n",
    "$$\n",
    "P(\\text{data}) = \\int_0^1 P(\\text{data} \\mid p) \\cdot P(p) \\, dp\n",
    "$$\n",
    "\n",
    "This integral represents **averaging the likelihood over all possible conversion rates**, weighted by our prior belief.\n",
    "\n",
    "**Interpretation**: \"How probable is this data, considering all possible values of $p$ and our prior beliefs?\"\n",
    "\n",
    "---\n",
    "\n",
    "### Proportionality Form\n",
    "\n",
    "Since $P(\\text{data})$ doesn't depend on $\\theta$, we often write:\n",
    "\n",
    "$$\n",
    "P(\\theta \\mid \\text{data}) \\propto P(\\text{data} \\mid \\theta) \\cdot P(\\theta)\n",
    "$$\n",
    "\n",
    "**Posterior ∝ Likelihood × Prior**\n",
    "\n",
    "This is the fundamental equation of Bayesian inference.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Beta-Binomial Conjugacy: The Closed-Form Miracle\n",
    "\n",
    "### Why Conjugacy Matters\n",
    "\n",
    "For most combinations of prior and likelihood, computing the posterior requires **numerical integration** — expensive and complex.\n",
    "\n",
    "But for certain \"conjugate\" pairs, the posterior has a **closed-form solution** — we can write down the answer directly!\n",
    "\n",
    "**Beta prior + Binomial likelihood = Beta posterior**\n",
    "\n",
    "This is why Beta distributions are ubiquitous in Bayesian A/B testing.\n",
    "\n",
    "---\n",
    "\n",
    "### The Derivation\n",
    "\n",
    "**Setup**: \n",
    "- Prior: $p \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0)$\n",
    "- Data: $k$ successes in $n$ trials\n",
    "- Goal: Find posterior $P(p \\mid k, n)$\n",
    "\n",
    "**Step 1: Write out Bayes' theorem**\n",
    "\n",
    "$$\n",
    "P(p \\mid k, n) \\propto P(k, n \\mid p) \\cdot P(p)\n",
    "$$\n",
    "\n",
    "**Step 2: Substitute the likelihood (Binomial)**\n",
    "\n",
    "$$\n",
    "P(k, n \\mid p) \\propto p^k (1-p)^{n-k}\n",
    "$$\n",
    "\n",
    "**Step 3: Substitute the prior (Beta)**\n",
    "\n",
    "$$\n",
    "P(p) = \\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}\n",
    "$$\n",
    "\n",
    "**Step 4: Multiply likelihood and prior**\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(p \\mid k, n) &\\propto p^k (1-p)^{n-k} \\cdot \\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)} \\\\\n",
    "&\\propto p^k (1-p)^{n-k} \\cdot p^{\\alpha_0-1}(1-p)^{\\beta_0-1} \\\\\n",
    "&= p^{k + \\alpha_0 - 1} (1-p)^{(n-k) + \\beta_0 - 1}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 5: Recognize the form**\n",
    "\n",
    "This has the form $p^{\\alpha-1}(1-p)^{\\beta-1}$, which is the **kernel of a Beta distribution**!\n",
    "\n",
    "Specifically, it's $\\mathrm{Beta}(\\alpha, \\beta)$ where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha &= k + \\alpha_0 \\\\\n",
    "\\beta &= (n - k) + \\beta_0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### The Result: Bayesian Update Rule\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{align}\n",
    "\\text{Prior: } &p \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0) \\\\\n",
    "\\text{Data: } &k \\text{ successes in } n \\text{ trials} \\\\\n",
    "\\text{Posterior: } &p \\mid \\text{data} \\sim \\mathrm{Beta}(\\alpha_0 + k,\\; \\beta_0 + (n-k))\n",
    "\\end{align}\n",
    "}\n",
    "$$\n",
    "\n",
    "**Simple update rule**:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_{\\text{posterior}} &= \\alpha_{\\text{prior}} + \\text{(observed successes)} \\\\\n",
    "\\beta_{\\text{posterior}} &= \\beta_{\\text{prior}} + \\text{(observed failures)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**No integration needed!** Just add counts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 Why the Integration Works Out\n",
    "\n",
    "### The Normalizing Constant\n",
    "\n",
    "We skipped the $P(\\text{data})$ term by using proportionality. But where does it come from?\n",
    "\n",
    "**Full Bayes' theorem**:\n",
    "\n",
    "$$\n",
    "P(p \\mid k, n) = \\frac{P(k, n \\mid p) \\cdot P(p)}{P(k, n)}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "P(k, n) = \\int_0^1 P(k, n \\mid p) \\cdot P(p) \\, dp\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Explicit Integration\n",
    "\n",
    "Substituting our Beta prior and Binomial likelihood:\n",
    "\n",
    "$$\n",
    "P(k, n) = \\int_0^1 \\left[\\binom{n}{k} p^k (1-p)^{n-k}\\right] \\cdot \\left[\\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}\\right] dp\n",
    "$$\n",
    "\n",
    "**Pull out constants**:\n",
    "\n",
    "$$\n",
    "P(k, n) = \\frac{\\binom{n}{k}}{B(\\alpha_0, \\beta_0)} \\int_0^1 p^{k + \\alpha_0 - 1} (1-p)^{(n-k) + \\beta_0 - 1} \\, dp\n",
    "$$\n",
    "\n",
    "**Recognize the integral**:\n",
    "\n",
    "The integral is exactly the **Beta function**:\n",
    "\n",
    "$$\n",
    "\\int_0^1 p^{\\alpha-1} (1-p)^{\\beta-1} \\, dp = B(\\alpha, \\beta)\n",
    "$$\n",
    "\n",
    "with $\\alpha = k + \\alpha_0$ and $\\beta = (n-k) + \\beta_0$.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "P(k, n) = \\frac{\\binom{n}{k} \\cdot B(k + \\alpha_0,\\, (n-k) + \\beta_0)}{B(\\alpha_0, \\beta_0)}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### The Full Posterior\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "P(p \\mid k, n) &= \\frac{\\binom{n}{k} p^k (1-p)^{n-k} \\cdot \\frac{p^{\\alpha_0-1}(1-p)^{\\beta_0-1}}{B(\\alpha_0, \\beta_0)}}{\\frac{\\binom{n}{k} \\cdot B(k + \\alpha_0,\\, (n-k) + \\beta_0)}{B(\\alpha_0, \\beta_0)}} \\\\\n",
    "&= \\frac{p^{k + \\alpha_0 - 1} (1-p)^{(n-k) + \\beta_0 - 1}}{B(k + \\alpha_0,\\, (n-k) + \\beta_0)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is exactly the PDF of $\\mathrm{Beta}(k + \\alpha_0,\\, (n-k) + \\beta_0)$! ✓\n",
    "\n",
    "---\n",
    "\n",
    "### Why This is Remarkable\n",
    "\n",
    "**The integral over all possible proportions**:\n",
    "$$\n",
    "\\int_0^1 p^{k + \\alpha_0 - 1} (1-p)^{(n-k) + \\beta_0 - 1} \\, dp\n",
    "$$\n",
    "\n",
    "has a **closed-form solution** (the Beta function), which means:\n",
    "\n",
    "1. **No numerical integration** needed — exact answer\n",
    "2. **Fast computation** — just add and subtract\n",
    "3. **Sequential updates** — posterior becomes next prior\n",
    "4. **Mathematically elegant** — form is preserved\n",
    "\n",
    "This is **conjugacy**: The posterior is in the same family (Beta) as the prior.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.6 Example: Updating with Real Data\n",
    "\n",
    "### Scenario\n",
    "\n",
    "Suppose we have:\n",
    "- **Prior**: Beta(20, 8) — weakly informative, centered at $\\frac{20}{28} \\approx 0.71$\n",
    "- **Data**: Variant C from our experiment\n",
    "  - $n = 2022$ trials\n",
    "  - $k = 1396$ successes\n",
    "  - Observed rate: $\\frac{1396}{2022} \\approx 0.690$\n",
    "\n",
    "### Bayesian Update\n",
    "\n",
    "**Prior**:\n",
    "$$\n",
    "p \\sim \\mathrm{Beta}(20, 8)\n",
    "$$\n",
    "\n",
    "**Posterior** (after observing data):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\alpha_{\\text{post}} &= 20 + 1396 = 1416 \\\\\n",
    "\\beta_{\\text{post}} &= 8 + (2022 - 1396) = 634 \\\\\n",
    "p \\mid \\text{data} &\\sim \\mathrm{Beta}(1416, 634)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Posterior mean**:\n",
    "$$\n",
    "E[p \\mid \\text{data}] = \\frac{1416}{1416 + 634} = \\frac{1416}{2050} \\approx 0.6907\n",
    "$$\n",
    "\n",
    "**Posterior variance**:\n",
    "$$\n",
    "\\mathrm{Var}(p \\mid \\text{data}) = \\frac{1416 \\times 634}{2050^2 \\times 2051} \\approx 0.000104\n",
    "$$\n",
    "\n",
    "**Standard deviation**: $\\sqrt{0.000104} \\approx 0.0102$ (about 1%)\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- **Prior belief**: $p \\approx 0.71$ with high uncertainty (only 28 pseudo-observations)\n",
    "- **Data**: $p \\approx 0.69$ with 2022 observations\n",
    "- **Posterior**: $p \\approx 0.69$ with low uncertainty (2050 effective observations)\n",
    "\n",
    "The **data dominates** because we have many more real observations than prior pseudo-observations.\n",
    "\n",
    "If we had used a **stronger prior** (e.g., Beta(200, 80)), it would have pulled the posterior closer to 0.71, but would require more data to overcome.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.7 Sequential Updates: The Bayesian Learning Loop\n",
    "\n",
    "### Conjugacy Enables Sequential Learning\n",
    "\n",
    "Because the posterior is also a Beta distribution, we can use it as the **prior for the next update**:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{Prior}_1 &\\xrightarrow{\\text{Data}_1} \\text{Posterior}_1 \\\\\n",
    "\\text{Posterior}_1 &= \\text{Prior}_2 \\xrightarrow{\\text{Data}_2} \\text{Posterior}_2 \\\\\n",
    "\\text{Posterior}_2 &= \\text{Prior}_3 \\xrightarrow{\\text{Data}_3} \\text{Posterior}_3 \\\\\n",
    "&\\vdots\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**This is how Thompson sampling works!**\n",
    "\n",
    "Each user:\n",
    "1. Current belief = Beta($\\alpha$, $\\beta$)\n",
    "2. Show variant, observe conversion\n",
    "3. Update: Beta($\\alpha + \\text{conversion}$, $\\beta + (1 - \\text{conversion})$)\n",
    "4. Repeat\n",
    "\n",
    "---\n",
    "\n",
    "### Mathematical Equivalence\n",
    "\n",
    "**Batch update** (all data at once):\n",
    "$$\n",
    "\\mathrm{Beta}(\\alpha_0, \\beta_0) \\xrightarrow{n \\text{ trials, } k \\text{ successes}} \\mathrm{Beta}(\\alpha_0 + k, \\beta_0 + (n-k))\n",
    "$$\n",
    "\n",
    "**Sequential update** (one at a time):\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathrm{Beta}(\\alpha_0, \\beta_0) &\\xrightarrow{x_1} \\mathrm{Beta}(\\alpha_0 + x_1, \\beta_0 + (1-x_1)) \\\\\n",
    "&\\xrightarrow{x_2} \\mathrm{Beta}(\\alpha_0 + x_1 + x_2, \\beta_0 + (2 - x_1 - x_2)) \\\\\n",
    "&\\quad\\vdots \\\\\n",
    "&\\xrightarrow{x_n} \\mathrm{Beta}(\\alpha_0 + \\sum x_i, \\beta_0 + (n - \\sum x_i))\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Result is identical!** Order of observations doesn't matter.\n",
    "\n",
    "This property enables:\n",
    "- **Continuous monitoring** (no need to wait for fixed sample size)\n",
    "- **Online learning** (update as data arrives)\n",
    "- **Thompson sampling** (sample → update → repeat)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.8 Summary: The Mathematical Beauty of Beta-Binomial Conjugacy\n",
    "\n",
    "### The Full Picture\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{array}{rcl}\n",
    "\\text{Prior} & : & p \\sim \\mathrm{Beta}(\\alpha_0, \\beta_0) \\\\\n",
    "&&\\\\\n",
    "\\text{Likelihood} & : & P(k \\mid p, n) = \\binom{n}{k} p^k (1-p)^{n-k} \\\\\n",
    "&&\\\\\n",
    "\\text{Evidence} & : & P(k, n) = \\displaystyle\\int_0^1 P(k \\mid p, n) \\cdot P(p) \\, dp = \\frac{\\binom{n}{k} \\cdot B(\\alpha_0 + k, \\beta_0 + n - k)}{B(\\alpha_0, \\beta_0)} \\\\\n",
    "&&\\\\\n",
    "\\text{Posterior} & : & p \\mid k, n \\sim \\mathrm{Beta}(\\alpha_0 + k,\\; \\beta_0 + (n-k))\n",
    "\\end{array}\n",
    "}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters for A/B Testing\n",
    "\n",
    "1. **Closed-form solution**: No numerical integration required\n",
    "   - Integration over all proportions $\\int_0^1$ has exact answer\n",
    "   - Result is another Beta distribution\n",
    "\n",
    "2. **Simple updates**: Just add counts\n",
    "   - $\\alpha \\leftarrow \\alpha + \\text{successes}$\n",
    "   - $\\beta \\leftarrow \\beta + \\text{failures}$\n",
    "\n",
    "3. **Sequential learning**: Posterior becomes next prior\n",
    "   - No need to recompute from scratch\n",
    "   - Enables online/streaming algorithms\n",
    "\n",
    "4. **Computational efficiency**: $O(1)$ per update\n",
    "   - No matrix inversions\n",
    "   - No iterative optimization\n",
    "   - Scales to millions of updates\n",
    "\n",
    "5. **Interpretability**: Parameters are pseudo-counts\n",
    "   - Easy to set priors\n",
    "   - Easy to explain to stakeholders\n",
    "\n",
    "6. **Enables Thompson sampling**: Natural exploration-exploitation\n",
    "   - Sample from posterior\n",
    "   - Update posterior\n",
    "   - Repeat\n",
    "\n",
    "---\n",
    "\n",
    "### The Fundamental Insight\n",
    "\n",
    "**Conjugacy transforms an intractable integration problem into trivial arithmetic.**\n",
    "\n",
    "Instead of:\n",
    "$$\n",
    "\\text{Posterior} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\int_0^1 \\text{Likelihood} \\times \\text{Prior} \\, dp}\n",
    "$$\n",
    "\n",
    "We get:\n",
    "$$\n",
    "\\text{Posterior parameters} = \\text{Prior parameters} + \\text{Data counts}\n",
    "$$\n",
    "\n",
    "This is the **mathematical foundation** that makes Bayesian A/B testing practical, fast, and elegant.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
